\section{Experimental Plan}

%Our experiment aims to uncover...


\section{Selecting datasets for evaluation}

In addition to evaluate the methods on the Sobazar dataset we want to make sure that our solution generalizes beyond our experimental dataset, in accordance to the general guidelines for experimental studies \cite{Shani2011}.

The data used for offline evaluation should match as closely as possible the data we expect the recommender system to face when it is deployed \cite{Gunawardana2009}. However, most publicly available datasets are not collected from e-commerce sites and does therefore not contain user browsing history, purchases etc.


Natural to assume that the number of users on Sobazar will increase as they system \emph{officially} goes live.

%Always a possibility to reduce the size of datasets by removing some ratings...




When selecting datasets for evaluation we focused on the following dataset properties:


\begin{itemize}
\item Size of dataset: Preferable as close as possible to Sobazar in terms of number of ratings, users and items.
\item Type of user feedback: Preferably implicit, as it allows us to test our implicit feedback to implicit rating function.
\item Domain: E-commerce datasets with purchase history would be most relevant.
\item Precence of features: To evaluate hybrid methods?
\end{itemize}


The reason for selecting the MovieLens 1M dataset over MovieLens 100K is that the number of users is closer to what we expect the SoBazar to have after the \emph{official} launch this summer. 


\subsection{Dataset 1}

Rossmann dataset?
%General statistics and averages
%Interesting findings/properties
%Was cleaning neccesary?
%How was the methods evaluated on the dataset?
%	- x-fold cross validation

\subsection{MovieLens 1M}

As our second dataset we have chosen the MovieLens 1M dataset. The dataset contain 1,000,209 anonymous ratings on approximately 3,706 movies (The readme mentions 3900 movies) made by 6040 users. User features included age, gender, occupation and zipcode, item features include movie genre. Each user included in the dataset have provided a minimum of 20 ratings. The average rating given to movies in the dataset is $3.58$. There are 18 different genres in the dataset, but each movie can have multiple genres assigned to it, of which there are 498 different combinations in the dataset.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
Male & Female \\ \hline
4331 & 1709 \\ \hline
\end{tabular}
\caption{MovieLens Gender Distribution}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Under 18 & 18-24 & 25-34 	& 35-44 	& 45-49 & 50-55 & 56+ \\ \hline
222		 &	1103 &	2096	&	1193	& 550	& 496	& 380 \\ \hline
\end{tabular}
\caption{MovieLens 1M Age Group Distribution}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
Other/not specified  & 711  \\ \hline
Academic/Educator  & 528  \\ \hline
Artist  & 267 \\ \hline
Clerical/Admin & 173 \\ \hline
College/Grad student  & 759 \\ \hline
Customer service & 112 \\ \hline
Doctor/Health care & 236 \\ \hline
Executive/Managerial & 679 \\ \hline
Farmer & 17 \\ \hline
Homemaker & 92 \\ \hline
K-12 student & 195 \\ \hline
Lawyer & 129 \\ \hline
Programmer & 388 \\ \hline
Retired & 142 \\ \hline
Sales/Marketing & 302 \\ \hline
Scientist & 144 \\ \hline
Self-employed & 241 \\ \hline
Technician/Engineer & 502 \\ \hline
Tradesman/Craftsman & 70 \\ \hline
Unemployed & 72 \\ \hline
Writer & 281 \\ \hline
\end{tabular}
\caption{MovieLens 1M Occupation Distribution}
\end{table}

As the sparsity of this dataset is fairly low (95.53164), we decided to evaluate this dataset using the holdout method. As the dataset include time-stamps we split the dataset based on time, meaning that all ratings given before a given time will be used to train the model and all ratings given after this point will be used as a testset. We set aside the last 25\% ratings for evaluation and train the model using the remaining 75\%.


\subsection{Sobazar Data}

%Highlight limitations of the sobazar dataset for evaluation.
	%-> Implications for evaluation
The Sobazar dataset is the most sparse of our datasets used for evaluation. This has several implications. 


The number of folds depends on the size of the dataset. For large datasets, even 3-fold Cross Validation will be quite accurate, while for very sparse datasets, we may have to use leave-one-out in order to train on as many examples as possible. The advantages of using a large number of folds is that the bias of the true error rate estimators will be small (the estimator will be very accurate), with the disadvantages being that the variance of the true error rate estimator will be large in addition to increased computation time. To exemplify this we ran a small experiment on the Sobazar data using IBCF, with $k-NN=3$, experimenting with different cross-validation splits:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
K-fold & 	$min_{RMSE}$ 	&	$max_{RMSE}$ 	& Average 	& Variance 					\\ \hline
3	   & 	0.783 			& 	0.789 			& 0.785 	& $8.730 \times 10^{-6}$	\\ \hline
5	   & 	0.759			& 	0.802 			& 0.781 	& $3.363 \times 10^{-4}$ 	\\ \hline
8	   & 	0.740			& 	0.781			& 0.758 	& $1.656 \times 10^{-4}$ 	\\ \hline
10	   & 	0.718 			& 	1.026			& 0.810  	& 0.0125					\\ \hline
\end{tabular}
\caption{Evaluation results from experimenting with different k-fold splits on the Sobazar dataset}
\end{table}

When increasing the number of folds we could see that it was unable to generate any recommendations at all for some folds, or getting really poor results, meaning that we get some difficult splits. E.g. when using 30 folds, IBCF was unable to provide any recommendations for 8 folds out of 30. Based on these results we decided to go with $5-10$ \marginpar{Determine final number...} folds for model validation.

\marginpar{What about \emph{all but one}, or leave one out?}


\subsection{Summary}

%Some fancy tables n stuff

%TODO - What else is interesting to know? Rating scale, average number of ratings per user, number of cold start users...

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
	Dataset			& 	Ratings 	& 	Users	& 	Items 	& 	Sparsity 	\\ \hline
	Sobazar 		& 	15,252  	& 	1,235	&	3,386	&	99.65599	\\ \hline
	Movielens 1M	& 	1,000,029   &	6040 	&	3706	&	95.53164	\\ \hline
	Dataset 2 		& 	-  			& 	-		&	-		&	-			\\ \hline
    \end{tabular}
    \label{table:datasets}
    \caption [Overview of the datasets used for evaluation]{Overview of the datasets used for evaluation}
\end{table}


\section{Simulating the cold-start problem}

%Describe the methodology used to simulate the cold-start problem

%	- Why did we choose this methodology?

%TODO - Look at reasoning in the papers



%	- How to split the dataset

%TODO - Describe the splitting method in detail

% Can the Sobazar data be treated similarly as the other datasets?


\subsection{Evaluation Metrics}



