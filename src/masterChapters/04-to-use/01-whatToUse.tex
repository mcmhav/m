% !TEX root = ../../report.tex

\section{What to use}

%Having a large amount of data like e.g. in the netflix dataset
% -> Do not require a great understanding of the data to get decent results
% -> Out case is a little different. What implications does the limited amount of data have?

%We need to take this into account when designing our system / selecting methods for recomendations


\subsection{Some Awesome Algorithms (Build up with project progress)}

This section aims to describe the algorithms which will be evaluated in our experiment.

\subsubsection{Most-popular Recommender}

We have developed a simple most-popular recommender that uses result dithering to \emph{randomize} the recommendations to the users. One could imagine to final system to leverage multiple recommendation techniques. When users are new to the system they are recommended the most popular items, until enough data is collected to provide personalized recommendations. One could imagine multiple categories of most popular recommendations: Most viewed, most wanted, most bought or a combination of all.

Dithering adds \emph{noise} to the algorithm, which permutes the results in such a way that the top few results have a high probability of remaining on the top spots, but as one goes deeper into the results, the degree of mixing increases dramatically. It is important to note that dithering is \emph{guaranteed} to make off-line performance worse, but is likely to make the actual performance better. We have experimented with two different methods of dithering:

\begin{itemize}
\item Score = log2(rank) - runif(x, y)
\item Score = log2(rank) - x*rexp(y)
\end{itemize}

Given $x=8$ and $y=2$ alternative one generated the following permutations of the original ranking in 10 runs:

\begin{enumerate}
	\item 8, 3, 24, 20, 1, 19, 22, 15, 42, 36
	\item 2, 0, 1, 32, 20, 3, 35, 34, 43, 10
	\item 7, 1, 3, 12, 2, 25, 0, 9, 24, 27
	\item 0, 4, 3, 5, 7, 16, 26, 22, 13, 33
	\item 0, 10, 8, 1, 15, 5, 30, 17, 11, 35
	\item 7, 4, 6, 12, 2, 1, 19, 0, 27, 9
	\item 1, 5, 0, 2, 9, 3, 20, 12, 4, 31
	\item 0, 1, 2, 5, 39, 4, 15, 41, 10, 22
	\item 4, 6, 0, 3, 1, 29, 36, 31, 35, 20
	\item 5, 1, 0, 8, 3, 18, 25, 24, 2, 28
\end{enumerate}

Given $x=3.0$ and $y=2.5$ alternative two generates the following permutations of the original most popular ranking in 10 runs:

\begin{enumerate}
	\item 2, 4, 0, 35, 3, 1, 15, 72, 9, 5
	\item 0, 10, 11, 17, 1, 3, 8, 41, 15, 5
	\item 44, 1, 0, 15, 9, 4, 5, 59, 26, 2
	\item 0, 98, 1, 4, 2, 41, 8, 26, 11, 94
	\item 0, 6, 1, 2, 70, 4, 19, 14, 8, 3
	\item 2, 5, 0, 16, 15, 18, 1, 3, 32, 6
	\item 2, 65, 4, 0, 3, 45, 8, 1, 48, 36
	\item 3, 49, 5, 0, 2, 82, 8, 77, 11, 4
	\item 0, 1, 21, 8, 4, 85, 2, 6, 47, 3
	\item 0, 1, 21, 70, 11, 20, 2, 10, 9, 3
\end{enumerate}

Here 0 is the most popular item before the permutation. The results show that alternative two has a higher degree of mixing than alternative one, as the added noise is larger than alternative one. The values of $x$ and $y$ can be modified to achieve the desired degree/level/amount of mixing.

\subsubsection{Item-Average}

Item average is a simple recommender that always estimates the preference for an item to be the average of all known preference values for that item. No information about users is taken into account. This recommender can therefore be considered a \emph{highest rated} recommender, as it is likely to recommend the highest rated items. The following equation shows the rating prediction procedure:

\begin{equation}
\label{equation:itemaverageratingprediction}
u(c,s) = k * \sum_{c' \epsilon C} u(c',s)
\end{equation}

Where $k$ again is a normalization factor ($1/|C|$). This is somewhat similar to collaborative filtering, except for the fact that the user similarity $sim(c, c')$ has been taken out of the equation. It is also worth mentioning that this method is not suited for binary ratings, as the result is likely to be very random, and should therefore not be used without item ratings to average.

\subsubsection{User-based Collaborative Filtering}

Recommend items by finding similar users. This is often harder to scale because of the dynamic nature of users. The pearson correlation coefficient is used to calculate the user similarities. For a more in depth description of user-based collaborative filtering see Section \ref{subsec:cf}.

\subsubsection{Item-based Collaborative Filtering}

Calculate similarity between items and make recommendations. Items usually don't change much, so this often can be computed offline. For a more in depth description of item-based collaborative filtering see Section \ref{subsec:cf}.%

\subsubsection{ALS-WR}

Alternating-least-squares with weighted-$\lambda$-regularization (ALS-WR) was designed fro the Netflix Prize Competition \cite{Netflix}, where it obtained an RMSE score of 0.8975, which was one of the best results based on a pure method.

Alternating-least-squares is a method to solve Equation \ref{equation:minimize}. Since both $q_{s}$ and $p_{c}$ are unknown, the equation is not convex. However if we fix one of the unknowns, the optimization problem becomes quadratic and can be solved optimally. The ALS technique rotate between fixing the $q_{s}$'s and fixing the $p_{c}$'s. When all the $p_{c}$'s are fixed, the system recomputes the $q_{s}$'s by solving a least-squares problem, and vica versa. This ensures that each step decreases the error until convergence. What makes ALS favorable over the simpler and faster stochastic gradient descent is two things. ALS can be parallelized since the system computes the $q_{s}$'s independently of the other item factors, the same can also be applied to the user factors. The second case if for systems centered around implicit data. Because the training set cannot be considered sparse, looping over each single training case as gradient descent would not be practical, but ALS can efficiently handle such cases \cite{Hu2008}.\newline

ALS solves the low-rank matrix factorization as follows:

\begin{itemize}
\item Step 1: Initialize the matrix M by assigning the average rating for that movie as the first row, and a small random numbers for the remaining entries;
\item Step 2: Fix P, solve Q by minimizing the objective function (the sum of squared errors);
\item Step 3: Fix Q, solve by minimizing the objective function similarly;
\item Step 4: Repeat Steps 2 and 3 until a stopping criterion is satisfied.
\end{itemize}

Zhou et. al. \cite{Zhou2008} used the difference in RMSEs between the rounds as a stopping criterion. Without regularization ALS might lead to overfitting due to the many free parameters. Regularization was therefore introduced in the form of weighted-$\lambda$-regularization to prevent the model from overfitting.

\begin{equation}
f(P, Q) = \sum_{(c,s)\epsilon C} (u(c,s) - p^{T}_{c}q_{s})^{2} + \lambda (\sum_{c} n_{p_{c}} \Vert p_{c} \Vert ^{2} + \sum_{s} n_{q_{s}} \Vert q_{s} \Vert ^{2})
\label{WeightedLamba}
\end{equation}

where $n_{p_{c}}$ and $n_{q_{s}}$ denote the number of ratings of user $c$ and item $s$ respectively. $S_{c}$ denote the set of items $s$ that user $c$ rated, then $n_{p_{c}}$ is the cardinality of $S_{c}$; similarly $C_{s}$ denotes the set of users who rated item $s$, and $n_{q_{s}}$ is the cardinality of $I_{s}$. A given column of P, $p_{c}$ is found by solving a regularized linear least squares problem involving the known ratings of user $c$, and the feature vectors $q_{s}$ of the items that user $c$ has rated. Similarly, we can compute individual $q_{s}$'s via a regularized linear least squares solution, using the feature vectors of users who rated item $j$, and their ratings of it.

\subsubsection{Content-based}

We also wish to see how well a content-based approach performs compared to our collaborative
filtering models.

To find the product type, material, style and color we looked at the title, description and meta-description
fields looking for certain keywords. The fact that descriptions could be either in Norwegian and in English we had to include the words from both languages in the check. To find the words to classify the items we looked through the top keyword lists in both languages, and tried to group them as logically as possible.

E.g. to determine if a product falls under the "sweater" category we check for the following keywords:

$['sweater','cardigan','jumper','hoody','genser','genseren']$

We also experimented with stemming from the nltk software package in python, but we did not feel that it improved
our results, and we therefore dropped stemming. We attempt to extract the following features from
the product-database content:

\begin{itemize}
\item Brand: Bik Bok, InWear, H&M...
\item Price range: 0-199, 200-399, 400-599...
\item Product type: Dress, Jacket, Top, Pants, Boots...
\item Material: Cotton, Wool, Polyester...
\item Style: Classic, Modern, Luxurious...
\item Color: Black, Grey, Blue...
\end{itemize}

The color attribute is missing for most items

It was a pleasant surprise to find that 3400 out of 3600 items could be found in the
product database and be assigned features.
%Problems? Limited amount of features

%Table with features?

\subsection{Cold-start Solutions}

%Justify why we only tested out one approach... 

The number of cold-start solutions we are currently able to test out is pretty limited.

As we never got access to more than item-features, we can cross out RBLF and others 
methods that require user features in addition to item-features. This leaves us with
Naive Filterbots \cite{Park2006} and Learning Attribute To Feature Mapping \cite{Gantner2010}.
However, the latter is a model for positive implicit feedback only, meaning that it can not be
combined with our implicit ratings.

\subsubsection{Filterbots}

As one simple solution to the cold-start problem we decided to experiment with Filterbots, as it easily
combined with our implicit ratings. Similarly as in \cite{Park2006} we decided to experiment with global bots.

We implemented the following filterbots:

\begin{itemize}
	\item BrandBot: Rate items based on the brand average,
	\item AverageBot: Rate items based on their average rating over all users,
	\item CriticBot: Select $n$ critics among the most active users and rate items based on their average,
	\item PopularityBot: Rate an item based on its popularity, more ratings equals a higher rating.
\end{itemize}

Each bot is added as a single user into our training set before we evaluate the methods on the test-set.
It is also worth noting that we use the training set only (obviously) to generate the bot ratings.
Potentially help solve the cold-start user problem by making it possible for new users to connect to users
that capture the general underlying trends of the entire user group. Filterbots were meant to improve
performance when data is scarce and not degrade performance when data is plentiful. Which means that
we hopefully will see the best improvements in our cold-start experiments, while getting similar
or slightly better results when all ratings are used.

\emph{Learning attribute to feature mapping?}

%Cold start item problem
%Downside... positive only ratings...

%\subsection{Some one-class collaborative filtering...}

\subsubsection{The Good}
\subsubsection{The Bad}
\subsection{Why Not To Use These (Same As above)}
\subsubsection{The Good}
\subsubsection{The Bad}
