

\section{Evaluation Metrics}
\label{sec:eval-metrics}

%http://www.slideshare.net/gunnar-schroeder

A large variety of metrics have been published, and some of these metrics are highly correlated \cite{Herlocker2004}.
There is little guidance for evaluating recommender systems and choosing metrics. However, there are
some important questions one should ask oneself when selecting evaluation metrics:

\begin{itemize}
	\item Which aspects of the usage scenario and the data influence the choice?
	\item Which metrics are applicable?
	\item What does these metrics express?
	\item What are the differences among them?
	\item Which metric represent our user-case best?
	\item How much do the metrics suffer biases?
\end{itemize}

It is safe to assume that the users are more interested in the top ranked items, than rating
predictions for the entire item collection. The social feed shown in Figure \ref{figure:sobazarfeed}
shows that the interface currently supports showing up to 20 items.

Evaluation of top-k recommendations suggests a classification or ranking task, the evaluation
should therefore focus on classification or ranking accuracy. A classification accuracy metric
will tell us something about the recommenders ability to retrieve relevant items. The ranking
accuracy metric should measure how well the different methods are at putting the most relevant items
e.g. purchases consistently over the assumed less relevant ones such as e.g. clicks. This gives us
two hints with regards to our evaluation: (1) a ranking accuracy that measures the overall ranking
is not appropriate, (2) the ranking accuracy metric should be able to factor in difference relevance
values for the different event types.

%TODO - Add some more cites

\subsection{Accuracy Metrics}

The Area Under Curve (AUC) is a popular classification accuracy metric. ROC curves provide a graphical
representation for the performance of a recommender system, by plotting the recall (true positive rate)
against the fallout (false positive rate) for increasing recommendation set size. A perfect recommender
would therefore yield a ROC curve that goes straight up towards 1.0 recall and 0.0 fallout until all
relevant items are retrieved. Afterwards it would go straight towards 1.0 fallout while the remaining
irrelevant items follow. One therefore obviously aims to maximize the area under the curve (AUC). The higher
up all relevant items (true positives) are in the recommendation list, the higher the AUC score.
AUC can therefore be used as a single measure for the overall quality of a recommender system. Table \ref{table:auc}
shows an example of the AUC values when varying the position of a single relevant document through the
recommendation list.

\begin{table}[H]
	\centering
	\begin{tabular}{*{12}l}
	\toprule
	\#Example	& R1 & R2 & R3 & R4 & R5 & R6 & R7 & R8 & R9 & R10 & AUC \\ \midrule
	1		& \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & 1.000 \\ 
	2		& \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & 0.889 \\ 
	3		& \xmark & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & 0.778 \\ 
	4		& \xmark & \xmark & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & 0.667 \\ 
	5		& \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & 0.556 \\ 
	6		& \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & 0.444 \\ 
	7		& \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark & \xmark & 0.333 \\ 
	8		& \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark & 0.222 \\ 
	9		& \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & 0.111 \\ 
	10		& \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & 0.000 \\
	\bottomrule
	\end{tabular}
	\caption{Varying the position of a single relevant item on a four out of ten recommendation list}
	\label{table:auc}
\end{table}

One should also be aware of that the AUC scores can be highly inaccurate, especially for cold-start users,
users which the recommender is unable to provide more than a handful of recommendations for. AUC requires
a total ordering of all items not yet rated by the user. This means that the items not recommendable
to the user will be appended in a random order after the known recommendations.
E.g. lets say the recommender is able to generate 10 recommendations for a user out of 4000 items.
The recommender was only able to retrieve one out of two relevant test items. The remaining 3990
items are therefor appended in random order to the users recommendation list.
Where the last test item is appended in the recommendation list can mean the difference between a AUC score of above
0.9 to less than 0.6 if the item is appended at the end of the list, the results can be even more
severe if none of the items is in the initial recommended list, then the results for that user will be
completely random. The most obvious solution to this problem would be to use resampling or to repeat
the experiment multiple times where the order of the items not in the recommendation list are drawn at random and
the results averaged over all the trials.

However, a frequently uttered point of criticism is that users are often more interested in the items
at the top of a recommendation list but that the AUC measure is equally affected by swaps at the top
or the bottom. To \emph{complement} AUC, we also wish to measure the rank accuracy to get a better
overall picture of the systems performance. This means that we want to measure whether or not
highly rated items such as purchases, likes etc. are ranked higher than less relevant items such as clicks,
as having these highly ranked items higher up in the recommendation list aligns with our previously
mentioned financial incentives.

%TODO - What is the "best" rank-accuracy metric?

\subsection{Rank accuracy}

\marginpar{TODO: Move to evaluation section?...}

When using rank accuracy metrics, it is worth knowing whether one is measuring total or partial orderings.
Most rank accuracy metrics (e.g. Kendall's tau and Spearman's rho) compare two total ordering. The problem
with these measures is that we in most cases are not provided with a full ranking of the items as most recommendation
algorithms only generate a partial list of items that are likely to be preferred by a user. The remaining items
would therefore have to be concatenated in random order. The recommendation list can also consist of several
items with similar rating that can appear in varying orders. Therefore, in order to create a full ranking of
the items all preference values for the user have to be known. Since the user can express the same rating for similar
items, the list will again contain groups of items that can appear in arbitrary order. However, the largest problem
is posed by items for which no rating is known. These items could hold an arbitrary place within the ranking.
Again, consider an example where a user e.g. have rated 5 items out of 4000, and the recommender is able to recommend
only 10 items for that user. To measure the rank accuracy we would have to randomly add 3995 items to the users known
preference list and 3990 to the users prediction list. Comparing the order of these lists would not make much sense.
The bottom line is that in most cases a rank metric for partial ordering would be more appropriate for comparing
recommendation lists that are produced by recommenders to item rankings from known user preferences.

We assume that we can recommend at most $k$ items for each user at a time. It also pays to submit all $k$
recommendations, because we are not penalized for bad guesses. We also assume that the order matters, so it
is better to submit more certain recommendations first, followed by recommendations we are less sure about.
Which means that we basically select the $k$ best candidates in order.
However, a problem is that the likeliness of finding a \emph{hidden} item in e.g. the top 20 recommended items
is not very large when one is working with large item catalogs. The problem then, is how to select the value of $k$.
Setting it to low you risk ending up with many $0.0$ values, and when setting it to high you risk including to many
\emph{randomly} concatenated ratings in the results. We believe a value of \textit{20} is natural choice for our
application as up to 20 recommendations can be shown in the user interface.

Mean average precision (MAP@k), described in Section \ref{subp:mean_average_precision_map_} is a popular metric for search
engines and is applied, for example, to report results at the Text Retrieval Conference (TREC). The main weakness with regard
to recommender systems is that it assumes that the user is interested in finding many relevant documents for each query, and
does not look at the relevance of the items. Consider the following examples where we have three lists of hidden items and
three recommendations lists. All none relevant items are labeled with the item-id $0$.

\begin{table}[H]
\label{table:ap}
\centering
\begin{tabular}{*{4}l}
\toprule
Example 	& 	Actual					& 	Recommended					&	AP@10   \\ \midrule
1			& [1,2,3,4,5,6,7,8,9,10]	&	[1,0,0,0,0,0,0,0,0,0]		&	0.100  \\ 
2			& [1,2,3,4,5,6,7,8,9,10]	&	[1,0,0,0,2,0,0,0,0,0]		&	0.140  \\ 
3			& [1,2,3,4,5,6,7,8,9,10]	&	[8,0,0,0,9,0,0,0,0,0]		&	0.140  \\
4			& [1,2,3,4,5,6,7,8,9,10]	&	[1,0,0,0,3,0,5,0,0,0]		&	0.183  \\ 
5			& [1,2,3,4,5,6,7,8,9,10]	&	[0,0,0,2,0,0,0,6,5,0]		&	0.083  \\ 
6			& [1,2,3,4,5,6,7,8,9,10]	&	[0,0,0,0,7,0,8,0,0,0]		&	0.049  \\
7			& [1,2,3,4,5,6,7,8,9,10]	&	[0,0,0,0,7,0,0,0,0,1]		&	0.040  \\ 
8			& [1,2,3,4,5,6,7,8,9,10]	&	[0,0,0,0,0,0,0,8,5,0]		&	0.035  \\ 
9			& [1,2,3,4,5,6,7,8,9,10]	&	[1,3,4,5,6,0,0,0,0,0]		&	0.500  \\
10			& [1,2,3,4,5,6,7,8,9,10]	&	[0,0,0,0,0,1,2,8,5,3]		&	0.178  \\ 
\bottomrule
\end{tabular}
\caption{AP\@10 Examples}
\end{table}

As you see from example two and three average precision does not consider the order of the actual item list. We want some way to
reward the recommender for getting the purchased ranked higher than both wants and clicks.


Normalized Discounted Cumulative Gain (nDCG) described in Section \ref{subp:normalized_discounted_cumulative_gain_} is another metric to
measure the rank accuracy. It is based on two main assumptions; (1) Highly relevant items are more useful than marginally relevant ones,
(2) The lower the ranked position of a relevant item, the less useful it is for the user, since it is less likely to be \emph{examined}.
The maybe most interesting aspect of nDCG is that it contains an utility function $rel_i$. One can replace the original utility function
and replace it with a function that is more suited to the application. One could e.g. assign the relevance of an item using
the following functions:

\begin{itemize}
\item $rel_i = 1/log(i+k)$, where $i$ is the index of the item in the actual sorted rating list, where $k$ is a constant.
\item $rel_i = u(i)$, simply assign a relevance or utility score such as e.g. the rating of the item or the event type score.
\end{itemize}

The latter would have been perfect, given that we could specify the importance of each event e.g. purchase gets a relevance of 3, want
a relevance of 2 and a click a relevance of 1, as it would give us a single number on the ranking quality of the recommendations.
However, we do not not know these values, meaning that nDCG cannot be used without making assumptions of the importance of the different
event types.

\marginpar{Mention why we think it is wrong to make these assumptions?}

To solve this problem we decided to chose a \emph{simpler} approach. We independently measure the recall and counts of clicks,
wants and purchases retrieved by the recommender in the top 20 positions. To check if an item is a click, want or purchase
we create a file which stores the log events in the following format:

\begin{table}[H]
\label{table:event-type}
\centering
\begin{tabular}{*{3}l}
\toprule
UserID	&	ItemID	 &  Event Type  \\ \midrule
11		&	21		 &	1			\\
11		&	101		 &	3			\\
13		&	213		 &	2			\\
13		&	202		 &  3			\\
15		&	202		 &  2			\\
15		&	21		 &  3			\\
\bottomrule
\end{tabular}
\caption{Evaluation - Example event type file}
\end{table}

For items which have multiple events for a given user we only use the most \emph{valuable} event. E.g. if a user have
both clicked and purchased and item we store the item as a purchase, giving it a value of 3. Clicks are given a value of 1 and wants are given a value of 2.

When evaluating the recommender we first count the occurrences of the different events before looking at the top 20 predictions to see how many of the
events in the different \emph{buckets} was retrieved. In addition we separately measure the MAP\@20 scores for the different
event types to give us some information about where in the top 20 the items are placed. Giving us the following columns:

\begin{itemize}
	\item $T_{click}$: Number of click events in the testset
	\item $T_{want}$: Number of test events in the testset
	\item $T_{purchase}$: Number of purchase events in the testset
	\item $P_{click}$: Number of click events retrieved in the top 20 prediction lists
	\item $P_{want}$: Number of want events retrieved in the top 20 prediction lists
	\item $P_{purchase}$: Number of purchase events retrieved in the top 20 prediction lists
	\item $R_{click}$: $P_{click}$ / $T_{click}$
	\item $R_{want}$: $P_{want}$ / $T_{want}$
	\item $R_{purchase}$: $P_{purchase}$ / $T_{purchase}$
	\item $MAP@20_{click}$: $MAP@20$ considering only click events as relevant items
	\item $MAP@20_{want}$: $MAP@20$ considering only want events as relevant items
	\item $MAP@20_{purchase}$: $MAP@20$ considering only purchase events as relevant items
\end{itemize}

Consider the following example where we have split our dataset into a training and test set and generated a set
of predictions for each user. The following table shows our test set:

\begin{table}[H]
\centering
	\begin{tabular}{*{3}l}
	\toprule
	UserID	&	ItemID	 &  Rating  	\\ \midrule
	11		&	21		 &	2			\\
	11		&	101		 &	5			\\
	13		&	213		 &	4			\\
	13		&	202		 &  3			\\
	15		&	202		 &  5			\\
	15		&	21		 &  2			\\
	\bottomrule
	\end{tabular}
	\caption{Evaluation - Test file example}
\end{table}

E.g. we can see that we have an interaction between user 11 and item 21 in our testset and from our event file in
Table \ref{table:event-type} we can see that the user clicked this item. Similarly we can see that user 15 purchased
item 21 from our event file. Giving us the following event distribution of our test set: $T_{click}$ = 1, $T_{want}$ = 2 and
$T_{purchase}$ = 3. Our recommender system then outputs the a ordered list of predictions for each user. The retrieved user-item touples matching the test set have been highlighted:

\begin{table}[H]
\centering
	\begin{tabular}{*{3}l}
	\toprule
	UserID	&	ItemID	 &  Rating  	\\ \midrule
	\rowcolor{Gray}
	11		&	21		 &	2			\\
	11		&	107		 &	5			\\
	11		&	123		 &	5			\\
	\rowcolor{Gray}
	13		&	213		 &	4			\\
	13		&	1022	 &  3			\\
	15		&	1202	 &  5			\\
	\rowcolor{Gray}
	15		&	21		 &  2			\\
\bottomrule
\end{tabular}
\caption{Evaluation - Example prediction file}
\end{table}


We then check each user-item touple against our test file, if it is a match we retrieve the event type for the predictions.  Giving us the following $P$ values: $P_{click}$ = 1, $P_{want}$ = 1 and $P_{purchase}$ = 1. Meaning that we retrieved one of each the item types, giving us the following recall score for the different event type: $P_{click}$ = 1/1, $P_{want}$ = 1/2 and $P_{purchase}$ = 1/3. We can then calculate the different $MAP@20$ scores. E.g. for user 11 the first item in his ranked list is a click meaning that his $AP@20_{click}$ score would be higher than if the item would have been the third item in his list. The $MAP@20$ numbers can therefore be used as a measure of where in the top 20 lists the different events can be found. E.g. a high $MAP@20_{purchase}$ score would indicate that the retrieved purchase events
normally are found high up in the recommendation list and vica versa.
 


We believe that having the raw counts coupled with the true-postive rate for the different event type and a measure of their ranking quality will be more informative and give us a clearer overview of the systems performance. Given that we have known the relevance of the different event types nDCG could have given us all these numbers summarized in one column, but instead we now have twelve.



%Normalized Discounted Cumulative Gain ($nDCG_{k}$), described in Section \ref{subp:normalized_discounted_cumulative_gain_}
% 
%
%For our application we believe it to be beneficial to use the rating of the item as a
%relevance score, as we could have multiple items with highly similar ratings (thus the same relevance). When using a low $k$ value for the logarithmic alternative this could mean that the relevance between two almost similarly rated items might end up being disproportionately large. We will also most likely look at the 20 top items, which means that the difference between the lower ranked items will be fairly small, as the logarithmic function converges. A third alternative would be to also use some linear decay function. The following table shows the $nDCG$ score for a set of examples where the function $rel_i = 1/log(i+1)$ is used to assign the relevance score to item $i$ in the sorted list of actual ratings. E.g. item one is assigned a relevance score of $1/(log(2)$ and so on. We hope the following table will give the reader a better idea of how $nDCG$ \emph{works}.
%
%\begin{table}[H]
%\label{table:ndcg}
%\centering
%\begin{tabular}{*{4}l}
%\toprule
%Example 	& 	Actual					& 	Recommended				&	nDCG 	\\ \midrule
%1			& 	[1,2,3,4]				&	[1,0,0,0]				&	0.470   \\ 
%2			& 	[1,2,3,4]				&	[2,0,0,0]				&	0.360   \\ 
%3			& 	[1,2,3,4]				&	[3,0,0,0]				&	0.330   \\ 
%4			& 	[1,2,3,4,5,6,7,8,9,10] 	& 	[1,2,3,4,5,6,7,8,9,10] 	&   1.000	\\ 
%5			& 	[1,2,3,4,5,6,7,8,9,10] 	& 	[10,9,8,7,6,5,4,3,2,1] 	&   0.900	\\ 
%6			&	[1,2,3,4,5,6,7,8,9,10] 	& 	[1,2,0,0,0,0,0,0,0,0] 	&   0.440	\\ 
%7			& 	[1,2,3,4,5,6,7,8,9,10] 	& 	[1,0,0,3,0,0,0,0,0,0] 	&   0.390	\\ 
%8			& 	[1,2,3,4,5,6,7,8,9,10] 	& 	[4,0,0,0,8,0,0,0,0,0] 	&   0.280	\\ 
%9			& 	[1,2,3,4,5,6,7,8,9,10] 	& 	[0,0,0,0,5,0,0,0,0,10] 	&   0.130	\\ 
%10			& 	[1,2,3,4,5,6,7,8,9,10] 	& 	[0,0,0,0,0,0,0,0,9,10]	&   0.110	\\
%\bottomrule
%\end{tabular}
%\caption{nDCG Test Examples}
%\end{table}
%
%As you can see from the above mentioned examples nDCG will give us an indication whether or not our recommender
%ranks \emph{highly relevant} items above those who are less relevant. As for the limitations, nDCG is designed for situations of non-binary notions of relevance, thus it cannot be used in our experiment where we wish to compare binary ratings with implicit ratings. The main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial relevance feedback is available. This is often the case when we have several equally good results. This is especially true when the metric is limited to only first few results as it is often done in practice.
%
%We chose take a closer look at the different ways of calculating nDCG. There are two commonly used methods used to calculate the DCG and IDCG scores.
%
%\begin{itemize}
%\item Method 1  $DCG_p = rel_1 + \sum_{i=2}^{p} \frac{rel_i}{log_2(i)}$
%\item Method 2: $DCG_p = \sum_{i=1}^{p} \frac{2^{rel_i}-1}{log_2(i+1})$
%\end{itemize}
%
%The following table shows the differences between Method 1 and Method 2 on a few selected examples. Here we set the value of $rel_i$ equal to the implicit rating of item $i$.
%
%\begin{table}[H]
%\label{table:ndcgfinal}
%\centering
%\begin{tabular}{*{5}l}
%\toprule
%\#Ex & 	Actual																		& 	Recommended				&	$nDCG^1$  	   & $nDCG^2$	\\ \midrule
%1 		& 	[$1_{5}, 2_{3.8}, 3_{3.55}, 4_{2.78},5_{1.3}$]								&	[5,0,0,0,0]				&	0.113 		   & 0.026  \\ 
%2 		& 	[$1_{5}, 2_{3.8}, 3_{3.55}, 4_{2.78},5_{1.3}$]								&	[0,0,1,0,0]				&	0.217 		   & 0.275   \\ 
%3   	& 	[$1_{5},2_{4.8}, 3_{3.55}, 4_{2.78}, 5_{2.2}, 6_{2.0},7_{1.77},8_{1.3}$]	&	[2,3,4,5,6,7,8,0]		&	0.827		   & 0.682   \\ 
%4  	& 	[$1_{5},2_{4.8}, 3_{3.55}, 4_{2.78}, 5_{2.2}, 6_{2.0},7_{1.77},8_{1.3}$]	&	[1,2,0,0,0,0,0,0]		&	0.592 		   & 0.805   \\
%5 		& 	[$1_{5},2_{4.93},3_{2.88}, 4_{1.85},5_{1.8},6_{1.77}, 7_{1.63},8_{1.52}$]	&	[1,0,0,0,0,0,0,0]		&	0.394 		   & 0.544   \\
%6 		& 	[$1_{5},2_{4.93},3_{2.88}, 4_{1.85},5_{1.8},6_{1.77}, 7_{1.63},8_{1.52}$]	&	[3,4,5,6,7,8,0,0]		&	0.542 		   & 0.206   \\
%\bottomrule
%\end{tabular}
%\caption{Comparison of nDCG scores for different methods of computing DCG. Each item in the Actual list is on the form $ItemId_{Rating}$, the first item in the actual list of example one therefore has en ItemId of 1 and a Rating of 5. All non relevant items are given the ItemId 0 in the Recommended list.}
%\end{table}
%
%As you can see Method 1 is a bit more \emph{balanced} than Method 2, as it does not give as much weight to highly rated items. Especially example 5 and 6 highlights this as Method 2 prefers retrieving one highly rated item over retrieving multiple less relevant ones. Method 2 is ultimately closer to what we want in addition to being commonly used by web search companies and on the data science competition platform Kaggle \cite{kaggle}. We have also chosen to truncate the recommendation list and only calculate the nDCG of the $k$ top items. Because nDCG has a relatively low positional discount, it allows us to set $k$ fairly high, but this only makes sense if we believe that the user will read large portions of the recommendation list. 
%
%\marginpar{TODO: Spend any more time looking at partial order ranking measures?...}
%
%%TODO - Select K value based on user interface
%%TODO - Justify why these metrics combined will give a good overview of the performance.
%%TODO - Cold-start evaluation Metrics

\subsection{Cold-Start Metrics}

In addition to looking at the above mentioned metrics it would be interesting to see how
the different sparsity levels affect both the user- and item-space coverage of the different
methods when evaluating the cold-start system performance, as having a recommender that can only
recommend a limited set of items to a small portion of the users is \emph{unfortunate}. The
user-space coverage is the number of users the recommender is able to produce recommendations for while the item-space
coverage is measured by looking at how many of the items are recommendable.
