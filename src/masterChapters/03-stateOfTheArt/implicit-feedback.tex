% !TEX root = ../../report.tex

\clearpage

\section{Implicit Feedback to Implicit Ratings}
\label{sec:implicit}

In this section, in accordance to our research goals (\textbf{Goal G6 and G7}),
we will explore several \textit{existing} methods for solving the following
formalized problem:

\vspace{5 mm}

\noindent \textit{Input:} A dataset with \textbf{implicit feedback} containing event logs
with the following information available: event type, product id, user id and
timestamp of the event.

\vspace{3 mm}

\noindent \textit{Output:} For every user $u$, a rating between 1 and $k$ on every item that
the user has interacted with. These ratings are called \textbf{implicit
ratings}.

\vspace{5 mm}

This problem definition is further used in
Section~\ref{sec:implementation-implicit} where multiple ways of quantification
of the SoBazaar dataset are presented based on ideas found in this section.
First however, before delving into specific methods, a deeper understanding on
the relationship between implicit and explicit feedback is needed.

Explicit feedback is present in many of the largest recommender systems today
and hence, is extensively researched \cite{Adomavicius2005}. The user is
commonly asked to rate item $i$ on a Likert scale from $1$ to $k$, ranging from
strongly disagree to strongly agree with an item. Its advantages are, among
others the ability to get precise feedback from the user and capturing both
positive and negative preferences. However, although having a high popularity,
the method has multiple weaknesses. The most prominent weakness is the
difficulty of collecting ratings: the method requires the user to spend time
rating items and the amount of feedback is often scarce, creating sparse data
sets. Further, explicit ratings are often subject to inconsistencies known as
natural noise~\cite{amatriain2009like} and users might also be pressed to
report different preferences due to peer pressure \cite{bell2007scalable}. The
fact that we are introducing a user overhead, makes it difficult to have a
complete view on the user preferences~\cite{Jawaheer2010}.

There are also practical reasons for not having explicit feedback in form of
ratings, dependent on how a website/application interacts and works with its
customers. One of the most popular recommender sites in the world, Netflix, ask
their users to rate items after consumation. However, this process does not
\textit{cost} anything for the user, and the possibility to rate is included in
the monthly subscription price, whilst in a webstore rating an item often
require the user to first buy it, consume it and finally rate it.

Implicit feedback on the other hand utilizes, often already collected,
analytics data and hence does not require any extra effort neither for the user
nor the frontend-developers. This creates a more frictionless experience for
the user, but can also yield better recommender results since often the event
log is less sparse than rating datasets and contains implicit knowledge which,
by only using explicit information, would never be exposed in a traditional
system.

It is important to understand that there are several fundemental differences
between implicit and explicit ratings, in everything from \textit{meaning} to
\textit{evaluation techniques}. Partly inspired by Hu et al.~\cite{Hu2008}, we
can identify the following key charecterstics and differences between them:

\vspace{3 mm}

\textbf{Explicit ratings:}
\begin{itemize}
\item Contains both positive and negative feedback. In other words, a user is
able to explicitly tell the system that he/she does not like an item, as well
as the opposite.
\item Indicates preference, often on a Likert scale (or similar), where scores
range from total dislike to high satisfaction with an item.
\item There is medium noisiness in the data, but the amount is highly dependent
on the domain in question~\cite{amatriain2009like}. With little noisiness we
achieve more precise ratings.
\item Metrics in use are commonly RMSE, MAE or other evaluation schemes where
we consider how well we match a test-set.
\item Both evaluation schemes and recommendation techniques using explicit
feedback is heavily researched and used. Many of these are easy to implement,
and multiple open-source tools and projects are at researchers and developers
disposal~\cite{mahout}~\cite{Gantner2011MyMediaLite}.
\end{itemize}

\textbf{Implicit ratings:}
\begin{itemize}
\item Usually only contain positive feedback, since we base our models on user
activity on an item representing something good. However, looking at metrics
such as bounce rates so forth, negative feedback can be produced with some
effort.
\item Indicates confidence, that is a recurring event is more likely to reflect
the user opinion, but not necessarily his/hers preference.
\item There is a high degree of noise and this is one of the main challenges
with implicit rating and its precision. Researchers and developers need to be
sure to select correct set of events and session-metrics to base implicit
ratings on.
\item As users do not provide numerical scores, a precision-recall evaluation
scheme is often preffered to more preference-based metrics.
\item Strengths include not requiring extra feedback for the user, having less
sparsity and catching actual behaviour in the application. Less influenced by
peer pressure and so forth.
\end{itemize}

Notice that several events such as purchasing an item may be considered as
explicitly giving positive feedback, but as we argue in this paper all events
in the SoBazaar dataset (including purchases) are treated as implicit feedback.
The reasoning behind this lies in the fact that purchasing, in contrast to
positivly rating, an item happens before actual consumption. Hence the user may
not like the item in question, it may be a gift for another person or a variety
of other reasons. Whilst in the scenerio of rating, the user has already
consumed it and is explicitly answering the question \textit{«To which degree
did you like it?»}.

\clearpage
\subsection{Binary implicit ratings}

One of the most common techniques of quantifying implicit feedback to implicit
ratings is to use binary ratings - that is if a user has shown any interest in
an item we give that item a weight of 1, similarily all items the user has not
shown interest in (e.g. not clicked on) we give a rating of 0. This method was
introduced and used by Hu et al.~\cite{Hu2008} where they formalized the
notion of confidence which the $r_{ui}$ variables measure, e.g. if a user $u$
had clicked on an item $i$ 2 times it would translate to the score $r_{ui} =
2.0$. This was further converted to a $p_{ui}$ value derived by binarizing the
$r_{ui}$ values:

\begin{equation}
  p_{ui} =
  \begin{cases}
    1 & \text{if $r_{ui} > 0$} \\
    0 & \text{if $r_{ui} = 0$}
  \end{cases}
\end{equation}

As mentioned earlier in this section, and as it is also noted in the original
research, when we have implicit rather than explicit feedback there may be
multiple reasons beyond preference that decide consumation of an item. It may
be due to the items availability or price; the users knowledge of the item
caused by e.g. the user interface or one of many other factors different from
the user preferring or disliking the item. To remidate this they introduce a
variable $c_{ui}$ which measure the confidence in observing $p_{ui}$, where
their main hypothesis is that as the $r_{ui}$ grows the confidence is stronger.
Hence, a plausible choice for $c_{ui}$ would be:

\begin{equation}
c_{ui} = 1 + \alpha r_{ui}
\end{equation}

Where $\alpha$ controls the rate of increase, and is set to $40$ in their
experiments as this produced good results. They now use these two scores
($p_{ui}$ and $c_{ui}$) in parallel when finding latent factors and training
their model. In short they find both a user and item vector ($x_u$ and $y_i$,
respectively) where the preference is assumed to be their inner product: $p_{ui}
= x_{u}^{T} y_{i}$. Notice as well how this method stand out from similar
matrix factorization methods (see Section~\ref{sec:model-based-methods}) by
accounting for varying confidence levels:

\begin{equation}
  min_{x,y} \sum _{u,i} c_{ui} (p_{ui} - x_{u}^{T} y_i)^2 + \lambda (\sum _{u}
  || x_u ||^2 + \sum_{i} || y_i ||^2)
\end{equation}

Where the $\lambda (\sum _{u} || x_u ||^2 + \sum_{i} || y_i ||^2)$ term is
necessary for regularizing the model, so that is will not overfit the
training data. $\lambda$ is highly data dependent and should be set by
cross-validation.

However, notice the primary term where the difference between the binary weight
and our user/item-models are minimized and perhaps more importantly multiplied
by our confidence. As the details on this topic are quite detailed and diverges
a bit from the generation of implicit ratings, we will stop our analysis of it
here. But is shows an important aspect which we will return to in subsequent
sections, namely the confidence in repetitive events and our continuous
challenge to obtain higher confidence in our implicit ratings. For further
information on matrix factorization and latent factors refer to
Section~\ref{sec:model-based-methods} or to Hu et al.\ original
research~\cite{Hu2008}.

\subsection{Implicit ratings for binary domains}
\label{implicit-binary-domains}

In the previous sub-section we saw how $r_{ui}$, a score for user $u$ on item
$i$, was set based on e.g. number of clicks but perhaps more commonly the
duration or amount of which the user has consumed the item. This way, in a
domain such as television a user would obtain $r_{ui} = 1.0$ if the whole
programme was watched, and similarily $r_{ui} = 0.5$ if half of the show was
watched, and so forth. Now, this works well in domains where actions on an item
are continious, but in many domains such as a e-commerce store we mostly have
binary events such as clicks or purchases. One scheme, proposed by
~\cite{pkghost2014implicit} is to differentiate between different events and by
weighting these by importance or relevence we can create ratings using the
whole scale between 0.0 and 1.0.

This is just one example of how the domain and data available affects our
methods, techniques and reliability to our predicted numbers. This is common
theme when dealing with implicit ratings, and it is an important aspect that
the reader should be aware of and keep in the back of his/hers mind. Now, in a
proposed e-commerce domain where our data is based on web-access logs we may
have the following events to choose from:

\begin{table}[H]
  \centering
  \begin{tabular}{ll}
  \toprule
  Event type & Description \\ \midrule
  0 & Item purchased \\
  1 & Item placed in shopping cart \\
  2 & Item placed in wish list \\
  3 & Item browsed based on search result \\
  4 & Item browsed \\
  \bottomrule
  \end{tabular}
\end{table}

Then, our heuristic would count the frequency of each event for an item $i$ on
user $u$. However, just counting is not a bounded function --- so if one give
\textit{Item browsed} the weight of $1$ and the \textit{Item purchased} event
a weight of $100$, then a user browsing an item more than 100 times would get a
higher implicit rating than a user buying it.

Instead,~\cite{pkghost2014implicit} proposes using a global rating mapping,
that uses \textit{levels of frequency}.

\begin{table}[H]
  \centering
  \begin{tabular}{ll}
  \toprule
  Event type & Scores \\ \midrule
  0 & 100 \\
  1 & 70, 77, 80 \\
  2 & 30, 40, 45, 48, 50 \\
  3 & 20, 25, 28, 30 \\
  4 & 10, 15, 18, 20 \\
  \bottomrule
  \end{tabular}
  \caption{Scores per event type, increases as frequency of each event
           increments}
\label{implicit-table}
\end{table}

Now, a user browsing an item $i$ two times, would get a score of 40 and a if
buying it the score would be 100 (e.g.\ the event type with highest interest
level supersede all others). Equally, if a user browsed the item 100 times, it
would make no difference in score compared to browsing it 4 times. The scores
given to the various event types may vary from what kind of dataset one have,
the domain and should be evaluated using one or more of the methods mentioned
in Section~\ref{evaluation}.

If we wanted ratings between 1 and $k$ one can transform the score $s$ provided
by table~\ref{implicit-table}:

\begin{equation}
  ImplicitRating(s, k) = 100 * \frac{k-1}{100} + 1
\end{equation}

The advantage of this heuristic is that it requires no training, it is simple
to understand and works reasonably well, if weights are chosen correctly.
The latter is also its largest weakness --- as finding these weights may be
difficult. If one had explicit feedback, as well as the implicit, one could
train a model automatically choosing weights. Further, when using the scores
defined as above we only use a small percentage of the scale 0-100. This leads
to problems when a user triggers one type of event many times more than
another, as both would reach the highest score of the given event type, but
there would be no difference in scoring even though they had very different
activity. We would like a scheme where we use the whole scale of scores and
where there were no global ceiling, but rather let the scoring be normalized to
the number of events for the user in question.

Improvements to this model are presented in
Section~\ref{sec:implementation-implicit}

\subsection{Regression analysis}
\label{sec:regression-sota}

\paragraph{Introduction to linear regression}
A different method for converting events to ratings is finding a mathematical
model which fits our observed data. Using this model we can predict new ratings
by extrapolation or intrapolaton. Our observed data can be in N-dimensions, but
in the figure below we consider two dimensions of observed properties, in which
the goal is to find a regression line matching the data points with minimal
errors. An example regression line matching five data points is provided below:

\begin{figure}[H]
  \centering
  \label{fig-regression}
  \begin{tikzpicture}
    \begin{axis}[
        xlabel=Feature 1,
        ylabel=Rating,
        ymax=5.5, ymin=0,
        xmax=35, xmin=0,
        ymajorgrids=true,
    ]
    \addplot[only marks]table[]{
      1 1
      2 2
      10 3
      25 4
      30 5
    };
    \addplot [dashed, color=red, domain=-3:35, samples=40] {0.114861*x + 1.437889};
    \end{axis}
  \end{tikzpicture}
  \caption{Regression line fitting the 5 observed datapoints}
\end{figure}

In this example we consider the relationship between one feature (the
independent variable) and the rating (ground truth/dependent variable), using
five observed datapoints. We can then, given a feature-value, predict future
ratings. The process of finding the regression line is better generalized by
solving the following equation, where we are given a number of independent and
dependent values:

\begin{equation}
  \label{eq-regression}
  \hat{y}(w,x) = w_0 + w_1 x_1 + \cdots + w_p x_p
\end{equation}

where we want to find the values of the $w$-parameters, also called the unknown
parameters. This is done by using multipe known values for $x$ and $y$, the
independent and dependent variables respectivly. In Figure \ref{fig-regression}
we have the following datapoints:

\begin{table}[H]
  \centering
  \begin{tabular}{ll}
  \toprule
  Feature-value (x) & Rating (y) \\
  \midrule
   1 & 1 \\
   2  & 2 \\
   10  & 3 \\
   25 & 4 \\
   30  & 5 \\
   \bottomrule
  \end{tabular}
\end{table}

Which, solving Equation~\ref{eq-regression}, five different equations are
used with the unknown parameters $w_0$ and $w_1$:

\begin{equation}
  \label{eqs-regression-example}
  \begin{split}
    1 = w_0 + 1 w_1 \\
    2 = w_0 + 2 w_1 \\
    3 = w_0 + 10 w_1 \\
    4 = w_0 + 25 w_1 \\
    5 = w_0 + 30 w_1
  \end{split}
\end{equation}

Using a minimization method called \textit{Least squares} we try to minimize
the residual $e = y_i - w^{T}x_{i}$, using sum of squares between the observed
values in the dataset and the values predicted by the linear approximation. In
other words we want a minimal distance between all observed datapoints the the
final regression line. Mathematically it solved the problem of the following
form:

\begin{equation}
  \label{least-squares}
  \hat{w} = \argmin_w \sum_{i} (y - w^{T}x_{i})^2
\end{equation}

Carrying out this minimization for our equations~\ref{eqs-regression-example},
we obtain an intercept value ($w_0$) of $1.4379$ and a slope ($w_1$) of
$0.1149$, which matches the regression line plotted in
Figure~\ref{fig-regression}. Predicting a new rating based on a feature value
in now very simple - as an example we assume a new independent value is
provided with the value 15 and an unknown rating, that is $x=15$ and we can
solve the equation $y = 0.1149 \cdot 15 + 1.4379$ which yields the $y$-value or
rating $3.1614$.

\paragraph{Using regression analysis on implicit feedback}
The process of using regression analysis in order to predict ratings based on
implicit feedback has already been done, in a different domain, by Parra et
al.~\cite{parra2011walk}. Where they first did a quantitiative user study
asking 114 active users of the music service Last.fm to rate items which was
found in their activity history. These ratings were used as dependent variables
and then using various schemes described below they mapped these to the
independent variables. Now, a key point in their study is the fact that all
models found used explicit feedback from a user study as a \textit{ground
truth}. The necessity of such a ground truth is the largest weakness in terms
of regression analysis on implicit datasets - as it is per definition
non-existent. A possibility is to give various events scores based on
importance in the system, with e.g. a purchase having top-score and clicking on
an item a low score. This is discussed further in Section~\ref{} where an
implimentaiton and evaluation of such a design is presented as well.

The study looked at three different features for each item, found in the
dataset:

\begin{itemize}
  \item User activity on the item
  \item Global popularity of the item
  \item How recent the user had acitivity on the item
\end{itemize}

For each feature all items are categorized into three buckets, so that items
are equally distributed across one feature. Thus bucket one for the feature of
global popularity contains items with low popularity and bucket three those
with the highest popularity, and so forth. The main reason for having this
sampling strategy is to create a homogenous set of items where outliers are
accounted for, and we are less prone to over or underfitting.

Four models are proposed, utilizing the features above and the ratings provided
in the user study:

\begin{equation}
  \begin{aligned}
    & r_{iu} = w_0 + w_1 if_{iu} \\
    & r_{iu} = w_0 + w_1 if_{iu} + w_2 re_{iu} \\
    & r_{iu} = w_0 + w_1 if_{iu} + w_2 re_{iu} + w_3 gp_{i} \\
    & r_{iu} = w_0 + w_1 if_{iu} + w_2 re_{iu} + w_3 if_{iu} \cdot re_{iu}
  \end{aligned}
\end{equation}

where every independent variable ($if_{iu}$, $re_{iu}$ and $gp_{i}$) is the
bucket number between 1 and 3. $if_{iu}$ indicates how many times user $u$ used
item $i$, $re_{iu}$ how recently user $u$ used item $i$ and $gp_{i}$ the global
popularity of item $i$.

Evaluating a regression model is done by confirming the \textit{goodness of
fit}, and the most common metric is to calculate the $R^2$ which based on
residual values in Equation~\ref{least-squares} yields a number between 0 and
1, indicating a bad or good fit, respectivly. By accounting for recentness in
their regression model (model 2\-4) they achieved an improvement in the $R^2$
value by \textit{10\%}.

Summarizing, the following requirements are needed in order to perform
regression analysis on implicit feedback:

\begin{itemize}
  \item A ground-truth score.
  \item A set of numeric features.
  \item Each feature have to be transitive, that is a linear relationship in
  the values have to exist.
\end{itemize}

The fact that the features have to be transitive implies that one can not use
e.g. \textit{item category} as training feature, unless the brands (or rather
the brand ids) themselves are sorted by price or equality, then renumbered.

\paragraph{Multivariate ordinal regression}
In~\cite{parra2011implicit} Parra et al. extend the model presented
in~\cite{parra2011walk} by considering that the dependent variable (rating) is
on an ordinal scale, that ranges from 1 to 5. A ordinal variable implies that
the numeric value has no significance beyond its ability to establish a ranking
over a set of data points. When solving regression problems on an ordinal scale
a logistic function is commonly used. Further since there exists more than two
outcomes (1 to 5) they use a multinomial regression type.

In \textit{multinomial logistic regression} problems such as this, the link
function (logistic function, also called \textit{logit}) is trained based on
features found in the data. In the refered study they examine the music domain
and hence they include listen count, recentness and the global popularity as
features to the link function.

When trained they have a probability function for all discrete outcomes and can
thus based on model input obtain a value between 0 and 1, indicating the
probability of the enquired outcome. The final rating is then calculated as:

\begin{equation}
  E[r_{ui}] = \sum_{k=1}^{5} k \cdot P(r_{ui} = k)
\end{equation}

where $u$ and $i$ is the user and item in question, respectivly. $k$ is the
ordinal variable between 1 and 5, and $P(r_{ui})$ yields the probability of $k$
being the rating for item $i$ on user $u$.

This model yields an improvement in performence both measured in MAP and
nDCG-scores (see Section~\ref{}), compared to their previous model. However,
when they discuss the accuracy of their evaluation metrics a conclusion is made
which is similar to what is laid out in this thesis as well:

\begin{quotation}
  [\dots], we do agree that there is no appropriate evaluation approaches for
  implicit feedback.
\end{quotation}

Their proposed method for evaluation is to compare the generated implicit
ratings to ones given in a parallel explicit scenario. However, in many
situations where the two techniques are not combined, we use implicit ratings
due to the lack of explicit ones. Consequently, without any ground truth (that
is, explicit ratings) both \textit{training a regression model} and
\textit{evaluating} the generated implicit ratings becomes difficult. If one
wants to reliably use these two methods a user study should be conducted, which
was not performed in the case of SoBazaar due to time and user constraints.

\subsection{Relative preferences using buying frequency}
A hybrid approach using sequential pattern analysis and collaborative filtering
techniques is presented by Choi et al.~\cite{choi2012hybrid}. In their
method, coined HOPE, they calculate an implicit rating by finding the
absolute preference $AP$ from users and items. These preferences are further
used to find the relative preference which finally are normalized into an
implicit rating. Once the implicit ratings are derived they calculate
similarity scores and find K-nearest neighbours in a traditional fashion.
Finally these neighbours are used in order to calculate a CF-based predicted
preference (CFPP). This score is integrated with a Sequential Pattern
Analysis-based predicted preference (SPAPP) which is derived from matching
subsequences of common sequential patterns between users. This process is
summarized in Figure~\ref{hope-system}.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{image/hope-system}
  \caption{Overall framework of HOPE system, calculating implicit ratings and
  combining these with sequential pattern analysis}
  \label{hope-system}
\end{figure}

We will continue this section focusing on the convertion from transaction data
into implicit ratings, rather than going in depth into sequential pattern
analysis and calculating the CFPP and SPAPP-scores, as the interested reader
rather should consult the original research for a indepth discussion. When
calculating the $AP$ score we focus solely on the purchase data of the user
$u$, but it can easily be extended to other types of events. However, the score
does makes more sense when using it on domains having a high degree of repeated
actions - such as e-commerce stores selling convienience products with short
life spans or a music service having users listening to a song multiple times.
The absolute preference is calculated as follows:

\begin{equation}
  AP(u,i) = \ln(\frac{trans(u,i)}{\sum_{e \in E}{trans(u, e)}} + 1)
\end{equation}

where $trans(u,i)$ is the number of transactions for user $u$ on item $i$, and
E is the set of all items in our system. As an example if a user $u$ purchases an
item $i$ four times out of ten transactions the $AP(u,i) = \ln(1.4)$, but another
user $p$ who has bought the same item one time out of one transaction the
$AP(p,i) = \ln(2.0)$, and as one can see we should consider a relative preference so
that repeated actions are rewarded. Further, the absolute preference only takes
into account the frequency of purchases and because the frequency is heavily
dependent on price, item category and lifespan of an item — so in the original
research they propose the following equation in order to calcualte the relative
preference@:

\begin{equation}
  RP(u,i) = \frac{AP(u,i)}{\max_{c \in U}(AP(c,i))}
\end{equation}

where $U$ denotes every user who purchased item $i$. The reason for using a
maximization function, is to make $RP(u,i)$ range from $0.0$ to $1.0$ (i.e.\
normalization) and one can thus find a rating on a common Likert scale by
multiplying with $k$:

\begin{equation}
  ImplicitRating(u,i) = \lceil k * RP(u,i) \rceil
\end{equation}

Here we round up in order to range from 1 to 5, but one could either round down
instead and have ratings range from 0 to 4 or one could use the normalization
equation where we based on minimal and maximal values in an interval $x_{min}$
and $x_{max}$ shift all values to a new interval $a$ and $b$, keeping the
ratios between items. Using this equation we can normalize any $x$ defined in
the interval, that is $x_{min} \leq x \leq x_{max}$:

\begin{equation}
  \label{eq-normalization}
  N(x, a, b, x_{min}, x_{max}) = a + \frac{(x-x_{min})(b-a)}{x_{max}-x_{min}}
\end{equation}

In order to normalize $ImplicitRating(u,i)$ we set $x_{min} = 0$ and $x_{max} =
1$ and normalize to the new interval $a=1$ and $b=5$.

\subsection{Challenges and weaknesses}
\label{implicit-weaknesses}

At this point we have looked at multiple existing methods for creating implicit
ratings based on implicit feedback, proving both their extensibility and
numerious variations. However, there are some challanges and weaknesses that
holds true for all methods. We will take great care to explain these, but in
addition describe the multiple ways of counteracting or minimizing their
impact as well. The two largest challenges in regards to generating implicit
ratings are:

% Goal G9: Find metrics in order to evaluate the implicit ratings.

\begin{itemize}
  \item A lack of ground truth.
  \item No negative feedback.
\end{itemize}

% First challenge, the lack of ground truth.
As mentioned in Section~\ref{regression-sota}, when discussing regression
analysis, without a ground truth both modelling and evaluating becomes a
inaccurate science based on assumptions. Some assumptions may very well hold
true, as proved by in~\cite{parra-2011walk}, however without conducting user
studies or evaluating the implicit ratings in an online environment using
conversion metrics such as an increase in purchases, we can not reliably
evaluate our results. We can thus calculate and compare the performance of one
set of implicit ratings compared to another set of implicit ratings, but only
based on how well they \textit{match our assumptions} and selected features,
not their real world performance. This in many answers, but also make it
irrealizable given our data, to fulfill our research goal concering finding
good metrics for evaluating the implicit ratings (\textbf{Goal G9}) as we
neither have a ground truth, nor a set of explicit ratings from the
application.

% Second challenge, no negative feedback.
The second challenge is a result of almost all research on implicit feedback
focusing on how implicit behaviours can be used as positive evidence in a
recommender system. Without any negative feedback direct comparisons to
explicit ratings become troublesome, as an implicit rating of 2 does not mean
\textit{dissatisfaction}, but instead only \textit{less} satisfaction than 3.
By adding negative events to implicit rating generation such as \textit{bounce
rates} or \textit{deleting/hiding} elements from a news feed, may improve both
the accuracy of the ratings, but also make them more robust when comparing and
combined with explicit ratings. Still, dependent on the domain, these negative
implicit feedbacks are often fewer in number and demand a larger effort to
extract.

% Other challenges; making too many generalizations on user behaviours.
There are many other smaller challanges when generating implicit ratings, one
of them concerning making too many generalization rules for user behaviour. In
particular, users bahave differently and have varying approaches to information
seeking, thus it becomes difficult to generate and dangerous to apply
all-purpose rules for describing their behaviour. This way, induvidual
differences can greatly impact the effectiveness of analyzing user behaviours,
which is at the core of our assumptions.

% Weaknesses propagate, and recommender+evaluation is based on impl. feedb.
When we create implicit ratings, these become the recommender system equivalent
of a ground truth. This data is further used, as we have seen in
Section~\ref{subsec:cf} as input to our recommender algorithm makes which makes
creates a model and later gives predictions on how well new data fits this
model. In the end we use the output from this model (the predictions) for
evaluation, determining the quality of how well the predictions were. However,
notice that if all input to creting the \textit{recommender model} were
unaccurate or wrong, then this would affect all later stages in the
\textit{recommender pipeline}. This is generally not a concern in explicit
ratings, after all the ground truth is the \textit{truth}, but when considering
implicit ratings it can instead be called the \textit{ground assumptions} and
if wrong, all later stages become unsound as well. Furthermore, as briefly
touched upon earlier, without much negative feedback our later stages in the
pipeline need to take height for this, not penalizing a low ranking as much as
in traditional settings --- as having some feedback is better than none. In
essence, we need to ensure that two factors are emperically chosen: first, all
assumptions should be grounded in statistical evidence and second, recommender
algorithms and evaluation schemes should take height for weak evidence of
negative feedback.

% We need in-depth understanding of the domain.
A consequence of using implicit ratings is therefore to statistically select
features that yields a good classification of user behaviours. In addition,
we need to consider the sparsity of the dataset and dependent on the domain
filter out implicit feedback based on criterias such as time sensitivity or
others. This is seen in the fashion domain, where we after some time want to
expire the implicitly provided feedback, as a users taste and preferences
constantly changes, as well as the current fashion trends. In other words an
\textit{in depth understanding of the domain} is required, and should be
ensured by answer a set of research goals before designing and implementing a
implicit rating system (See research goals \textbf{G1, G2 and G3}).
