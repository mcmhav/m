% !TEX root = ../../report.tex

\section{Experimental Results}

The following section will present our experimental results, dicuss our findings and
discuss whether or not our results supports our hypotheses.

\section{Binary Purchase Only Dataset}

Attempting to do collaborative filtering, let alone do recommendations with only 466 users, 1188 items and
an average of 1.34 ratings per items is no easy task using traditional recommender systems. However, these numbers
are meant as a baseline to show what one could expect when only looking at purchase data.

\begin{table}[H]
    \centering
    \begin{tabular}{*{5}l}
    \toprule
    Model 			&	AUC			&	$MAP@20$ \\ \midrule
    \rowcolor{Gray}
    Most Popular	&	0.30314		&	0.00000	\\
    ItemBasedKNN	&	0.52366		&	0.00000	\\
    UserBasedKNN	&	0.52189		&	0.00000	\\
    BPR-MF			&	0.34490		&	0.00000	\\
    \bottomrule
    \end{tabular}
\caption[Experimental Results - Purchase Only Dataset]{Experimental Results for the purchase only dataset using random splits. The results are averaged over 5 runs}
\end{table}

To our surprise our Most Popular recommender performed much worse than random guessing. But since the average item
have been given on average 1.34 ratings one could not expect to much since the ratings are evenly distribution over
all the items. Both item-based and user-based collaborative filtering performed marginally better than random guessing,
while BPR-MF performed much worse than random guessing. The $MAP@20$ scores for all recommenders are to small to be shown
using 5 descimals, meaning that close to zero relevant items were retrieved in the top 20 lists.

\section{Multiple Events Datasets}

When incorporating both clicks and likes in addition to purchases we ended up with a total of 1,511 users, 5,855 items
and an average of 4.76 ratings per item.

Here each event type is seen as equally important and is not weighted differently.
These numbers are meant as a baseline to show what is possible using binary positive feedback only recommendation methods.
Our goal is therefore to prove that these results can be beaten using our implicit ratings.

This subsection will present the experimental results for the SoBazaar dataset using implicit ratings. First we present
our results using both time-based and random splits testing different implicit rating mapping functions on different recommendation methods.

\subsection{Time-based splits}

As we have access to timestamps we decided to split the dataset on time, making the evaluation more \textit{realistic}, but also to measure the
affect of factoring in recentness. However, there seem to have been particularly few purchase activity over the last two months. We first attempted to evaluate the recommenders
using a 90:10 timesplit, making predictions for the events between 7. may - 19. may. However, there have only been recorded two purchase or 0.07\% of
all events in the last twelve day period, making it a bad period to use for evaluation. We did therefor not run multiple runs on this
data, as we thought it more beneficial to get a better split to evaluate on. The results we did get can be found in the appendix \ref{90:10-timesplits}
We instead opted to use a 80:20 timesplit, predicting the events between 16. April - 22. May. Our baselines, or binary methods are colored gray to
make the separation between them clearer. For our implicit ratings we wish to test all implicit rating functions individually and by making two different blends.
Blend 1 factors in Count, Popularity, Recentness and Price, Blend 2 only factors in Price and Popularity.

\begin{table}[H]
\centering
\resizebox{
\columnwidth}{!}{
\begin{tabular}{*{19}l}
\toprule
Model & AUC &	$MAP@20$ &	$T_{click}$ &	$T_{want}$ &	$T_{purchase}$ &	$P_{click}$&	$P_{want}$ &	$P_{purchase}$ &	$R_{click}$ &	$R_{want}$ &	$R_{purchase}$ &	$MAP@20_{click}$ &	$MAP@20_{want}$	& $MAP@20_{purchase}$ &	 \\
\midrule
\rowcolor{Gray}
Most Popular 		 		&	0.613218 &	0.004856 &	3038 &	2328 &	36 &	28 &	10 &	0 &	0.009217 &	0.004296 &	0 		 &	0.004622 &	0.003046 &	0 		 &	 \\
\rowcolor{Gray}
ItemBasedKNN k=100			& 	0.464325 &	0.002638 &	3038 &	2328 &	36 &	14 &	7  &	1 &	0.004608 &	0.003007 &	0.027778 &	0.002576 &	0.000623 &	0.002778 &	 \\
\rowcolor{Gray}
UserBasedKNN k=200  		&	0.585442 &	0.004358 &	3038 &	2328 &	36 &	17 &	14 &	3 &	0.005596 &	0.006014 &	0.083333 &	0.002927 &	0.007883 &	0.053241 &	 \\
\rowcolor{Gray}
BPR-MF	 					&	0.601683 &	0.004902 &	3038 &	2328 &	36 &	21.8 &	9 &	  0.8 &	0.007176 &	0.003866 &	0.022222 &	0.004574 &	0.002854 &	0.004762 &	\\
\rowcolor{Gray}
BPR-Linear					&	0.559033 &	0.000443 &	3038 &	2328 &	36 &	2 	  &	2 	  &		0 &	0.000658 &	0.000859 &	0 		 &	0.000136 &	0.000709 &	0 		 &	\\
ALS-WR Count 		        &   0.628547 &  0.006958 &  3037 &  2328 &  36 & 	19.25 & 22.25 & 	3 & 0.006338 &  0.009558 &  0.083333 &  0.005049 &  0.009524 &  0.062978 &   \\
ALS-WR Popularity   	    &   0.625682 &  0.001705 &  3037 &  2328 &  36 & 	13.5  & 12.75 & 	3 & 0.004445 &  0.005477 &  0.083333 &  0.000739 &  0.003915 &  0.014694 &   \\
ALS-WR Recentness     		&   0.631948 &  0.006444 &  3037 &  2328 &  36 & 	19    & 17.5  & 	3 & 0.006256 &  0.007517 &  0.083333 &  0.004529 &  0.010394 &  0.073505 &   \\
ALS-WR Price          		&   0.611358 &  0.003687 &  3037 &  2328 &  36 & 	18.75 & 13    &    4  & 0.006174 &  0.005584 &  0.111111 &  0.002123 &  0.007305 &  0.038556 &   \\
ALS-WR Blend 1              &   0.623146 &  0.005666 &  3037 &  2328 &  36 & 	19.25 & 14.5  &  	3 & 0.006338 &  0.006229 &  0.083333 &  0.004144 &  0.007086 &  0.059737 &   \\
ALS-WR Blend 2				&   0.617061 &  0.006219 &  3037 &  2328 &  36 &    17.75 & 12.75 & 3 	  & 0.005845 &  0.005477 &  0.083333 &  0.004658 &  0.008841 &  0.039871 &   \\
\bottomrule
\end{tabular}
}
\caption{Time-based split results (16. April - 19. May). All scores are averaged over 10 runs}
\end{table}

Again we see that the event distribution if far from optimal as it contains only 0.6\% purchases. However, further increasing the size of the split did not improve this ratio noticeably.
To our surprise none of the binary methods significantly outperformed the non-personalized most-popular recommender.
There is no dithering implemented in the collaborative filtering algorithms used, combined with the fact that the dataset is the same each time, giving us the same list of
recommendations each time. We can see that ALS-WR trained with our implicit ratings marginally outperformed all binary methods for most metrics, in particular the implicit ratings that factored in recency only seems
to give the best performance. Which was a pleasant surprise, since this is a time-based split. We chose to not include the results from the collaborative filtering methods using implicit ratings as they were really
poor in comparison to ALS-WR. All our results can be seen in the Appendix \ref{appendix:time-based-results}.

\subsection{Random splits}

Since we were not completely satisfied with the event distribution on our time-splits we decided to also evaluate the recommenders on random dataset splits.
As it is a random split, we chose to only factor in popularity and price, in addition to blending them.

\begin{table}[H]
\centering
\resizebox{
\columnwidth}{!}{
\begin{tabular}{*{19}l}
\toprule
Model & AUC &	$MAP@20$ &	$T_{click}$ &	$T_{want}$ &	$T_{purchase}$ &	$P_{click}$&	$P_{want}$ & $P_{purchase}$ &	$R_{click}$ &	$R_{want}$ &	$R_{purchase}$ &	$MAP@20_{click}$ &	$MAP@20_{want}$	& $MAP@20_{purchase}$ &	 \\
\midrule
%Binary Methods
\rowcolor{Gray}
Most-Popular    			&   0.751516     & 0.013236  &   1323    &   1235    &   135     &   63      &   40.75   & 3.5     &   0.0476135   &   0.03300275  &   0.0270445   &   0.0147725   & 0.0111475 &   0.0098995 & \\
\rowcolor{Gray}
ItemBased Log	   			&   0.690723     & 0.002094  &   1353.9  &   1250.5  &   138     &   8.8     &   4.6     & 1.47    &   0.006488    &   0.003672    &   0.007252    &   0.001470    & 0.002043  &   0.000618  & \\
\rowcolor{Gray}
ItemBasedKNN k=120  		&   0.7357796    & 0.0069094 &   1295.8  &   1231.4  &   146.8   & 	37.6    &   53      & 7.2     &   0.0290256   &  0.0431066    &  0.0487262    &  0.0044356    &  0.0097038&   0.0060544 & \\
\rowcolor{Gray}
UserBasedKNN K = 200 cos	&   0.7926842	& 0.0317002	&	1282.2	&   1186.2 &	136.4	&	102.2	&	101.4	& 12.6    &	  0.0797042	  &	0.0854958	  &	0.471684	  &	0.128002	  &	0.182436  &	0.0349802	&	\\
\rowcolor{Gray}
BPR-MF          			&   0.7286434    & 0.0099586 &   1332.8  &   1233.8  &   135.4   &   51.4    &   31      & 5.2     &   0.0385224   &   0.025142    &   0.0385042   &   0.0120152   & 0.0058942 &   0.0133786 & \\
\rowcolor{Gray}
BPRLinear       			&   0.7097516    & 0.0070648 &   1344.3  &   1241.9  &   141.3   &   27.4    &   25.1    & 2.5     &   0.0203941   &   0.0202133   &   0.017821    &   0.006298    & 0.0075964 &   0.0022692 & \\
%ItemBased
IBCF Price					&	0.680758 &	0.004616 &	1332.8 &	1245.7 &	137.15  &	21.1   		&	17.6    &	5.63   &	0.015859 &	0.014138 &	0.041189 &	0.003722 &	0.005    &	0.008681 &	 \\
IBCF Popularity				&	0.554495 &	0.001209 &	1335.7 &	1230.1 &	149.9 	&	0.3    		&	6 	    &	5.95   &	0.000198 &	0.004871 &	0.038575 &	0.000028 &	0.002344 &	0.006537 &	 \\
IBCF Blend 2			    &	0.681773 &	0.006226 &	1328.5 &	1245   &	147.5   &	20.5        &	18.5    &	4.5    &	0.015464 &	0.014836 &	0.030416 &	0.004825 &	0.006603 &	0.009863 &	 \\
%ALS-WR 100 100 15 true 10
ALS-WR Price				&	0.797892 &  0.034154  &	1340.5	&	1250.8	&	138.0	&	108.3		&	121.4	&	15.0   &	0.080842 &	0.09701	 &	0.108833 &	0.028443 &	0.035474 &	0.033493 & 	\\
ALS-WR Popularity		 	&	0.816561 &	0.023916 &	1340.5	&	1246.25 &	139   	&	86.5		&	98.25   &	8.5    &	0.064534 &	0.078812 &	0.060836 &	0.018396 &	0.031169 &	0.01758 &	\\
ALS-WR Blend 2				&	0.800422 &	0.034519 &	1344.75 &	1246.5  &	134.5 	&	104.5 		&	120.75  &	13.5   &	0.077686 &	0.097038 &	0.100486 &	0.026307 &	0.041865 &	0.025649 &	 \\

\bottomrule
\end{tabular}
}
\caption{Random split results 90:10 - The Results are averaged over 5-10 runs}
\end{table}

%TODO - If time, play with BPR-MF Parameters
%TODO - Makes no sense to look at recentness here, make other blends
%TODO - If time, more tuning of ALS-WR parameters
%TODO - Test two or three more blends
%TODO - Compare with binary results, do we see any improvements?



As you can see the random splits significantly improved our event distribution giving us approximately 5\% purchase events for each run. The scores in general for all metrics
are much higher for the random splits. \marginpar{TODO: Why?}
As you can see from the table, ItemBasedKNN using cosine similarity with 200 neighbors dominates the binary method category.
Another interesting observation is that IBCF produce poor recommendations using popularity alone, but that the result improves drastically when blending it with Price.
Again we see that ALS-WR is marginally better than the best baseline method. All our results can be seen in the Appendix \ref{appendix:random-split-results}


\subsection{Cold-Start Results}

The following subsections will present our results from our cold-start evaluation runs, first measuring the native cold-start performance
of the different methods. We then try to inject different combinations of filterbots to see if the recommendation quality improves.
Running cold-start experiments costly in the sense that we need to train 9 different models for each implicit rating function and model.
In addition we also have $2^5$ possible filterbots combinations, and the option of using different parameters for each method. We therefore chose
to limit the number of combinations tested to 3, based on our preliminary results. The first combination we tested only injects 12 brand
bots which each give each item of a particular brand the maximum rating. Combination two injects brand bots in a popularity bot, which rates
the items based on their popularity. Combination three injects all the bots described in \ref{implementation-filterbots}.



As ALS-WR and IBCF gave us the best results on our random split we chose to only apply filterbots to these models.
\marginpar{TODO: Figure out what to do with filterbot-als-wr combo. I do not like it...}


\subsection{Time-based splits}


\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{*{17}{l}}
\toprule &
\multicolumn{3}{c}{AUC} & \multicolumn{3}{c}{$MAP@20$} & \multicolumn{3}{c}{$R_{click}$} &  \multicolumn{3}{c}{$R_{want}$} 	& \multicolumn{3}{c}{$R_{purchase}$} & \\
\cmidrule(lr){2-4}    \cmidrule(lr){5-7}    \cmidrule(lr){8-10}    \cmidrule(lr){11-13}    \cmidrule(lr){14-16} Model &
\multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} &
\multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} &
\multicolumn{1}{c}{75\%} & 	\\ \midrule 


%PRE FILTERBOTS
\rowcolor{Gray}
Most-Popular &	0.2964	&	0.908403	&	0.973461	&	0	&	0.015807	&	0.043607	&	0	&	0.122245	&	0.149551	&	0	&	0.083855	&	0.116645	&	0	&	0.085254	&	0.114296	&	\\

\rowcolor{Gray}
IBCF &	0.581572	&	0.783782	&	0.647155	&	0.000183	&	0.008705	&	0.002948	&	0.000926	&	0.021223	&	0.01396	&	0.001526	&	0.018261	&	0.015304	&	0	&	0.076197	&	0.013116	&	\\

IBCF [0,0,0,0,1]	0.541997	&	0.58478	&	0.642857	&	0.014604	&	0.005501	&	0.001959	&	0.009187	&	0.009037	&	0.006354	&	0.01123	&	0.009327	&	0.005027	&	0.009601	&	0.008387	&	0.019131	& \\	
																																									
IBCF [0,0,1,0,1] &	0.558101	&	0.758747	&	0.78616	&	0.000614	&	0.002795	&	0.00437	&	0.003311	&	0.010315	&	0.015321	&	0.001911	&	0.006484	&	0.009145	&	0.004033	&	0.019358	&	0	& \\

\rowcolor{Gray}
ALS-WR & 	0.548427	&	0.808723	&	0.728263	&	0.000192	&	0.037148	&	0.011774	&	0.000926	&	0.092468	&	0.037826	&	0.000382	&	0.108875	&	0.03708	&	0	&	0.095247	&	0.026232	&	\\
%Non-baselines
ALS-WR [0,0,1,0,1] &  0.625073	&	0.727069	&	0.785433	&	0.000518	&	0.013534	&	0.036564	&	0.001505	&	0.039285	&	0.096876	&	0.002867	&	0.045097	&	0.112471	&	0	&	0.038716	&	0.109127	& \\

ALS-WR [0,0,0,0,1]&	0.541997	&	0.58478	&	0.642857	&	0.014604	&	0.005501	&	0.001959	&	0.009187	&	0.009037	&	0.006354	&	0.01123	&	0.009327	&	0.005027	&	0.009601	&	0.008387	&	0.019131	& \\																														
																														




\bottomrule
\end{tabular}
}
\caption{Cold-start item results - time splits}
\end{table}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{*{17}{l}}
\toprule &
\multicolumn{3}{c}{AUC} & \multicolumn{3}{c}{$MAP@20$} & \multicolumn{3}{c}{$R_{click}$} &  \multicolumn{3}{c}{$R_{want}$} 	& \multicolumn{3}{c}{$R_{purchase}$} & \\
\cmidrule(lr){2-4}    \cmidrule(lr){5-7}    \cmidrule(lr){8-10}    \cmidrule(lr){11-13}    \cmidrule(lr){14-16} Model &
\multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} &
\multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} &
\multicolumn{1}{c}{75\%} & 	\\ \midrule 


%Pre Filterbots
\rowcolor{Gray}
Most-Popular &	0.653873	&	0.658753	&	0.657664	&	0.017394	&	0.005375	&	0.010465	&	0.023064	&	0.018463	&	0.01834	&	0.020592	&	0.021551	&	0.021091	&	0.023878	&	0.04188	&	0.041581	&	\\

\rowcolor{Gray}
IBCF &  0.525182	&	0.569436	&	0.618759	&	0.013867	&	0.003454	&	0.005097	&	0.00811	&	0.001534	&	0.010209	&	0.013058	&	0.009157	&	0.011673	&	0.026996	&	0.017607	&	0.018497	&	\\

IBCF [0,0,0,0,1]	0.596502	&	0.6813	&	0.801234	&	0.0015	&	0.003662	&	0.006053	&	0.003688	&	0.012979	&	0.022237	&	0.002271	&	0.008763	&	0.016609	&	0.004107	&	0.019544	&	0.03705	& \\

IBCF [0,0,1,0,1] &	0.587419	&	0.659185	&	0.66947	&	0.016058	&	0.003011	&	0.000629	&	0.006526	&	0.00264	&	0.001355	&	0.012636	&	0.006135	&	0.001868	&	0.030767	&	0.005515	&	0	& \\
																													
\rowcolor{Gray}
ALS-WR &	0.669121	&	0.71492	&	0.747457	&	0.021574	&	0.012256	&	0.015096	&	0.015861	&	0.024894	&	0.017286	&	0.018796	&	0.030244	&	0.024057	&	0.035962	&	0.024274	&	0.039299	&	\\															
ALS-WR [0,0,1,0,1] & 0.672386	&	0.707324	&	0.737931	&	0.028034	&	0.016305	&	0.011515	&	0.016251	&	0.014753	&	0.022635	&	0.0179	&	0.019615	&	0.025434	&	0.041984	&	0.03672	&	0.030601 & \\
																													
ALS-WR [0,0,0,0,1]&	0.596502	&	0.6813	&	0.801234	&	0.0015	&	0.003662	&	0.006053	&	0.003688	&	0.012979	&	0.022237	&	0.002271	&	0.008763	&	0.016609	&	0.004107	&	0.019544	&	0.03705	& \\



\bottomrule
\end{tabular}
}
\caption{Cold-start user results -  time-split}
\end{table}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{*{17}{l}}
\toprule &
\multicolumn{3}{c}{AUC} & \multicolumn{3}{c}{$MAP@20$} & \multicolumn{3}{c}{$R_{click}$} &  \multicolumn{3}{c}{$R_{want}$} 	& \multicolumn{3}{c}{$R_{purchase}$} & \\
\cmidrule(lr){2-4}    \cmidrule(lr){5-7}    \cmidrule(lr){8-10}    \cmidrule(lr){11-13}    \cmidrule(lr){14-16} Model &
\multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{60\%} & \multicolumn{1}{c}{80\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{60\%} & \multicolumn{1}{c}{80\%} & \multicolumn{1}{c}{40\%} &
\multicolumn{1}{c}{60\%} & \multicolumn{1}{c}{80\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{60\%} & \multicolumn{1}{c}{80\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{60\%} &
\multicolumn{1}{c}{80\%} & 	\\ \midrule 

%Pre Filterbots
\rowcolor{Gray}
Most Popular &  0.619492	&	0.593922	&	0.601734	&	0.005504	&	0.004694	&	0.003103	&	0.009514	&	0.007218	&	0.006234	&	0.006499	&	0.006066	&	0.004766	&	0.027778	&	0	&	0 & \\
\rowcolor{Gray}
IBCF & 0.337922	&	0.340169	&	0.344166	&	0.000842	&	0.00055	&	0	&	0.002297	&	0	&	0	&	0.002166	&	0.0013	&	0	&	0.027778	&	0	&	0	&	\\
IBCF [0,0,1,0,1] & 	0.344166	&	0.340169	&	0.337922	&	0	&	0.00055	&	0.000842	&	0	&	0	&	0.002297	&	0	&	0.0013	&	0.002166	&	0	&	0	&	0.027778	& \\		
%With Filterbots
\rowcolor{Gray}
ALS-WR &	0.570946	&	0.568137	&	0.600738	&	0.002545	&	0.001761	&	0.006208	&	0.003937	&	0.002625	&	0.004265	&	0.003466	&	0.005199	&	0.004766	&	0.027778	&	0.027778	&	0 & \\
ALS-WR [0,0,1,0,1] & 0.576757	&	0.570766	&	0.617729	&	0.001443	&	0.002491	&	0.003569	&	0.002953	&	0.003609	&	0.005249	&	0.003033	&	0.006066	&	0.005199	&	0	&	0.027778	&	0	& \\																													
ALS-WR [0,0,0,0,1]&	0.328139	&	0.333955	&	0.362105	&	0.000348	&	0.002444	&	0.001836	&	0.000656	&	0.000328	&	0.004593	&	0.000433	&	0.002166	&	0.002166	&	0	&	0.027778	&	0	& \\																														
IBCF [0,0,0,0,1]	0.328139	&	0.333955	&	0.362105	&	0.000348	&	0.002444	&	0.001836	&	0.000656	&	0.000328	&	0.004593	&	0.000433	&	0.002166	&	0.002166	&	0	&	0.027778	&	0	& \\

\bottomrule
\end{tabular}
}
\caption{Cold-start system results - time split}
\end{table}






\subsection{Random splits}

The following results shows the baseline cold-start performance of our models before applying the filterbots.



\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{*{17}{l}}
\toprule &
\multicolumn{3}{c}{AUC} & \multicolumn{3}{c}{$MAP@20$} & \multicolumn{3}{c}{$R_{click}$} &  \multicolumn{3}{c}{$R_{want}$} 	& \multicolumn{3}{c}{$R_{purchase}$} & \\
\cmidrule(lr){2-4}    \cmidrule(lr){5-7}    \cmidrule(lr){8-10}    \cmidrule(lr){11-13}    \cmidrule(lr){14-16} Model &
\multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} &
\multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} &
\multicolumn{1}{c}{75\%} & 	\\ \midrule 


\rowcolor{Gray}
Most-Popular		& 0.3096225 & 0.90448 & 0.8469825 & 0 & 0.0267585 & 0.0234215 & 0 & 0.1207895 & 0.0943945 & 0 & 0.093357 & 0.07894715 & 0 & 0.08314335 & 	0.07447935 & \\
\rowcolor{Gray}
IBCF				& 0.585499 & 0.660409 & 0.7477435 & 0.000022 & 0.00571 & 0.005563 & 0.000308 & 0.019538 & 0.013896 & 0.000188 & 0.022364 & 0.0170475 & 0 & 0.034109 & 0.0284045 & \\ 
IBCF [0,0,0,0,1] 	&	0.625537	&	0.691961	&	0.659555	&	0.000211	&	0.007045	&	0.004775	&	0.004016	&	0.009116	&	0.008949	&	0.002999	&	0.004002	&	0.001258	&	0	&	0.012195	&	0.0125	& \\																														
IBCF [0,0,1,0,1] 	& 0.541085	&	0.762218	&	0.707801	&	0.000543	&	0.002345	&	0.001898	&	0.001826	&	0.008287	&	0.007559	&	0.002628	&	0.007263	&	0.001294	&	0	&	0.017544	&	0.012658	& \\
IBCF [1,1,1,1,1]	 &	0.494814	&	0.541715	&	0.500892	&	0.000643	&	0.001378	&	0.000324	&	0.003681	&	0.006961	&	0.003198	&	0.006074	&	0.005627	&	0.001332	&	0	&	0	&	0	& \\																														
\rowcolor{Gray}
ALS-WR 				&	0.572094& 0.8063845	& 0.698199	& 0.000124	&	0.024819	&	0.024362	&	0.00069	&	0.0745415	&	0.06881	&	0.000192	&	0.0819305	&	0.088515	&	0.0025135	&	0.071247	&	0.071688	& \\
ALS-WR [0,0,0,0,1] &	0.706584	&	0.762294	&	0.805556	&	0.034523	&	0.022352	&	0.014329	&	0.0298	&	0.029776	&	0.037975	&	0.036834	&	0.047708	&	0.03638	&	0.030965	&	0.030137	&	0.054795	&\\																														
ALS-WR [0,0,1,0,1&	0.566195	&	0.812083	&	0.795006	&	0.000035	&	0.030628	&	0.045217	&	0.000301	&	0.074944	&	0.106405	&	0	&	0.088501	&	0.156992	&	0	&	0.095808	&	0.174603	& \\																												
ALS-WR [1,1,1,1,1]	& 0.637067	&	0.79768	&	0.762618	&	0.000114	&	0.026056	&	0.038596	&	0.000615	&	0.070052	&	0.115016	&	0.000381	&	0.073538	&	0.120321	&	0.003937	&	0.10119	&	0.138889	& \\ \\																																																																																		

\bottomrule
\end{tabular}
}
\caption{Cold-start item results - random splits}
\end{table}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{*{17}{l}}
\toprule &
\multicolumn{3}{c}{AUC} & \multicolumn{3}{c}{$MAP@20$} & \multicolumn{3}{c}{$R_{click}$} &  \multicolumn{3}{c}{$R_{want}$} 	& \multicolumn{3}{c}{$R_{purchase}$} & \\
\cmidrule(lr){2-4}    \cmidrule(lr){5-7}    \cmidrule(lr){8-10}    \cmidrule(lr){11-13}    \cmidrule(lr){14-16} Model &
\multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} &
\multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{75\%} & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{40\%} &
\multicolumn{1}{c}{75\%} & 	\\ \midrule 

%Pre Filterbots
\rowcolor{Gray}
Most-Popular		& 0.7206055	& 0.7260445 &	0.729684&	0.029688&	0.017628	& 0.0062015 &	0.039089 &	0.03653 &	0.021841 &	0.02732585 &	0.02744665 &	0.01734265 &	0.02675 & 0.02343615 &	0.01177485& \\
\rowcolor{Gray}
IBCF				& 0.538305	& 0.6320575 &	0.705002 &	0.021137 &	0.006926 &	0.0006575 &	0.0163445 &	0.0133165 &	0.0022905 &	0.020079 &	0.012804&	0.0042755 &	0.0310315&	0.033987&	0.012295& \\																																																						
IBCF [0,0,0,0,1] 	& 0.643336  &	0.68084	&	0.661735&	0.015582&	0.002554 &	0.000117&	0.010884&	0.003094&	0.000904&	0.020657&	0.003727&	0.000887&	0.025501&	0.008219&	0& \\																																
IBCF [0,0,1,0,1] 	& 0.618597  &	0.70795	& 0.711321&	0.012549&	0.002649&	0.000948&	0.014675&	0.004016&	0.002068&	0.011554&	0.006788&	0.001481	&	0.034169	&	0.020906	&	0.01626	 & \\
IBCF [1,1,1,1,1] 	& 0.573466	&	0.522617&	0.503525&	0.008001&	0.003985&	0.000301&	0.005298&	0.005475&	0.002742&	0.008436&	0.005151&	0.002597&	0.016791&	0.018277&	0.014388& \\																																																								
\rowcolor{Gray}	
ALS-WR 				& 0.741754&	0.796946&	0.7571865&	0.051159&	0.0305585 &	0.0325415&	0.0401135&	0.0409645&	0.0316075&	0.054792&	0.057723&	0.047797&	0.063628&	0.064841	&	0.0428965	& \\
ALS-WR [0,0,0,0,1]  & 0.615937&	0.776737&	0.781276&	0.00005	&	0.029282 &	0.030254&	0.000927&	0.070647&	0.100671&	0.001124&	0.066324&	0.113208&	0&	0.097561	&	0.125	& \\
ALS-WR [0,0,1,0,1] 	& 0.695013&	0.767194&	0.788278&	0.045986 &	0.045413&	0.019044&	0.03157&	0.052224&	0.031098&	0.042866&	0.05386	& 0.045775&	0.040752& 0.055& 0.034884& \\																											
ALS-WR [1,1,1,1,1] 	& 0.696383&	0.780863&	0.813625&	0.03936	&	0.045689&	0.0152	&	0.030845&	0.046977&	0.036929&	0.036536	&	0.057541	&	0.041757&	0.038356	&	0.07173	&	0.044248	& \\
																															
\bottomrule
\end{tabular}
}
\caption{Cold-start user results - random splits}
\end{table}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{*{17}{l}}
\toprule &
\multicolumn{3}{c}{AUC} & \multicolumn{3}{c}{$MAP@20$} & \multicolumn{3}{c}{$R_{click}$} &  \multicolumn{3}{c}{$R_{want}$} 	& \multicolumn{3}{c}{$R_{purchase}$} & \\
\cmidrule(lr){2-4}    \cmidrule(lr){5-7}    \cmidrule(lr){8-10}    \cmidrule(lr){11-13}    \cmidrule(lr){14-16} Model &
\multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{60\%} & \multicolumn{1}{c}{80\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{60\%} & \multicolumn{1}{c}{80\%} & \multicolumn{1}{c}{40\%} &
\multicolumn{1}{c}{60\%} & \multicolumn{1}{c}{80\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{60\%} & \multicolumn{1}{c}{80\%} & \multicolumn{1}{c}{40\%} & \multicolumn{1}{c}{60\%} &
\multicolumn{1}{c}{80\%} & 	\\ \midrule 


%Pre Filterbots
\rowcolor{Gray}
Most Popualar &	 0.663937	&	0.6752165	&	0.708515	&	0.0139605	&	0.0125785	&	0.014044	&	0.033597	&	0.0370545	&	0.041488	&	0.029427	&	0.0297455	&	0.03214465	&	0.021974	&	0.0243725	&	0.02887465 & \\
\rowcolor{Gray}
IBCF 		& 0.5730765	&	0.5847735	&	0.613753	&	0.003057	&	0.0034495	&	0.002457	&	0.009752	&	0.0125695	&	0.0086305	&	0.021379	&	0.0136815	&	0.011022	&	0.018308	&	0.037721	&	0.0271465 & \\

																														
IBCF [0,0,0,0,1] &	0.617179	&	0.620468	&	0.652882	&	0.006085	&	0.005518	&	0.004676	&	0.011398	&	0.012158	&	0.011398	&	0.018145	&	0.021371	&	0.010887	&	0.024648	&	0.024648	&	0.021127	& \\
IBCF [0,0,1,0,1] &	0.55964	&	0.570314	&	0.624521	&	0.003104	&	0.002879	&	0.003668	&	0.009557	&	0.013761	&	0.014908	&	0.02075	&	0.019553	&	0.009178	&	0.032727	&	0.010909	&	0.007273 & \\
IBCF [1,1,1,1,1]	& 0.637067	&	0.79768	&	0.762618	&	0.000114	&	0.026056	&	0.038596	&	0.000615	&	0.070052	&	0.115016	&	0.000381	&	0.073538	&	0.120321	&	0.003937	&	0.10119	&	0.138889	& \\
\rowcolor{Gray}
ALS-WR & 	0.6094695	&	0.646568	&	0.7473595	&	0.0081595	&	0.0110905	&	0.030684	&	0.0199265	&	0.02659	&	0.076963	&	0.028979	&	0.03916	&	0.1011035	&	0.018231	&	0.034093	&	0.0823435	 & \\
ALS-WR [0,0,0,0,1] & 	0.615021	&	0.634666	&	0.68831	&	0.006068	&	0.009741	&	0.014679	&	0.016717	&	0.022796	&	0.035334	&	0.020968	&	0.020161	&	0.039113	&	0.021127	&	0.024648	&	0.038732	& \\
ALS-WR [0,0,1,0,1] & 	0.587088	&	0.620744	&	0.681467	&	0.008626	&	0.010318	&	0.018522	&	0.022104	&	0.019436	&	0.043445	&	0.02518	&	0.035971	&	0.049161	&	0.01476	&	0.03321	&	0.070111 & \\																																																							
ALS-WR [1,1,1,1,1] &	0.666988	&	0.677643	&	0.726329	&	0.011568	&	0.009503	&	0.022999	&	0.023229	&	0.025133	&	0.043412	&	0.027375	&	0.038647	&	0.062399	&	0.031359	&	0.02439	&	0.062718	& \\

\bottomrule
\end{tabular}
}
\caption{Cold-start system results - random splits}
\end{table}













The reason why most popular does so well on the cold-start item problem is due to the fact that we select items that have been rated over a certain number of times as test items.

















\subsection{Does our proposed implicit rating methods improve the recommendation quality over binary preference data?}

Does our findings support our hypothesis?



\subsection{Compare the different implicit rating functions}

Does our findings support our hypothesis?

\subsection{Select the best combination of methods for the SoBazar recommender system}

Does our findings support our hypothesis?











%http://datacommunitydc.org/blog/2013/05/recommendation-engines-why-you-shouldnt-build-one/

Recommender systems are arguably one of the trendiest uses of data science startups today. However, with the exception of very
rare cases, it is not \emph{the killer} feature of your application which make users flock to you. The reason it works
well for Amazon and Netflix is because they have \emph{millions} of titles and a large existing user base. Presenting users
with recommended movies and products increase usage and sales, but does not create either to begin with.

The more data the better. With little or no data you won't be able to make recommendations \emph{at all}. Unless you have the users, domain expertise, algorithm development skill, massive inventory and frictionless user data entry your recommender
system will not be \emph{the milkshake that brings all the boys to the yard}. Instead the focus should be on building your core product, optimizing your e-commerce funnel, growing your user base, developing user loyalty and growing your inventory. In the meantime you can drive serendipitous recommendations simply by using a combination of most popular content and editors.










