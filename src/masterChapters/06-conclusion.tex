% !TEX root = ../report.tex

\chapter{Conclusion}
\label{chap:conclusion}
\minitoc

% TODO: Should we write a summary statement about our goals for this chapter? I
% don't think so... ?

\clearpage
\section{Main results of this work}

% Our proposed system and its components
In this thesis we have proposed a new system for making recommendations using
an extremely sparse dataset with implicit feedback. The proposed system
consists of four core functions:

\begin{itemize}
	\item Generating implicit ratings from event logs (implicit feedback).
	\item Boosting rating quality by simulating user probabilties with filterbots.
	\item Making recommendations - in light of sparsity, implicit feedback and
	fashion domain.
	\item Evaluation and precision metrics.
\end{itemize}

% We have also studied the fashion domain.
In addition, in accordance to set research goals, we have conducted a careful
study of the fashion domain, considering both common user behaviours and the
state of recommender systems. We showed that whilst there exists multiple
competitors, very few provide \textit{personalized recommendations} to its
users, hence this segment was identified as having the largest future potential
for SoBazaar. We identified several unique user properties, consequently making
recommendations more challenging compared to other domains, such as the
importance of subcultures, conformity and recentness of items. 

% Summarize our results from the State of the Art.	
Furthermore an in-depth \textit{state of the art} analysis were carried out,
focusing in particular on existing or similar implementations of our proposed
components. In addition we present a set of essential methods and techniques
used in modern recommender systems, and which lay the foundation of our thesis.
Great care is taken to discuss and consider all presented methods for both
recommending and evaluating recommendations, with regards to having an
extremely small dataset and designing a solution for the fashion domain.

% First component: generating implicit ratings.
Our first component consists of creating implicit ratings based on a extremely
sparse dataset with \textit{implicit} feedback such as user clicks and item
interactions. First we consider the probability of each item-related event type
to contruct a scoring scheme. Then, for every event exhibited by a user on an
item we then calculate the maximum score possible based on the event type and
various features such as recentness, price and popularity. Every feature is
given a carefully selected penalization function controlling the score,
designed to account for observed behaviours in the domain.

% Second component: filterbots.
Our second component consists of creating pseudo users or filterbots in an
attempt to \textit{mitigate} the cold start problem. These pseudo users rate
items algorithmically according the item-attributes or by aggregating rating
information. A few such examples includes a \textit{PopularityBot} that rate
items based on their popularity, \textit{BrandBots} that gives all items of a
brand a maximum rating and so on. We tested out different filterbot combinations
for all three cold-start scenarios, disappointingly we did not find the filterbots
to improve the cold start performance of the system. 

% Third component: recommendation.
Our third component is the system that takes ratings as input and outputs recommendations.
We tested a wide array of different recommender systems on our dataset such as simple 
non-personalized approaches, one-class collaborative filtering methods and more sophisticated
latent factor models. The non-personalized and one-class collaborative filtering methods
was used as a baseline for our implicit rating experiments. We found ALS-WR, a latent
factor method for implicit feedback, to be a good fit for our implicit ratings.

% Fourth component: evaluation.
Our fourth component we came up with an set of evaluation metrics to measure the
overall quality of the recommender systems. Evaluating recommender systems using implicit
feedback differs from traditional recommender system evaluation, simply using RMSE was not
enough. We determined that a combination of accuracy and rank-accuracy metrics was the best
fit for our top-20 recommendation task. AUC measures the overall quality of the recommender
system while $MAP@20$Â and our own unique event specific measurements measure the systems
ability to rank the top 20 recommendations.


% How the proposed system performs
The proposed system outperformed all other methods tested for all metrics. 



% Concluding thoughts
Better results with larger dataset
Lack of negative feedback
Implicit feedback tells us a lot about user behaviour (which is a good thing)
Our proposed system performs better than only looking at purchases, which
proves this.
By testing in a online scenario and performing user-studies the real
performance may be evaluated.

\clearpage
\section{Future Work}

As previously mentioned in \ref{sec:experimental-plan} sec the main reasons for
implementing a recommender system is the desire to improve user satisfaction
and to increase the economic success of a platform.  We believe it would be
interesting to look into how one could implement a reward attribute for the
items, that factors in how much the retailer will profit from its sale.

\begin{equation}
ExpectedReturn_i = P(Sale_i) * Reward(i)
\end{equation}

The question then, is how this information can e.g. be used/incorporated into
the recommender to increase profits without sacrificing (too much or any) user
satisfaction by recommending more \emph{expensive} items to the user.


%Discussion on product database features to improve content-based recommendations
%Missing color, category, age-group and others
However, the content-based filtering methods require rich descriptions of items
and well built and well informed user profiles. These ideal cases are rare in
real applications. This dependence on the quality and structure of data is the
main weakness of methods based on content.

With regards to recommendation quality we believe the main focus should be on
getting more and higher quality data.

\begin{itemize}

\item More extensive crawling the webstores collecting likes or popularity
information, better content descriptions, and so on.

\item Provide the user with \emph{cool} and \emph{fun} ways of expressing their
preferences. A tinder-like \emph{hot or not} interface, the ability to follow
brand or friends are a few such examples.

\item Negative feedback in the form of a dislike button or similar.
\end{itemize}

Adding data entry amounts by adding a step to your funnel could hurt sales more
than it helps. One should therefore carefully consider adding features the
require user effort. If a \emph{hot or not} interface is to be implemented one
should preferably conduct a study to find the best strategy of selecting items
to display to the users.

Especially better quality and structure of the data. Table
\ref{table:extracted-content-features} shows an overview of the percentage of
items we managed to extract the different attributes for. Attributes such as
category, color, brand-name ...  are particulary useful and should be easily
extracted from the product database for most products.

%Recommender systems for anonymous users - Context
%Recommend other items with a high similarity to the item currently being viewed

Contextual item-to-item recommendations should be considered implemented to
display related items to anonymous users in the context of viewing an item
instead of the current system which recommends "People how love this also
love".

Finding and incorporating more implicit factors should also be considered as a
possible future direction.

Do trends repeat year after year?

Personalization of the impact of different implicit factors could be
interesting to look at.  Implicit factors such as brand preference, importance
of global popularity, weight given to the different event types and so on could
all be personalized for each user.  However, we feel it is important to mention
the connection to matrix factorization methods aka SVD which \emph{basicly}
does the same thing automatically. It reduced the rating matrix to a set of
item and users factors for each individual item and user and a set of utility
values for these factors.


Other implicit factors such as brand preference, ... could all be implemented
to

As shown in \cite{FranceTelecom} most successful commercial recommender
incorporate multiple sources of information. The following information sources
should therefore be examined more closely to assess if they can add any value
to the system:

\begin{itemize}
\item Social information. Trust could be inferred from functionality such as
\emph{follow user}, which could be used to improve recommendation quality and
improve cold-start performance.
\item Content information.
\item Demographic information.
\end{itemize}

% TODO: Future work: perform user study to confirm implicit ratings
% performance.

\marginpar{Short discussion on the above mentioned information sources and how
they can help. BE EXPLICIT!}
%Importance of brand
%Importance of social groups - fashion
%Demographic filtering as a solution to the cold-start user problem?


This thesis have only on a small part of the possibilities. There is much work
to be done. %%% THE END %%%
