% !TEX root = ../report.tex

\chapter{Preliminary Study}
\minitoc

\clearpage

\section{State Of The Art}
\subsection{System Coldstart Handling}

Cold-start scenarios in recommender systems are situations in which little/no prior events, like ratings or clicks, are known for certain users or items.

The cold start problem can divided into three sub problems: (1) Cold-start system, (2) Cold-start user and (3) Cold-start item

\subsubsection{Cold-start System}

%Having a large amount of data like e.g. in the netflix dataset
% -> Do not require a great understanding of the data to get decent results
% -> Out case is a little different. What implications does the limited amount of data have?


\subsubsection{Cold-start user}

\begin{quotation}
Ask the right questions if you're going to find the right answers
\end{quotation}
- Vanessa Redgrave

%What is the cold-start user problem?

One crucial problem of recommender system is how to best learn from new users. Collaborative Filtering (CF), is the best known technology for recommender systems and is based on the idea that like-minded users have similar tastes and preferences. A new user therefore poses a challenge to CF recommender, since the system has no knowledge about the preferences of the new user, and can therefore not provide any personalized recommendations, this is known as the cold start problem for new users. The system must therefore acquire some information about the new user in order to make personalized recommendations. However, the system must be careful to present useful items to garner information. A food recommender should probably not ask whether a new user likes vanilla ice cream since most people like vanilla ice cream. Therefore, knowing that a new user likes vanilla ice cream tells you very little about the user. The choice of what questions to ask a new user, then, is critical.

Rashid et. al. \cite{Rashid2002} performed a study of different item selection strategies that collaborative filtering recommender systems can use to learn about new users. They presented the users with a questionary with items asking them to rate/select the ones they like. Their strategies can be divided into five classes:

\begin{itemize}
\item \emph{Random} strategies: Strategies that avoid bias in the presentation of bias
\item \emph{Popularity:} Select among the top N items where the probability that is proportionate to the items popularity.
\item \emph{Pure entropy:} Present the items with the highest entropy that the user has not seen
\item \emph{Balanced strategies:} A balanced approach combining both popularity data and entropy.
\item \emph{Personalized:} As soon as some information is known about a user, present items specifically tailored to that user using e.g. item-item similarity
\end{itemize}

This study was later extended by Rashid et. al. \cite{Rashid2008} where they more closely examined information theoretic strategies for item selection.

%Their suggestion for e-commerce: Recommend most popular items rather than the highest rated ones, and then use item-item similarity as quickly as possible

A new user preference elicitation strategy needs to ensure that the user does not 1) lose interest in returning due to low quality initial recommendations, 2) as quickly as possible being able to provide good personalized recommendations (find the right neighbourhoods).

We are constrained to unobtrusively learn user-profiles from the natural interactions of users with the system, meaning that we can not require the user to rate e.g. 10 items before we can start providing recommendations. We have a \emph{mixed initiative} system meaning that there is provisions for both user and system controlled interactions. We (the system) can only select which items to recommend to the user, and this does not mean that the user actually will click an item or rate it. 

\subsubsection{Cold-start item}

%What is the cold-start item problem? / Introduction


%What strategies exist?


%What is suitable in our case?

\subsection{Fashion Recommendation}

% Building Recommender Systems using a Knowledge Base of product semantics
% http://images.accenture.ca/SiteCollectionDocuments/PDF/recommenderws02.pdf
% 	- Would probably require some more product semantics

%What are the challanges of making recommendations for fashion?
%	- How often are items relevant?
%	- Implicit feedback (Based around users fashion browsing habits and an occational purchase...)
%	- Changing interest of users
%	- Unstructured content/multiple content providers
%	- Sparsity
%	- Trends?

\subsection{Session Based Recommendation}
Articles 4 l8er:
In Proceedings Of
the 1995 International Joint Conference on Artificial Intelligence, 1995. Montreal,
Canada.

S. Schechter, M. Krishnan, and M. D. Smith. Using path profiles to predict http
requests. In Proceedings of 7th International World Wide Web Conference, Novem-
ber 1998. Brisbane, Australia.

M. Spiliopoulou and L. C. Faulstich. Wum: A web utilization miner. In In Pro-
ceedings of EDBT Workshop WebDB98, 1999. Valencia, Spain.

R. Cooley, B. Mobasher, and J. Srivastava. Data preparation for mining world wide
web browsing patterns. Journal of Knowledge and Information Systems, 1(1), 1999.

B. Mobasher, H. Dai, T. Luo, and M. Nakagawa. Discovery of aggregate usage
profiles for web personalization. In Proceedings of the Web Mining for E-Commerce
Workshop (WebKDD’2000), 2000.

C. Shahabi, A. Zarkesh, J. Adibi, and V. Shah. Knowledge discovery from users
web-page navigation. In Proceeding of the IEEE RIDE97 Workshop, pages 20–29,
April 1997. Birmingham, England.

O. Nasraoui, R. Krishnapuram, and A. Joshi. Mining web access logs using a fuzzy
relational clustering algorithm based on a robust estimator. In Proceedings of Eight
International World Wide Web Conference, 1999. Toronto, Canada.

Y. Yan, M. Jacobsen, Garc ̈ıa-Molina H, and U. Dayal. From user access patterns to
dynamic hypertext linking. In Proceedings of the Fifth International World Wide
Web Conference, 1996. Paris, France.

A. Nanopoulos, D. Katsaros, and Y. Manolopoulos. Effective prediction of web-
user accesses: a data mining approach. In Proceedings of WEBKDD workshop,
2001. San Francisco, CA, USA.

R. Agrawal and R. Srikant. Mining sequential patterns. In Proceedings of the In-
ternational Conference on Data Engineering (ICDE), March 1995. Taipei, Taiwan

M. Deshpande and G. Karypis. Selective markov models for predicting web-page
accesses. In Proceedings of the First SIAM International Conference on Data Min-
ing (SDM’2001), 2001.

R. R. Sarukkai. Link prediction and path analysis using markov chains. In Proceed-
ings of the Ninth International World Wide Web Conference, 2000. Amsterdam.


%http://dl.acm.org/citation.cfm?id=1136004
%http://link.springer.com/chapter/10.1007/3-540-46119-1_42
%http://dl.acm.org/citation.cfm?id=1082567
%http://link.springer.com/chapter/10.1007%2F978-3-540-30214-8_20
%http://dl.acm.org/citation.cfm?id=502935
%http://dl.acm.org/citation.cfm?id=1835896
%http://dl.acm.org/citation.cfm?id=345169
%http://dl.acm.org/citation.cfm?id=345169

\subsection{Recommenders (Similar systems? somethingsomething)}

% Trust based CF recommenders

% ### Hybrid Systems ###

% ### COLD START NEW ITEM ARTICLES ###

% Regression-based Latent Factor Models - http://dl.acm.org/citation.cfm?id=1557029
% Learning Attribute-to-Feature Mappings for Cold-Start Recommendations - http://ieeexplore.ieee.org/xpl/abstractCitations.jsp?tp=&arnumber=5693971&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5693971
% fLDA: Matrix Factorization through Latent Dirichlet Allocation - http://dl.acm.org/citation.cfm?id=1718499
% Matchbox: Large Scale Bayesian Recommendations


\section{Data Findings}
\subsection{What Can Be Understood From The Data}

%Dataset summary
	%How many users?
	%How much preference data?
	%Which events are interesting to look at?

\subsubsection{The Expected}
Event "app_started"; all have user_id's
Event "app_first_started"; all user_id's are NULL
Event "user_logged_in"; all have user_id's... (assigned with login, event saved after login?)

\subsubsection{The Strange}
NULL valued events: (Not all strange, but put together for readability)
facebook_share_changed
collection_viewed
wantlist_menu_entry_clicked
app_became_active

app_first_started
facebook_login_failed

> db.prod.distinct('event_json.ipAddress').length
9033
> db.prod.distinct('event_json.eventData.device_id').length
2644
> db.prod.distinct('user_id').length
1660

More devices than users, can't fill the blanks with device_id

\subsection{Graphs N' Shit}

\section{What to use}


%
\subsection{Some Awesome Algorithms (Build up with project progress)}

Given enough data, item-based CF methods often performs as well or better than almost any other recommendation method. However, in cold-start situations where a user, an item, or the entire system is new, simple non-personalized recommendations often fare better...

\subsubsection{The Good}
\subsubsection{The Bad}
\subsection{Why Not To Use These (Same As above)}
\subsubsection{The Good}
\subsubsection{The Bad}

\section{How to evaluate}

Explicit feedback ("relevance judgement")

Explicit feedback are more precise than implicit feedback, but more difficult to collect since it requires the user to spend time rating items and the amount of feedback is often scarce. The main difference between the two is that implicit feedback is inferred from user behaviour, such as noting which news articles they do and do not view. Explicit feedback unlike implicit feedback provides the users with a mechanism to unequivocally express their ratings on a scale from usually in the form of a Likert scale ranging from 1-5 (strongly disagree - strongly agree). Thus explicit feedback captures both positive and negative feedback, while implicit feedback only can be positive. Furthermore, explicit feedback tend to concentrate on either side of the rating scale, as users are more likely to express their preference if they feel strongly for or against an item.

product_wanted, in the form of like/no feedback, can this be considered "explicit feedback?" no negative feedback...

Implicit feedback (Indirectly reflect opinion through observing user behaviour)

Unlike the more extensively researched explicit feedback, we do not have any
direct input from the user regarding their personal preferences. In particular
we do not have any substantial evidence of which items the user dislikes (e.g. a
low rating for a movie). But in return implicit feedback is more easily
collected, and usually more abundant.

In the case of our project we collect the following data, which can be considered as implicit feedback.
- product_detail_clicked, product_purchase_intended, collection_viewed(?)...

Other types of implicit feedback include purchase history, browsing
history, search pattern and even mouse movements

Hu et. al. \cite{Hu2008} identify four unique characteristics of implicit feedback, which differentiates it from explicit feedback...

\begin{enumerate}

\item No negative feedback. By observing user behaviour we can infer which items the user consume and probably like. However, it is hard to infer
which items the user did not like. This asymmetry has several implications; Explicit feedback provides a more detailed picture of the
users preferences, but for implicit data the low ratings are treated as missing data and omitted from the analysis. Hence it is crucial to address
the missing data where most negative feedback is expected to be found

\item Implicit feedback is inherently noisy. While we track user behaviour, we can only guess their preferences and true motives. For example, a
purchase does not necessarily indicate a positive view of an item, the item may have been purchased as a gift, or perhaps the user was disappointed
with the item

\item The numerical value of explicit feedback indicates preference, whereas the numerical value of implicit feedback indicates confidence. Explicit
feedback could e.g. range from total dislike to really like, on the other hand implicit feedback describe the frequency of actions, e.g. how
frequently a user buys an item. But a higher frequency might not necessarily indicate a stronger preference. A user might choose to only
watch a really good movie once. However, a recurring event is more likely to reflect the user opinion. However, the numerical value of the feedback
is definitely useful, as it tells us about the confidence we have in a certain observation

\item Evaluation of implicit feedback requires appropriate measures. In the case of explicit feedback where a user specify a numerical score, measures such
as mean squared error (MSE) could measure the success of the predictions. However, with implicit models we have to take into account
the availability of the item, competition with other items, and repeat
feedback.

\end{enumerate}

Accuracy metrics not suited for implicit feedback datasets, as they require knowing which items are undesired by a user \cite{Hu2008}.

Being accurate is not always enough. Striking a balance between accuracy and user satisfaction \cite{McNee2006}
	% High accuracy != User satisfaction
	% Therefore, important to consider other evaluation metrics beyond the conventional ones
	% Which are the most important factors to consider with regards to user satisfaction

The data available strongly influences the choice of evaluation method/metrics. E.g. classification accuracy metrics seem to be the most suitable when working with binary preferences, e.g. in the form of recall-oriented measures.

\subsection{What Has Been Done Before}

%Evaluation by looking at sessions
%Evaluation using implicit feedback datasets
%Turnover rate?
% ++

\subsection{What To Use}
\subsubsection{The Good}
\subsubsection{The Bad}

\section{Evaluation}
