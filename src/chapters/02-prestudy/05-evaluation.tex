% !TEX root = ../../report.tex

\label{evaluation}
\section{Evaluation}

% What will be adressed in the section?

	% What evaluation methodologies exist?
		% Offline Evaluation
		% 	Recommender System Datasets
		%		Explicit vs. Implicit
		% 	Offline Evaluation Measures
		%		Traditional evaluation measures
		%		Cold-start system evaluation
		%		Implicit feedback evaluation measures
		% Online Evaluation


%Some key questions in evaluating recommender systems on testbed data are: what
%to predict, how to grade performance and what baseline to compare with.

In many cases a system designer that wished to employ a recommender system must
choose between a set of candidate approaches. A first step towards selecting an
appropriate algorithm is to decide which properties are the most important for
the application. Recommender systems have a variety of properties such as
accuracy, robustness, scalability, etc. The following section will discuss how
to compare recommenders based on the set of properties  relevant for the
application. We have several different experimental settings which can be used
to evaluating recommender systems. We will focus on two types of experiments:
Offline experiments where the system is evaluated without user interactions and
Online experiments where real users interact with the system. This section will
also cover some of the most frequently used evaluation metrics, a discussion on
the suitedness of different evaluation measures for implicit feedback, and
summary of the methodologies and evaluation measures used to evaluate the
cold-start performance of recommender systems.

Shani et\. al.\ \cite{Shani2011} lists the following guidelines for general experimental
studies:

\begin{itemize}

\item Hypothesis: before running an experiment one must form an hypothesis. For
	example, an hypothesis can be that algorithm $A$ better predicts user ratings
	than algorithm $B$. In that case the experiment should test the prediction
	accuracy, and not look at other factors.

\item Controlling variables: when comparing a few candidate algorithms on a
	certain hypothesis, it is important that all variables that are not tested
	are fixed.

\item Generalization power: when drawing conclusions from experiments, we may
	wish that our conclusions generalize beyond the immediate context of the
	experiments. When choosing an algorithm for a real application, we may want
	our conclusion to hold on the deployed system, and generalize beyond the
	experimental data set. To increase the probability of generalization of the
	recommender results one must typically experiment with several data sets or
	applications.

\end{itemize}


\subsection{Offline Evaluation}

Offline experiments are performed using pre-collected dataset\(s\) and a protocol
that models the user behavior to estimate recommender performance through
different evaluation measures. Offline experiments are attractive because they
require no interactions with real users, and thus allows researchers to compare
a wide range of algorithms at a low cost. The downside of offline experiments
is that they can answer a very narrow set of questions, typically questions
about the predictive power of the algorithm, and does not measure other user
factors.

\subsubsection{Datasets for Offline Evaluation}

The main aim of a recommender system is to identify the set of items in a
dataset that might be intersting to a user based on their expressed
preferences. For a fashion recommender this would mean estimating how much a
user might like an item, by e.g.\\ predicting what rating a user might give an
item. In recent years, various test collections for different domains such as
books, music, movies have been made available to the public. These datasets
usually consist of user ratings in the form of \emph{<UserID, ItemID, Rating>}.

In recent years more or more datasets have been made available which contains
additional information such as demographic information about the users,
trust-networks, user-assigned tags and etc. Under we have listed a few selected
popular datasets containing additional information:

%TODO - Find more cool dataset

\begin{itemize}

\item MovieLens 100k dataset~\cite{Movielens}: The movielens dataset
	incorporates demographic information about the user in addition the
	traditional rating matrix

\item Epinions dataset~\cite{Epinions}: The Epinions dataset includes a
	trust-network, which specifices who-trust-whom in a social network based on
	customer reviews for the website Epinions.com

\end{itemize}

\paragraph{Explicit-feedback}

The definition of explicit is defined as `stated clearly and in detail, leaving
no room for confusion or doubt'. Explicit feedback are more precise than
implicit feedback, but more difficult to collect since it requires active user
involvement. One serious implication of this is that the amount of feedback
often is scarce since many users opt not to provide any feedback. Explicit
feedback mechanisms allow the users to unequivocally express their ratings on a
scale (usually in the form of a Likert scale (strongly disagree – strongly
agree). Thus explicit feedback is able to capture both negative and positive
feedback, while implicit feedback \emph{only} can be positive. It is worth
noting that explicit feedback tend to concentrate on either side of the rating
scale, as users are more likely to express their preference if they feel
strongly for or against an item~\cite{Jawaheer2010}.

\paragraph{Implicit-feedback}

%TODO - Herman, maybe you want to add something more?
%	Implicit feedback classfication (Type, confidence, precision)

Unlike explicit feedback, we do not have nay direct input from the user
regarding their personal preferences. In particular we do not have any
substantial evidence of which items the user dislikes, such as low ratings.
However, implicit-feedback is more easily collected, and usually more abundant.
Types of implicit feedback include purchase history, browsing history, search patters, or even mouse movements. For example, a user who purchases many clothes from the same brand probably likes that brand. For a larger discussion surrounding the differences between
implicit and explicit feedback, see Section~\ref{implicit-feedback}


%Implicit feedback methods are
%based on assigning a relevance score to speciﬁc user actions on an item, such as
%saving, discarding, printing, bookmarking, etc. The main advantage is that they do
%not require a direct user involvement, even though biasing is likely to occur, e.g.
%interruption of phone calls while reading

\subsubsection{Validation Methods}

Validation techniques are motivated by two fundamental problems; model
selection and performance estimation. Almost all pattern recognition techniques
have one or more free parameters, and we want a way to select the
\emph{optimal} parameter\(s\) or model for a given problem. Once we have chosen a
model, we wish to estimate how well it is doing.

\paragraph{The Holdout Method}

When using the holdout method you split the dataset into two groups; a training
set used to train the classifier and a test set used to estimate the error rate
of the trained classifier. The Netflix Prize Competition~\cite{Netflix}
provided a training set consisting of 100,480,507 ratings given by 480,189
users to 17,300 movies. The testset consisted of 2,817,131 items where the
ratings were unknown. The submitted predictions were scored against the true
ratings in terms of root mean squared error (RMSE), and the goal was to
minimize this error.

The holdout method has two basic drawbacks; (1) In problems with sparse
datasets we may not be able to afford the luxury of setting aside a portion of
the dataset for testing, (2) Since it is a single train-and-test experiment,
the holdout estimate can be misleading if we happen to get an `unfortunate'
split.

\paragraph{Cross-Validation}

K-fold Cross-Calidation creates a K-fold partition of the dataset. For each of
the $K$ experiments, use $K-1$ folds for training and the remaining one for
testing. By setting $K=2$, this is the same as the holdout method. The true
error is found by taking the average error from all the experiments.

Leave-one-out is the degenerate case of K-fold Cross-Validation, where $K$ is
chosen as the total number of examples. For a dataset with $N$ examples we
perform $N$ experiments. For each experiment we use $N-1$ examples for training
and the remaining example for testing. Again, the true error is found by taking
the average error rate from the experiments.

In practice the number of folds often depends on the size of the dataset. For
large datasets, even 3-Fold Cross Validation will be quite accurate, which for
sparse datasets, one may wish to train as many examples as possible. A common
choice is $K$ value between 5 and 10.

%TODO - Advantages/Disadvantages

\subsubsection{Offline Evaluation Metrics}

When evaluating a recommender system, you wish to estimate a user's
satisfaction for a given recommendation. Traditionally recommender systems have
been evaluated by means of predictive accuracy. However, there is now a widely
agreed that accurate predictions are crucial but insufficient to deploy a good
recommendation engine~\cite{Shani2011, McNee2006}. Some of the properties can
be traded-off, one such example is the trade-off between accuracy and
diversity. It is important to understand and evaluate these trade-offs and
their effect on the overall performance. This subsection will cover the most
popular metrics used for offline evaluation, a discussion of evaluation
measures for implicit feedback, and a summary of the cold-start evaluation
methodologies found in the literature.

\paragraph{Predictive Accuracy Metrics}

Predictive accuracy metrics measure how close the predicted ratings are to the
true user ratings. More formally, the system tries to predict ratings
$\hat{u(c,s)}$ for a test set $T$ of user.item pairs $(c, s)$ for which the
true ratings are known. Traditionally, mean absolute error (MAE) has be used to
evaluate the performance of collaborative-filtering algorithms, but other
measures such as root mean squared error (RMSE) are also commonly used.

\begin{equation}
MAE = \sqrt{\frac{1}{\vert T \vert} \sum_{(c,s) \epsilon T}{(\hat{u(c,s)} - u(c,s))}^{2}}
\end{equation}

\paragraph{Measuring Usage Prediction}

In many applications the recommender system does not predict the user's
preferences of items, but tries to recommend to users items that they may use.
In an offline evaluation of usage prediction, we typically have a dataset
consisting of items each user has used. We then select a test user, hide some
of her selections, and ask the recommender to predict a set of items the user
will use. We then have four possible outcomes for the recommend and hidden
items:

\begin{center}
\begin{table}[H]
\begin{tabular}{|c|c|c|}
\hline
			&	Recommended		&	Not Recommended \\ \hline
Used		&	True-Positive 	&	False-Negative	\\ \hline
Not Used	&	False-Positive	&	True-Negative	\\ \hline
\end{tabular}
\label{table:usageprediction}
\caption{}
\end{table}
\end{center}

This model assumes that unused items would not have been used if they had been
recommended to a user. This assumption may be false, such as when the set of
unused items contains some interesting items that the user did not select. For
example, a user may not have used an items because she was unaware of its
existence, but after the recommendation exposed that item, the user can decide
to select it. We can count the number of examples that fall into each cell in
the table and compute the Precision, Recall and False Positive Rate.

\paragraph{Rank Accuracy Metrics}

Rank accuracy metrics measure the ability of a recommendation method to produce
a recommended ordering of items that matches how the user would have ordered
the same items. Shani et.\ al.~\cite{Shani2011} lists two different appraoches
for measuring the ranking accuracy: Try determining the correct order of a set
of items for each user and measure how close a system comes to this correct
order, or we can attempt to measure the utility of the system's ranking to a
user.

Herlocker et.\ al.~\cite{Herlocker2004} argue that rank accuracy metrics may be
overly sensitive for domains where the user just wants an item that is `good
enough' (binary preferences) since the user won't be concerned about the
ordering of items beyond the binary classification. These metrics are therefore
most suitable to evaluate algorithms that are used to present ranked lists to
the user in domains where the user preferences are expressed using numerical
values.

\paragraph{Beyond Accuracy}

There is an emerging understanding that good recommendations accuracy alone does not give the users of the recommender system an effective and satisfying experience \cite{Herlocker2004}. The following \emph{measures} attempts to assess a recommender systems usefulness beyond being able to provide accurate recommendations to the users.

\subparagraph{Coverage}

The term coverage can refer to several distinct properties of the system. Most
commonly, the term coverage refers to the proportion of items the
recommendation system can recommend, also known as \emph{item-space coverage}.
The simplest measure of catalogue coverage is the percentage of all items that
can ever be recommended. Coverage can also be the proportion of users
interactions for which the system can recommend items, known as
\emph{user-space coverage}. In many applications the recommender system may not
provide recommendations for some users due to e.g.\\ low confidence in the
accuracy of predictions for that user. In such cases one may prefer a
recommender that can provide recommendations to a wider range of users. However, an increase in coverage is only beneficial if the accuracy does not drop significantly.

\subparagraph{Novelty and Serendipity}

Some recommender systems produce highly accurate recommendations and also have reasonable coverage - and yet that are useless for practical purposes. For instance, a music recommender can recommend Rihanna to every customer who have not yet listened to Rihanna. However, statistically, this is highly accurate as most people have listened to Rihanna or at least knows about her and have consciously chosen not to listen to her. Much more valuable would be a recommendation to a kick-ass indie-rock band that the active user would love, but will never hear about in the news. We therefore need a new dimension for analysing recommender systems that consider "nonobviousness" of the recommendations. One such dimension is \emph{novelty}. Another related dimension is \emph{serendipity}. A serendipitous recommendation helps a user find a surprisingly interesting item the user might not have discovered otherwise. To clarify the difference between the two, a novel recommendation could be to recommend an unknown album from one of the users favorite artists, that the user likely eventually would have discovered herself. However, a recommendation by an unknown artist is more likely to be serendipitous. Serendipity is therefore a measure of how surprisingly the successful recommendations are.

\subparagraph{Diversity}

Diversity is generally defined as the opposite of similarity. In some cases suggesting a set of similar items may not be as useful for the user, because it may take longer to explore the range of products. E.g. when presenting a list of 5 recommendations, the system should not recommend 5 Ralph Lauren shirts with different colors. As diversity may come at the expanse of other properties such as accuracy, one should evaluate the decrease in accuracy vs. the increase in diversity.

Other evaluation methods worth knowing about includes confidence, trust, utility, robustness, adaptivity, scalability mentioned in \cite{Herlocker2004, Shani2011}.

\subsubsection{Evaluation using Implicit Feedback}

Accuracy metrics not suited for implicit feedback datasets, as they require
knowing which items are undesired by a user~\cite{Hu2008}.

%TODO - What evaluation measures are suited for implicit feedback, and why are traditional evaluation measures such as RMSE, ... not as suitable when working with implicit feedback?

\subsubsection{Evaluation of Cold-start Recommendations}

The cold-start problem can be considered a sub problem of coverage because it
measures the system coverage over a specific set of items and users. When
evaluating the cold-start system performance one is interested in measuring the
system accuracy for these users and items.

The evaluation metric used seem to depend on the type of feedback available.
Most experiments carried out have used \emph{traditional} explicit feedback datasets such as
MovieLens, EachMovie, Netflix etc. Accuracy metrics such as MAE~\cite{Rashid2002, Rashid2008, Massa2004,
Massa2007, Stern2009} and RMSE~\cite{Agarwal2009, Agarwal2010} are therefore
the most used ones. In the experiments where binary rating data have been used
Precision@N~\cite{Liu2011, Gantner2010}, ROC curves~\cite{Agarwal2009,
Gantner2010, Schein2002} and Area Under Curve (AUC) \cite{Liu2011, Gantner2010} seems to be the
preferred evaluation metrics.

Another way to discriminate between different recommender techniques is
coverage. The recommender system may not be able to make predictions for every
item. For this reason, it is important to measure the portion of ratings that
an RS is able to predict (ratings coverage). However, this quantity is not
always informative about the quality of a recommender system. A RS is likely to
be good at predicting nearly all the ratings for heavy users and not be able to
do the same for users who have rated few items. For this reason, one should
also compute the users coverage, defined as the portion of users which the RS
is able to predict at least one rating for. Good et.\ al.~\cite{Good1999}
measure the item-space coverage, while Massa et.\ al.~\cite{Massa2004,
Massa2007} measures both the item-space coverage and user-space coverage of
their methods.

Massa et.\ al~\cite{Massa2004} argue that performance measures such as Mean
Absolute User Error (MAUE) is a good measure for cold-start recommendations
since every user is taken into account once and a cold start user is as
influential as an heavy rater. Similarly, Park et.\ al.~\cite{Park2006} measure
normalized MAE (NMAE) by macro-averaging, which first calculates the mean
average error of each users and taking the average of all users.

To simulate the cold-start scenario, different approaches have been employed.

One popular way to simulate a cold-start user scenario used by~\cite{Stern2009,
Lam2008} is to split the dataset in two disjoint sets, a training set
containing 90\% of the users and the remaining 10\% being in the test set. For
each test user one trains a model with a random subset $T\%$ of their ratings
e.g.\ $5\%$ or $75\%$, and then use the model to predict their remaining
ratings. The same methodology can also be used to simulate a cold-start item
scenario. Another highly similar way to simulate the cold-start user and
cold-start item scenario was used in~\cite{Rashid2002, Rashid2008}. For the
cold-start user scenario one selects a subset of the users with e.g.\ more than
200 ratings. One then trains the model with a subset of the ratings. In the
case of~\cite{Rashid2002} 30, 45, 60 and 90 ratings was used. After training
the model one computes the error on the hidden ratings for the same user.

Another \emph{simpler} approach employed by~\cite{Massa2007, Jamali2009} is to
determine a cutoff point for what is considered a cold-start user. E.g.\ that
every user with less than 5 ratings is considered a cold-start user. Then
separately measure the error on predictions made to these users.

To simulate a cold-start system scenario Agarwal et.\ al.~\cite{Agarwal2009}
split the dataset in two using 75\% of the dataset for training and 25\% for
testing. They then train the model using 30\%, 60\% and 75\% of the data and
compare their performance on the testset.

%Summary of articles read

%	What evaluation metrics are used?

%\cite{Rashid2008}: Accuracy metric: MAE, Expected Utility (Penalize false positives more than false negatives)
%\cite{Rashid2002}: Accuracy metric: MAE
%\cite{Massa2004}: Leave one out, MAE, MAUE, Rating Coverage, User Coverage
%\cite{Massa2007}: Leave one out, MAE, MAUE, Rating Coverage, User Coverage,
%\cite{Jamali2009}: Leave one out, Recall/Hit-ratio
%\cite{Agarwal2009}: Movie: RMSE, Yahoo: ROC curves, 5-fold cross validation
%\cite{Agarwal2010}: RMSE, True Positive Rate, True Positive Rate
%\cite{Liu2011}: Precision at N, Mean average precision, area under curve
%\cite{Park2006}: NMAE
%\cite{Good1999}: Coverage, MAE, ROC
%\cite{Stern2009}: MAE
%\cite{Ganter2010}: Precision at N (5 & 10), AUC (General ranking measure)
%\cite{Schein2002}: GROC (hit/miss rate)

%	What type of user feedback is used?

%\cite{Rashid2008}: Explicit feedback, MOVIELENS, Only users with 80 or more ratings
%\cite{Rashid2002}: Explicit feedback, MOVIELENS, Only users with 200 or more ratings
%\cite{Massa2004}: Explicit feedback, EPINIONS + Web of trust
%\cite{Massa2007}: Explicit feedback, EPINIONS + Web of trust
%\cite{Jamali2009}: Explicit feedback, EPINIONS + Web of trust
%\cite{Agarwal2009}: Explicit feedback, MovieLens + EachMovie, also incorporates user features
%\cite{Agarwal2010}: Explicit feedback + User features and bag of words rep of crawled movie data - MovieLens, Yahoo! Buzz (1 or -1), BookCrossing
%\cite{Liu2011}: Explicit feedback. Netflix...
%\cite{Park2006}: Explicit feedback, Yahoo!, MovieLens, EachMovie
%\cite{Stern2009}: MovieLens, Netflix,
%\cite{Ganter2010}: MovieLens - Binary (likes, not likes)
%\cite{Schein2002}: MovieLens, Movielens(Stripped of ratings -> implicit)

%	How do they similate the "cold-start situation"?

%\cite{Rashid2008}: Use the movies found when presenting 15, 30, 45, 60, 75 movies to provide predictions for the remaining movies in the list of each user
%\cite{Rashid2002}: Use the movies found when presenting 30, 45, 60, 90 movies to provide predictions for the remaining movies in the list of each user
%\cite{Massa2004}: Consider users who provided 2, 3 or 4 ratings, How does trust propagation of 1,2,3,4 affect rating & user coverage and predictive accurracy?
%\cite{Massa2007}: All users, cold users, heavy users, Controversial items, Black sheep, Trust propagation performance on entire dataset
%\cite{Jamali2009}: All users, cold start users (<5 ratings), recall for different neighborhood sizes
%\cite{Agarwal2009}: 25\% set aside for evaluation, train each model with 30\%, 60\%, 75\% of data, compare performance
%\cite{Agarwal2010}:
%\cite{Liu2011}: User cold start: Split users into disjoint sets (training, test). Item cold start: Split into disjoint sets
%\cite{Park2006}: Fraction of training data used [0.1 -> 1.0]. Cold-start user: Select users with more than 40 ratings in the training set and more than 1 in the test set. Split into 5, with 20\% of the users in each training set. Starting at 2 ratings, add 2 additional training set ratings per iteration until 40 ratings are added. Take the average of the 5 to compute the NMAE. Cold-start item: Items rated by more than 40 users in training data, and at least 1 user in test set. Split in 5. Starting from 2, add 2 more users per iteration. Take average NMAE from each split.
%\cite{Stern2009, Lam2008}: Divide users in two sets (90:10), train model on the 90\%. For each test user train the model on a random subset of T\% of their ratings for T = 5, T=75, then use the model to predict the remaining ratings for the user

%Clues
% http://delivery.acm.org/10.1145/570000/564421/p253-schein.pdf?ip=129.241.103.83&id=564421&acc=ACTIVE%20SERVICE&key=CDADA77FFDD8BE08%2E5386D6A7D247483C%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=419807217&CFTOKEN=62708098&__acm__=1394537427_86c608d0d7733db023faa5a09da46de7

\subsection{Online Evaluation}

Instead of doing offline evaluations on the system, one could also run large
scale experiments on a deployed system. Such experiments evaluate the
performance of recommender systems on real users which are oblivious to the
conducted experiment. The real effect of a recommender system depends on a
variety of factors such as user’s intent, the user’s context and how the
recommendations are presented to the user. All these factors are hard to
capture in an offline setting. Thus, the experiment that provides the strongest
evidence as to the true real value of the system is an online evaluation, where
the system is used by real users to perform real tasks

\subsubsection{Online Evaluation Metrics}

Online studies in recommendations and advertisement usually measure the
click-through-rate (CTR) of the recommendations, which aligns with financial
incentives and implicitly factors in accuracy, novelty, diversity, etc.,
according to the preferences of the distribution of users.  The
click-through-rate of an algorithm is defined as the number of clicks your
recommendations get divided by the total number of recommendations that have
been made. A high CTR therefore indicates that your system is doing well.

\begin{equation}
CTR = \frac{Clicks}{Recommendations}
\end{equation}

\subsection{What To Use}

The main idea behind a recommendation system is to produce a set of items which are of interest to the user.
For the system to be successful, this sets of items needs quality.
But what needs to be considered when determining the quality of the recommendations?
According to a user study~\cite{Pu:2011:UEF:2043932.2043962} the most central aspects to this quality is:

\marginpar{as list or in table, or not}

\paragraph{Perceived accuracy}.
The degree of how well the user perceives the recommended items match the actual want of the user.

\paragraph{Novelty}.
The degree of new and interesting items recommended for the user.

\paragraph{Attractiveness}.
How well the recommended items are able to evoke interest or desire in the user.

\paragraph{Diversity}.
How different the recommended items are.

\paragraph{Context compatibility}.
How the system uses contextual factors to supply the recommendations with more personalized recommendations.

% \todo{RMSE, MSE, P/R \dots}


\subsubsection{The Good}
Since the data at hand is mainly implicit feedback and this data is sparse, some natural ways of approaching the evaluations task would be:

\textbf{MPR}
\begin{itemize}
	\item good
	\item Possible to produce a score for a recommender system which relies on implicit feedback
	\item Usable even though there are no feedback indicating undesired items
	\item Does not need a ranked list of the actual preferences of the user
	\item bad
	\item Needs distinct ranking of the different items
\end{itemize}

\textbf{MAP}
\begin{itemize}
	\item good
	\item Possible to produce a score for a recommender system which relies on implicit feedback
	\item Usable even though there are no feedback indicating undesired items
	\item Does not need a ranked list of the actual preferences of the user
	\item Differentiates between predicted more desired items and those that are not
	\item bad
	\item Needs distinct ranking of the different items
\end{itemize}


\subsubsection{The Bad}
\textbf{RMSE}
\begin{itemize}
	\item good
	\item Well known, so it produces a clear score of the system
	\item bad
	\item Needs the actual rating of the items
	\item Needs a predicted rating for the items
	\item Not easy to gather reliable information about undesired items trough implicit feedback
\end{itemize}



\subsection{Evaluation Metrics}

% Evaluate rating
\subsubsection{Root-mean-square error (RMSE)}
Often used to measure difference between a set of predicted values with a set of actual values.

\begin{equation}
	RMSE = \sqrt{\frac{\sum_{t=1}^{n}{(\hat{y_t} - y_t)^{2}}}{n}}
\end{equation}
% \begin{equation}
% 	RMSE = \sqrt{\frac{1}{\vert T \vert} \sum_{(c,s) \epsilon T}{(\hat{u(c,s)} - u(c,s))}^{2}}
% \end{equation}

How to calculate the $RMSE$ value is shown in~\ref{equation:ndcg}.
Here $y_t$ is the actual value and $\hat{y_t}$ is the estimated value.
This approach is often used in evaluating recommender systems producing ratings for domains such as the movie domain.


% implicit
\subsubsection{Mean Percentage Ranking (MPR)}
This measure is a recall-oriented metric.
A known issue with implicit feedback is and often lack of the actual user's preference.
This approach is used to measure the user satisfaction of items in an ordered list.

\begin{equation}
	MPR = \frac{\sum_{u,i}{r_{ui} * rank_{ui}}}{\sum_{u,i}{r_{ui}}}
	\label{equation:mpr}
\end{equation}

How to calculate $MPR$ is shown in ~\ref{equation:mpr}.
A list of all the items for user $u$ is ordered based on the rank $rank_{ui}$ of the $u$.
Where $rank_{ui}$ is the percentile rank of item $i$ in this list for $u$.
$rank_{ui} = 0$ means that $i$ is the most preferred item for $u$.
$r_{ui}$ indicates whether $u$ has consumed $i$ or not.
This makes a $MPR$ value of 0\% to be the most preferred value, and a value of 50\% meaning a near randomly produced list.


\subsubsection{User-Centric Evaluation}
\cite{Pu:2011:UEF:2043932.2043962}


% Evaluate list of recommendations
\subsubsection{Mean Average Precision (MAP)}
MAP~\cite{Manning:2008:IIR:1394399} measures quality across recall levels.

% \begin{equation}
% 	MAP(N) = \frac{1}{|N|}\sum_{i=1}^{|N|}{\frac{1}{n_i}}\sum_{j=1}^{n_i}{Precision(R_{ij})}
% 	\label{equation:apn}
% \end{equation}
% $Precision(R_{ij})$ is the precision of the items in an ordered predicted list to the actual preferred list.

\begin{equation}
	ap@n = \sum_{k=1}^{n}{\frac{P(K)}{min(m,n)}}
	\label{equation:apn}
\end{equation}

\begin{equation}
	MAP@n = \sum_{i=1}^{N}{\frac{ap@n_i}{N}}
	\label{equation:map}
\end{equation}

How to calculate the $MAP$ value is shown in~\ref{equation:map}.
\ref{equation:apn} calculates the average precision at $n$ for a user.
From \ref{equation:apn}, $P(K)$ is the precision at $k$ in the item list, $n$ is the maximum number of predicted items and $m$ is the actual length of the predicted items list.
\ref{equation:map} calculates the mean of all the values from \ref{equation:apn}.


\subsubsection{AP correlation}
$AP correlation$~\cite{Yilmaz:2008:NRC:1390334.1390435} measures the overall precision and is a variant of $Kendall's tau$.
It counts the amount of items correctly placed in a ordered predicted rank list $list1$ and a list of the actual rank ordering of the preferences of the user $list2$.

\begin{equation}
	AP = \frac{2}{N - 1} * \sum_{i=2}^{N}{(\frac{C(i)}{i - 1})} - 1
	\label{equation:ap}
\end{equation}

How to calculate the $AP correlation$ value is shown in ~\ref{equation:ap}.
$C(i)$ is the number of items ranked correctly above rank $i$.
The value of AP is between -1 and 1, where a score of 0 means that $list1$ can be considered a randomly generated list and 1 is a perfect match with the actual list $list2$.


\subsubsection{Normalized Discounted Cumulative Gain (nDCG)}
$nDCG$ measures the graded relevance of the recommended item, the ranking quality or the usefulness of the recommended item based on its rank position.
It is often used to measure the performance of web search recommendation systems.

\begin{equation}
	DCG_k = \sum_{i=1}^{k}{\frac{2^{rel_i}-1}{log_2(i+1)}}
	\label{equation:dcg}
\end{equation}

\begin{equation}
	nDCG_k = \frac{DCG_k}{IDCG_k}
	\label{equation:ndcg}
\end{equation}

How to calculate the $nDCG$ value is shown in~\ref{equation:ndcg}.
Where $k$ is the maximum amount of suggested items, and $rel_i$ is the graded relevance of the result at position $i$.
$IDCG_k$, from~\ref{equation:ndcg}, is the ideal $DCG_k$ value.
This is the result list sorted on relevance.



\subsubsection{Receiver Operating Characteristics (ROC))}
The $ROC$ curve is the recall against the fallout, where recall is the amount of relevant retrieved items amongst all the correct items (true positive).
Fallout is the amount of retrieved items which is not relevant amongst all the non relevant items (false positive).
The goal is to maximize the recall while minimizing the fallout.


\subsubsection{MRR}
\subsubsection{..}

