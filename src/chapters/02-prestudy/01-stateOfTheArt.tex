% !TEX root = ../../report.tex

\section{State Of The Art}
\label{sec:SotA}

This section will present and discuss previous work in the field of recommender systems, existing solutions to the cold-start problem and fashion recommender systems. The section will also contain a discussion of the domain specific challanges of designing a recommender system for the fashion domain.

\subsection{Recommender Systems foundations}

Recommender systems have become an important research topic since the introduction of Tapestry \cite{Goldberg1992}, the first collaborative filtering system back in 1992. Recommender systems now play an important role in many of the most popular web-sites such as Amazon, YouTube, Netflix, Tripadvisor, Last.fm, and IMDb. In its most common formulation the recommendation problem is reduced to the problem of estimating the preference/rating of items that have not been seen by a user. Usually, this estimation is based on ratings previously given by the user to other items in the collection. Once we have estimated these ratings we can recommend the items with the highest rating to the user. These recommendations relate to various decision-making processes, such as what items to buy, what music to listen to, or what online news to read.

Recommender systems are usually classified into the following categories, based on how the recommendations are made \cite{Adomavicius2005}

\begin{itemize}
\item \emph{Content-based recommendations:} The user will be recommended items with similar content to the ones the user preferred in the past;
\item \emph{Collaborative recommendations:} The user will be recommended items that people with similar testes and preferences have liked in the past;
\item \emph{Hybrid approaches:} These methods combine collaborative and content-based methods.
\end{itemize}

\subsubsection{Content Based Filtering}

In a content-based system, we must construct for each user $c$ a \emph{profile} $ContentBasedProfile(c)$, which is a record or collection of the attributes which characterizes each item $Content(s)$ of all the items $s_{i} \epsilon S$ previously assigned an utility $u(c, s)$ by user $c$. For our example in a fashion recommender system the content-based recommender system tries to understand the commonalities among the items user $c$ has rated highly in the past (color, brand, store, price, etc.). Then recommend items that have a high degree of similarity to these items. $ContentBasedProfile(c)$ can be designed as a vector of weights $(w_{c1} ... w_{ck})$, where each weight $w_{ci}$ denotes the importance of the keyword $k_{i}$ to user $c$.

Items that can be recommended to the user can often be stored in a database table. Figure \ref{figure:contentbaseddb} shows a simple database with rows describing 5 items that have been rated by 3 users. The column names starting with $X_{n}$ are the properties of the items, often referred to as "attributes".

\begin{figure}[H]
    \includegraphics[width=5in]{image/contentbaseddb.png}
    \centering
    \caption[A clothing database]{A clothing database. Rows are items, columns are users and properties of the items and predictable trust statements}
    \label{figure:contentbaseddb}
\end{figure}

From the rating matrix and content properties one can then construct a $ContentBasedProfile(c)$ for each user $c$, for user Arya one could image it could look something like this.

\begin{figure}[H]
    \includegraphics[width=3in]{image/contentprofile.png}
    \centering
    \caption[Content Profile Example]{Content Profile Example}
    \label{figure:contentbaseddb}
\end{figure}

The recommendation process consists of matching up the attributes of the user profile against the attributes of an item. The result is a relevance judgment that represents the user's level of interest in that object. The utility $u(c, s)$ of item $s$ for user $c$ is estimated based on the utilities $u(c, s_{i})$ assigned by user c to items $s_{i} \epsilon S$ that exhibit a similarity to item $s$. E.g. for the user Arya items with the attributes low price and low formality could safely be recommended as they fit her user profile. The utility function $r(u, i)$ is usually defined as:
\begin{equation}
r(u,i) = score(ContentBasedProfile(u), Content(i)).
\end{equation}

\subsubsection{Collaborative Filtering}

The goal of collaborative filtering methods is to suggest new items or to predict the utility $u(c, s)$ of a certain item s for a particular user c based on the user's previous activities and/or likings and similarity to other users. In a typical CF scenario, there is a list of $n$ users $C = {c_{1}, ... c_{n}}$ and a list of $m$ items $S = {s_{1},...s_{m}}$. Each user $c_{i}$ has a list of items $S_{si}$, which the user have expressed her opinion about, which makes up our rating matrix of size $SxC$. More formally, the utility $u(c, s)$ of item $s$ for user $c$ is estimated based on the utilities $u(c_{j}, s)$ assigned to item $s$ by the users $c_{j} \epsilon C$, which can be considered "similar" to the active user $c$. This is exemplified in Figure \ref{figure:ratingmatrix}. For example, in our fashion recommender system, in order to recommend clothes to user $c$, the collaborative filtering method must find the "peers" of users $c$, which share the same tastes in clothes (user which tend to rate clothes similarly). Then, recommend the clothes that are most liked among these "peers".

\begin{figure}[H]
    \includegraphics[width=5in]{image/ratingmatrix.png}
    \centering
    \caption[Collaborative filtering rating matrix]{Collaborative filtering rating matrix}
    \label{figure:ratingmatrix}
\end{figure}

Researchers have devised a number of collaborative filtering algorithms that can be divided into two main categories: Memory-based and Model-based algorithms \cite{Su2009}.

\textbf{Memory-based Methods}

Memory-based Collaborative Filtering methods utilize the entire user-item database to generate predictions. More formally, the value of an unknown utility $u(c,s)$ for user $c$ and item $s$ is usually computed by taking the weighted average of the utilities assigned by the $N$ most similar users for the same item $s$. The similarity between user $c$ and $c'$, $sim(c, c')$ is used as the weight. The more similar a user $c'$ is to $c$, the more weight is given to the utility $u(c', s)$, and thus, will carry more weight in the prediction for $u(c,s)$.

\begin{equation}
\label{equation:cfratingprediction}
u(c,s) = k * \sum_{c' \epsilon C} sim(c, c') * u(c',s)
\end{equation}

Where k serves as a normalization factor, usually being $1/|C|$. Various appraoches have been used to compute the similarity $sim(c, c')$ between the users. Generally these approaches are based on the rating similarities for items both users have rated. The most popular similarity measure is The Pearson Correlation Coefficient. Equation \ref{equation:pearson} shows how to calculate the Pearson Correlation Coefficient between two users $c$ and $c'$, Here $S_{cc'}$ is the set of items the user have in \emph{common}.

\begin{equation}
sim(c, c') = \frac{\sum_{s \epsilon S_{cc'}} (u(c, s)-\bar{u_{c}})(u(c',s)-\bar{u_{c'}})}{\sqrt[â€¢]{\sum_{s \epsilon S_{cc'}} (u(c, s)-\bar{u_{c}})^{2}(u(c',s)-\bar{u_{c'}})^{2}}}
\end{equation}

Where $u_{c}$ is the mean utility of user $c$. The Pearson Correlation Coefficient and other similarity measures such as consine based approaches are more commonly known user-based collaborative filtering.

Item-based Top-N Recommendation methods calculates the similarity between items instead of users. In these approaches, the historical information is analyzed to identify the relations between items such that a purchase of another item (or set of items) often leads to the purchase of another item. These models are often used since they quickly can recommend a set of items, and have shown to produce recommendation results comparable or better than traditional user-based approaches \cite{Karypis2001}.

The algorithm first computes the $k$ most similar items for each item according to the ratings given by users they both share. Items that often are rated similarly by users are considered more similar than items which share few similar ratings. Figure \ref{figure:itemsim} illustrates the process of finding the item-similarities.

\begin{figure}[H]
    \includegraphics[width=5in]{image/itemsim.png}
    \centering
    \caption[Item-item similarity]{Item-item similarity}
    \label{figure:ratingmatrix}
\end{figure}

Each user a set $S_{c}$ of items previously rated by the user which we want to compute top-N recommendations for. First, we identify the set $C$ of candidate items recommended items by taking the union of the k most similar items and removing each of the items in the set $S_{c}$ the user already has rated; then calculate the similarities between each item of the set $C$ and the set $S_{c}$, using only the $k$ most similar items for each item in $S_{c}$. The resulting set of items in $C$ are sorted in descending order of similarity and will be the recommended as the item-based Top-N list \cite{Karypis2001}.

\textbf{Model-based Methods}

As the name implies, Model-based approaches provide recommendations by first developing a model of the user ratings, which is then used to make predictions. Algorithms of this category often take a probabilistic approach and envision the rating prediction task as computing the expected utility $u(c,s)$, given his/her rating on other items. Popular approaches include Bayesian networks, clustering models, singular value decomposition and Markov decision process based models.

Latent factor models comprise an alternative approach by transforming both items and users to a latent factor space. The latent factor space tries to explain the ratings by characterizing both items and users on factors automatically inferred from the data. The most popular latent factor models are based on matrix factorization techniques \cite{Koren2009}.

The main idea behind matrix factorization is just as its name implies, factorize a matrix, finding two or more matriches such that when you multiply them you get back the original matrix. Matrix factorization can be used to discover latent factors underlying the interactions between the users and items. These factors \emph{explain} how a user rates an item (i.e. that a user would give high ratings to a certain shirt if he likes the brand, or if the color is nice). If we can discover these factors, we should be able to predict a rating with respect to a certain user and a certain item based on the correlation between their factors.

Consider the following rating matrix R \ref{table:RatingMatrix} containing the rating of four users $C$ for four items $S$, giving us a $CxS$ matrix with explicit ratings on a scale from 1 to 5.

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
5.00 	& 5.00	& 2.00 & - 	  \\ \hline
2.00 	& -		& 3.00 & 5.00 \\ \hline
 - 		& 5.00	& -    & 3.00 \\ \hline
3.00	& - 	& -	   & 5.00 \\ \hline
\end{tabular}
\label{table:RatingMatrix}
\caption{Rating matrix R ($C \times S$)}
\end{table}

A matrix factorization model map both users and items to a joint latent factor space of dimensionality $f$, where $f$ is the number of latent factors. The number of latent factors are usually determined by using a hold-out dataset or cross-validation by evaluating the prediction error experimenting with different values. It is also worth mentioning that this is a trade off between model building complexity and accuracy as having more features makes the model building more expensive. Each user $c$ is associated with a vector $p_{c} \epsilon \mathbb{R}^{f}$, and each item $s$ is associated with a vector $q_{s} \epsilon \mathbb{R}^{f}$. Giving us a matrix Q containing the user factors and a matrix P containing the item factors as exemplified in Figure \ref{figure:matrixdecomp}.

\begin{figure}[H]
    \includegraphics[width=5in]{image/matrixdecomp.jpg}
    \centering
    \caption[Matrix decomposition of the rating matrix R]{Matrix decomposition of the rating matrix R}
    \label{figure:ratingmatrix}
\end{figure}

User-item interactions are modeled as inner products in that space. For a given item $s$, the elements of $q_{s}$ measures the extent to which the item posessess those factors, positive or negative. Likewise, for a given user $c$, the element $p_{c}$ measures the extent of interest that user has in items that are high on the corresponding factors. The resulting dot product $\hat{u(c,s)}$ captures the overall interest of the user in the characteristics of the items.

\begin{equation}
u(c,s) = p_{c}^{T}q_{s} = \sum_{k=1}^{f} q_{sk}p_{kc}
\end{equation}

Then problem then, is to discover the user factor matrix $P$ and the item factor matrix $Q$ such that their product approximates the original rating matrix $R$.

\begin{equation}
R \approx Q \times P^{T} = \hat{R}
\end{equation}

To learn the factor vectors the system minimizes the regularized square error on the set of known rating $K$.

\begin{equation}
\label{equation:minimize}
min_{q, p} = \sum_{(c,s)\epsilon K} (u(c,s) - p^{T}_{c}q_{s})^{2} + \lambda ( \Vert q_{s} \Vert ^{2} + \Vert p_{c} \Vert ^{2})
\end{equation}

However, it is important to remember that our goal is generalize beyond the observed ratings, in a way that we can predict future unknown ratings. The system should therefore avoid overfitting the data by regularizing the learned parameters, whose magnitudes are penalized. $\lambda$ controls the extent of regularization, and if much like $f$, often determined by cross-validation. Two possible approaches to minimizing Equation \ref{equation:minimize} is to use Stochastic Gradient Descent or Alternating Least Squares \citep{Koren2009}.

Given that $f = 3$, we might end up with the following matrix $P$ and $Q$

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|}
\hline
1.81	&1.62	&0.74\\ \hline
2.66	&1.71	&-1.08\\ \hline
1.73	&-0.23	&0.78\\ \hline
3.16	&-0.24	&0.90\\ \hline
\end{tabular}
\label{table:ItemFeature}
\caption{User factor matrix $P$ ($C \times f$)}
\end{table}

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
1.12	&	1.49	&	0.48\\ \hline
1.31	&-0.52	&0.59\\ \hline
1.13	&0.67&	-0.52\\ \hline
1.39	&0.05&	0.45\\ \hline
\end{tabular}
\label{table:UserFeature}
\caption{Item factor matrix $Q$ ($S \times f$)}
\end{table}

Giving us the following rating prediction matrix $\hat{R}$

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
4.79	&5.01	&1.97	&3.61 \\ \hline
1.97	&1.96	&2.85	&4.80 \\ \hline
2.75	&4.71	&1.40	&2.94 \\ \hline
2.93	&3.30	&2.74	&4.78 \\ \hline
\end{tabular}
\label{table:PredictionMatrix}
\caption{Rating prediction matrix $\hat{R}$}
\end{table}

As you can see the values of known ratings in Table \ref{table:RatingMatrix} are fairly similar to the corresponding ratings in the rating prediction matrix.

\subsubsection{Hybrid approaches}

A term \emph{hybrid recommender systems} is used to describe any recommender system that combines multiple recommendation techniques together to provide recommendations. Burke et. al. \cite{Burke2002} identified seven different classes of hybrid recommender systems:

\begin{itemize}
\item Weighted: The score of different recommendation components are combined numerically
\item Mixed: Recommendations from different recommenders are presented together.
\item Feature Combination. Features derived from different knowledge sources are combined together and given to a single recommendation algorithm
\item Feature Augmentation: One recommendation technique is used to compute a feature or set of features, which is then part of the input to the next technique.
\item Cascade: Recommenders are given strict priority, with the lower priority ones breaking ties in the score of the higher ones
\item Meta-level: One recommendation technique is applied and produces some sort of model, which is then the input by the next technique
\end{itemize}

Most commonly hybrid systems are built by combining collaborative and content-based methods in an attempt to mitigate the limitations the approaches suffer individually. Adomavicius and Tuzhilin \cite{Adomavicius2005} lists the following approches to building hybrid recommender systems:

\begin{itemize}
\item Implementing the systems separately and combining their predictions
\item Incorporating content-based characteristics into a collaborative approach
\item Incorporating collaborative characteristics into a content-based approach
\item Constructing a general unifying model that incorporates both content-based and collaborative characteristics
\end{itemize}

\subsubsection{Challenges}



\subsubsection{Terminology}



\subsection{System Cold-start Handling}

In the literature, the term cold is used about an object in a system, or a
whole system, which is new \cite{Schein2002, Park2006}. Cold-start scenarios in recommender systems are
situations in which little/no prior events, like ratings or clicks, are known
for certain users or items. The cold-start problem can be divided into three sub problems:

\begin{itemize}
  \item \emph{Cold-start system}: A situation where we only have new users and
  little or no ratings for the items.

  \item \emph{Cold-start item}: The problem of recommending items that are new
  to the system, which have not received any ratings.

  \item \emph{Cold-start user}: The problem of giving accurate recommendations
  to a user who is new to a recommender system.
\end{itemize}

For example in a scenario where the average item in an item collection have 5 000 ratings, a new item with only 5 ratings would be considered a \emph{cold-item}. Likewise, in a recommender system where the average user has rated 25 items, a user who only has rated 2 items, would be considered a \emph{cold-user}.

The cold-start system problem is mainly a collaborative filtering problem, and
can be seen as a combination of the cold-start user and cold-start item problem
where the majority of the users are new to the system and have expressed few
preferences, resulting in a very sparse user-item matrix, rendering traditional collaborative-filtering methods futile. Most traditional algorithms only work effectively in environments where the datasets has high information density. In fact, in extreme cases, when data
is very scarce, simple non-personalized recommendations based on global
averages can outperform collaborative-filtering algorithms \cite{Park2006}

In content-based systems, new items can easily be
recommended using the content information of the item, making it a popular
solution to the \emph{cold-start item} problem. This problem is more
severe in collaborative-filtering systems where items are only recommendable if
they have been rated by substantial amount of users. New items will therefore not be
recommendable before multiple users somehow stumble upon the new item while
e.g. browsing the item collection, unless additional measures are taken to
solve this problem. To solve the new-item problem, there are two commonly used (simple) solutions often used in E-commerce websites:

\begin{itemize}ollaborative filtering
\item Advertising at the homepage/front-page of the website, putting the new
items in an eye catching position. This solution, however, may this result in
that some users, which don't like these new items, might leave the website.
\item Requesting the user to choose one or more of his/hers categories while
registering for the site, and recommend items from the selected categories.
This approach however, requires active user involvement and complicates the
sign up process. Many users might chose not to give up any
personal interest information, thus the user group covered by this solution could end up not being large enough.
\end{itemize}

The cold-start user problem is present both in content-based and
collaborative-filtering systems. Collaborative Filtering is based on the idea that like-minded users have similar tastes and
preferences. A new user therefore poses a challenge to a CF recommender, since
the system has no knowledge about the preferences of the new user, and can
therefore not provide any personalized recommendations. The system must therefore acquire some
information about the new user before it can start making personalized recommendations. In a typical domain, for example in the domain of books, the number of items is very large (in the order of tens of thousands) while the number of items rated by every single user is in general small (in the order of dozens or less). This means that it is very unlikely two random users have rated any items in common and hence they are not comparable. The system will therefore most likely struggle to find users. Similarly, in content-based systems, the lack of ratings
given by the target user, means that the target user will have a limited
content-profile, since the users content profile is constructed using content-information from his/hers rated items.
with tastes that are \emph{truly} similar to the target user. In both cases, recommendation quality is most likely bound to suffer.

In this section we will present a few different solutions to the cold-start problem, focusing mainly on \emph{complete} solutions to the cold-start problem and all the sub problems. Most traditional algorithms only work effectively in environments where
the datasets has high information density. In fact, in extreme cases, when data
is very scarce, simple non-personalized recommendations based on global
averages can outperform collaborative-filtering and content-based algorithms.
Pure collaborative filtering cannot help in a cold-start setting, since no user
preference information is available to form any basis for recommendations

%Can the different approaches be classified? E.g. 3 main categories of approaches
	%Initial categorization
		%Interview process
 		%Hybrid approaches
 		%Key figures / Seed users
 		%Filterbots
 		%Trust-aware / Trust propagation


\subsubsection{Trust Aware Recommender Systems}

Due to the popularity of social networks such as Facebook, more and more
researchers turn to incorporate the social relationships (e.g. trust) of users
to help complement usersâ€™ preference in addition to item ratings, in order to overcome the limitations of existing recommender systems. For example, when looking for movie recommendations we often turn to our friends which we share a similar taste in movies with. Trust can be defined as: "believe in the reliability, truth, or ability of", and in the context of recommender systems a trusted user would be a user you trust to provide you good recommendations. Trust relationships of users are often employed in order to correlate more potential raters for the active users who require recommendations \cite{Massa2004, Massa2007}. Massa et. al. \citep{Massa2004} also show that some of the weaknesses of recommender systems such as data sparseness and their susceptibility to shilling attacks could be alleviated by incorporating trust. 

% The formals
In \cite{Massa2004}, Massa et. al. describes their Trust-Aware recommender system archictecture.
To capture all the trust statements we need a $CxC$ matrix, where $C$ is the number of users, since each user is allowed to express a trust value in every other user. This matrix will make up our trust network among the users. If $u$
trusts $v$, then there is a value $t_{u,v}$ for this trust which is a real
number in $[0,1]$. Zero means no trust and one means full trust. E.g. in the case of the Epinions dataset \cite{Epinions}, users can explicitly state whether they trust or distrust a user [1, -1], i.e. reviewers whose reviews and ratings they have consistently found to be valuable or reviewers which they find consistently offensive, inaccurate or not valuable. In decentralized environments where everyone is free to create content and there
is no centralized quality control entity, evaluating the quality of the content becomes an important issue. This phenomenon can be observed in online
marketplaces such as E-bay where users can create "fake" auctions, peer-to-peer networks where peers can enter corrupted items. In these environments, it is often a good strategy to delegate the quality assessment task to users themselves. E.g. \emph{Ebay.com} allows users to express their level of satisfaction after every interaction with another user.

This additional information (trust statements) can be organized in a trust network and a trust metric can be used to predict the
trustworthiness of other users as well (for example, friends of friends). The idea here is to not search for similar users as CF does but to search for
trust-able users by exploiting trust propagation over the trust network. The items appreciated by these users are then recommended to the active user. 

%How do we get this matrix?
%Chen et. al. \cite{Chen2010} proposed a framework to estimate trust between facebook users...

In addition to the trust network we will also have a Rating matrix of size $CxS$, where $S$ is
the number of items. This rating will not differ from a standard rating matrix,
which are used in traditional collaborative filtering systems. The value $u(c,s)$, is the rating given by user $c$ to item $s$, the rating scale may differ from system to system.

% Web of Trust - Figure explanation
Using explicit trust statements, it is possible to predict trust in unknown
users by propagating trust. Consider the example shown in Figure
\ref{figure:weboftrust}. User $A$ has issued a trust statement in $B$ and $C$; hence
$B$ and $C$ are in the web of trust of $A$. Using these explicit trust statements, it
is possible to predict trust in unknown users by propagating trust, making it
possible to infer something about how much user $A$ could trust $D$.

\begin{figure}[H]
    \includegraphics[width=5in]{image/webofTrust.png}
    \centering
    \caption[Trust Network]{Trust Network. Nodes are users and solid edges are trust statements. The dotted edge is one of the undefined and predictable trust statements (Adopted from \cite{Massa2004})}
    \label{figure:weboftrust}
\end{figure}

% Architecture
The systems takes as input the trust network (representing the trust
statements) and the ratings matrix (ratings given by users to items) and
produces, as output, a matrix of predicted ratings that the users would assign
to the items. Figure \ref{figure:trustarchictecture} shows a conceptual
overview of the trust-aware recommender system architecture.

\begin{figure}[H]
    \includegraphics[width=5in]{image/trustawarearchitecture.png}
    \centering
    \caption[Trust-Aware Recommender System Architecture]{Trust-Aware
    Recommender System Architecture (Adopted from \cite{Massa2004})}
    \label{figure:trustarchictecture}
\end{figure}

The \emph{Trust Metric} module takes the trust network as input, and exploits
trust propagation in order to predict, for every user, how much she could trust
other users. Trust metrics can either be local and global. Global trust metrics
produces an estimated trust matrix with all the rows equal, meaning that the
estimated trust in a certain user (column) is the same for every user (row). A
simple local trust metric could e.g. for each user assign to every other user a
predicted trust based on her minimum distance from the source user. More
sophisticated ones could also be employed. If we again consider Figure
\ref{figure:weboftrust}, we could employ a local trust metric where the
predicted trust is based on the minimum distance from the source user. If we
set the maximum propagation distance $d$, a user at distance $n$ from the
source user will have a predicted trust value of:

\begin{equation}
t_{u,v} = (d-n+1)/d
\end{equation}

Giving users not reachable within the maximum propagation distance a trust of
$0$. Using user $A$ as the source user, the users at distance $1$ ($B$ and $C$)
would get a trust value of $(4-1+1)/4 = 1$, while the user at distance 2 (D)
would get a predicted trust value of $(4-2+1)/4 = 0.75$. Meaning that we will
have a linear decay in trust based on the distance from the source user.

Massa et. al. \cite{Massa2007} experimented with both local and global trust
Metrics. They used the PageRank algorithm as a global trust metric. PageRank
tries to infer the authority of every single user by examining the structure of
the network. The algorithm follows a simple idea: if a link from user $A$ to user
$B$ represent a positive vote casted by $A$ to $B$, then the global rank of a page
depends on the number (and quality) of the incoming links. The trust values
assigned by users to users are used to predict the trustworthiness of unknown
users. Their findings, not surprisingly, indicate that Global Trust Metrics are
not suited for the task of finding good neighbors, especially for providing
personalized recommendations, but is more suited to applications such as \emph{Ebay.com} to find untrustworthy users. As a local trust metric they chose MoleTrust, which is a depth-first graph walking algorithm with a tuneable trust horizon
which allowed them to experiment with different propagation distances.

The \emph{Similarity Metric} module computes the user similarities, this is one
of the standard steps of any traditional collaborative filtering technique,
user similarities can be found e.g. by using the Pearson Correlation
Coefficient. The intuition is that, if a user rates in a similar way to another
user, then her ratings are using for predicting the ratings for that users.

The \emph{Rating Predictor} can use the neighbors from the user similarity
matrix, the estimated trust matrix or a combination of both in order to
calculate the predicted ratings.


% Using a Trust Network to Improve Top-N Recommendation

Jamali et. al. \cite{Jamali2009} propose two different methods for getting
around the cold-start user problem using a trust network.

Their first approach called \emph{Random Walk} only utilize the trust network to provide recommendations. Starting from the active user $u$, we perform a random walk on the trust network. Each
random walk stops at a certain user. Then the items rated highly by that user
will be considered as recommended items, ordered according to the ratings
expressed by that user. We perform several random walks to gather more
information and compute a more confident result. The estimated rating of each
item is the average of ratings for that item over all raters considered. At the
end, we output items with the highest estimated rating as top-N recommended
items.

Their second approach called \emph{Combined Approach} uses both user-user similarities and the trust network to provide recommendations. In this approach we compute the top $K$ trusted users in the network and rank
the items rated by these trusted users to compute top-N recommended items. The top $K$ trusted users can either be found by
\emph{Breadth First Search} or \emph{Random Walk in the social network}. We use the collaborative filtering approach to compute another set of top-N
recommended items. Finally, we merge these two lists to produce a combined list of top-N recommended items. Items returned by CF is denoted as $CF_{u}$, while the items returned by Trust-based approach are denoted $TR_{u}$.

\begin{equation}

\[
 \hat{u}(c,s) =
  \begin{cases}
   \frac{u_{tr_{c,i}} + u_{cf_{c,i}}}{2} & i \in TR_{u};i \in CF_{u} \\
   \hat{u_{tr_{c,i}}}       & i \in TR_{u};i \not \in CF_{u} \\
   \hat{u_{cf_{c,i}}}		 & i \in CF_{u};i \not \in TR_{u}
  \end{cases}
\]
\end{equation}

The top-N items with the highest value of $\hat{u}(c,s)$ will be returned as the top-N recommended items. The authors also experimented with weighted averaging in the case where the item appear in both $TR_{u}$ and $CF_{u}$.

Their approaches showed great improvements in recall for cold-start users,
improving the performance by 50$\%$ over standard CF methods. The main improvements however, are the coverage of the trust-based approaches, while still maintaining the same or even slightly better precision than the standard CF methods.

% Trust-aware Recommender Systems + Trust-Aware Collaborative Filtering for Recommender Systems
% Article Comments:
% 	Requires user involvement (explicitly express trust) - is this a acceptable?

%Massa et. al. \cite{Massa2004, Massa2007} propose using trust information
%explicitly expressed by the users. Users are allowed to state how much they
%consider every other user trustworthy that, in the context of recommender
%systems, is related to how much they consider the ratings provided by a certain
%user valuable and relevant. 

% Alleviating the Sparsity Problem of Collaborative Filtering Using Trust Inferences
% Article Comments:
%	- Pretty good general model for dealing with sparsity
%	- Requires no additional information such as product details, demographic information about userstrus

Papagelis et. al. \cite{Papagelis2005} proposed to alleviate sparsity using
trust interfaces. Trust interfaces are transitive associations between users in
the context of an underlying social network, and does therefore not require users to explicitly express their trust in other users, unlike the approaches described above. Their approach is built on the
idea of social networks in recommender systems. Instead of reducing the
dimension of the user-item matrix, in an attempt to make it more informative,
they propose a method that permits to define transitive properties between
users in the context of a social network. Their approach does not require the
user to explicitly express which users they trust, the trust information is
inferred from the underlying social network of the rating matrix.

\begin{figure}[H]
    \includegraphics[width=5in]{image/trustnetwork.png}
    \centering
    \caption[Underlying Social Networks in Recommender Systems]{Underlying Social Networks in Recommender Systems}
    \label{figure:cfsocialnetwork}
\end{figure}

Due to the number of ratings that exist in recommendation systems, underlying
social networks are very sparse. There are cases in which insufficient or loss
of information is detrimental for the recommendation algorithms. Consider
Figure \ref{figure:cfsocialnetwork}, classic CF will associate only the users
which have co-rated an item (User $1$ and $2$ and user $1$ and $3$). To deal
with the problem of a sparse social network, it is possible to infer trust
between a source user $S$ and a target user $T$ through an intermediate user
$N$ (User $2$ and $3$ are connected through the intermediary user $1$), as
shown by the \emph{Inferred Association} arrow. According to this process,
trust is propagated in the network and associations between users are built,
even if they have no co-rated item. Trust paths can be of variable length,
depending on the number of associations that one needs to traverse in order to
reach the target user.

% Trust-paths
For example, if the trust $T_{1 \rightarrow 2} = 0.7$ based on 5 co-rated items
and $T_{1 \rightarrow 3} = 0.35$ based on 2 co-rated items, then the trust
between user $2$ and $3$ through $1$ is, $T_{2 \overrightarrow{1} 3} = 0.6$
($\frac{0.7*5}{5+2} + \frac{0.35*2}{7}$)

In order to express the subjective notion of trust, the authors set up a
confidence model that is assigned to each direct association of the network
that expresses the reliability of the association. Confidence is related to the
number of co-rated items between two users. The confidence scores are all
expressed in relation to the most confident association for each user.

\begin{equation}
C_{s \rightarrow t} = \frac{n(I_{s} \cap I_{t}}{n(I_{s} \cap I_{u_{MAX_CONF}})}
\end{equation}

Using the above example, assuming that the maximum number of co-rated items
user $1$ has with any user is 7, $C_{1 \rightarrow 2} = \frac{5}{7}$

% Results/Findings
The authors achieved improved accuracy for all sparsity levels. With a sparsity
level of $99.9\%$ the 2-HOP CF (friend of friends) increased the MAE
performance by $17\%$ over standard CF methods.


%TODO - READ: A Matrix Factorization Technique with Trust Propagation for Recommendation in Social Networks

%TODO - Discussion on Trust-Aware Recommenders
%What to include:
%	Discussion on the authors findings
%	Scalability
%	How can we incorporate trust?

The main advantage is not improved accuracy, but improved coverage. Massa et. al. \cite{Massa2007} found trusted users to be good predictors. For the cold-start users they achieved a MAE of 0.674 when looking at friends of friends, compared to traditional collaborative filtering which scored 1.094. The difference is very high, and particularly relevant as it is important for recommender systems to generate personalized recommendations as soon as possible for new users, so that these users appreciate the system and keep using it. The authors also experimented with
propagating the trust up to a distance of 4. By using the Pearson Correlation
coefficient they found on average 160.73 neighbors. However just propagating a
few steps help to increase significantly the number of neighbors that can be
considered for rating predictions. Propagating at distance 2 (friends of
friends) it is possible to reach 399.89 users, further increasing the trust
horizon to 3 and 4 allows respectively 4,386.32 and 16,333.94 users. This
pattern is even more evident on cold start users \cite{Massa2004}. These
results say that by propagating trust it is possible to increase the coverage
(generate more recommendations) but that it also likely to considers users who are worse
predictors for the current user so that the prediction error increases as well.
The trust propagation horizon basically represents a trade-off between accuracy
and coverage.


\subsubsection{Filterbots}

% NaÂ¨Ä±ve Filterbots for Robust Cold-Start Recommendations

Park et. al. \cite{Park2006} propose using filterbots to improve the cold-start
performance of collaborative filtering methods. Their filterbots are a varition of RipperBots, described in \cite{Good1999}.

A filterbot is an automated agent that rates all or most items using information filtering (IF) techniques. The filterbots injects psuedo
users or bots into the system. These bots rate items algorithmically according to item features and user profiles. For their movie recommendation systems the author propose using 7 global bots which rates movies based on average item rating, a critic bot that generates ratings based on the average critic (pre-selected users) ratings, an award bot that generates rating based on the awards a movie has won, and so on. These ratings generated by these bots are injected into the user-item matrix along with actual user-item ratings. Standard CF algorithms are then applied to generate
recommendations.

Their approach clearly demonstrated better robustness to all three cold-start situations than standard item-based
and user-based collaborative filtering. The improvements were most evident on
the datasets with a high degree of sparsity.

% Discussion


\subsubsection{Wisdom of the better few / Seed users}

% Wisdom of the Better Few: Cold Start Recommendation via Representative based Rating Elicitation

Liu et. al. \cite{Liu2011} propose an approach in which they elect a few
representative users and items. The representative set should represent a set
of active users or items who well represent the entire population but with
little taste overlap. In their approach they wish to find a rank-k
factorization of the form $Y \approx XR$ or $Y \approx CX$ where $X$ is a
loading matrix consisting of free parameters and $R$ and $C$ which is the
component matrix consisting of actual rows or columns from $Y$. The
representative users and items are found using dimensionality reduction
techniques by reducing the column space of the rating matrix from $m$ to $k$.
And then applying basis selection based on the maximum-volume principle to
select the $k$ most representative users or items. In order to be able to
recommend new items to the users it must first be rated by the $k$
representative users, likewise for new users to be rated they need to rate the
$k$ most representative items. Their method therefore easily allows new users
and items to be \emph{folded in}.

% Discussion
%	Not so sure about new items having to be rated by all the representative users, will this actually happen?...
%	After training the model, all new items must be rated by these users...


\subsubsection{Intelligent Selection / Interview Process}\mbox{}\\

%My though behind including these articles:
%Use e.g. a tinder like interface and ask the user to like/dislike 15 items when first logging in

\begin{chapquote}[30pt]{Vanessa Redgrave}
  "Ask the right questions if you're going to find the right answers"
\end{chapquote}

% Getting to Know You: Learning New User Preferences in Recommender Systems

As pointed out by Rashid et. al. \cite{Rashid2002}, the most direct way of
acquiring information for use in personalized recommendations from a new users
is to present item for the user to rate. However, they argue that the system
must be careful to present useful items to garner information. A food
recommender should probably not ask whether a new user likes vanilla ice cream
since most people like vanilla ice cream. Therefore, knowing that a new user
likes vanilla ice cream tells you very little about the user. The choice of
what questions to ask a new user, then, is critical. The authors performed a
study of different item selection strategies that collaborative filtering
recommender systems can use to learn about new users. They presented the users
with a questionnaire with items asking them to rate/select the ones they like.
Their strategies can be divided into five classes, which they evaluated based on user effort and accuracy:

\begin{itemize}
\item \emph{Random:} strategies: Strategies that avoid bias in the presentation
of bias
\item \emph{Popularity:} Select among the top N items where the probability
that an item is selected is proportionate to the items popularity.
\item \emph{Pure entropy:} Present the items with the highest entropy that the
user has not seen
\item \emph{Balanced strategies:} A balanced approach combining both popularity
data and entropy.
\item \emph{Personalized:} As soon as some information is known about a user,
present items specifically tailored to that user using e.g. item-item
similarity
\end{itemize}

Their suggestion for an e-commerce recommender system is to start off recommending the most popular
items rather than the highest rated ones, and then use item-item similarity as
quickly as possible.

% Learning preferences of new users in recommender systems: an information theoretic approach

This study was later extended by Rashid et. al. \cite{Rashid2008} where they
more closely examined information theoretic strategies for item selection. In the article they introduced three new strategies, which where compared based on user effort and accuracy:

\begin{itemize}
\item \emph{Entropy0}: Entropy Considering Missing Values
\item \emph{HELF:} Harmonic mean of Entropy and Logarithm of Frequency
\item \emph{IGCN:} Information Gain through Clustered Neighbors
\end{itemize}

% User effort vs. accuracy in rating-based elicitation

Cremonesi et. al. \cite{Cremonesi2012} performed a set of experiments where
they looked at the trade-off between user-effort and accuracy. More
specifically, how many ratings are enough to provide good quality
recommendations to new users? The authors conclusion is that between 5 and 20
ratings are optimal for the movie domain. They concluded that 10
ratings is \emph{enough}, but that this number depends on the recommendation method
and the dataset used.

% Discussion

Our rational for including these articles is that we envision a simple "hot or not" interface to be used to present items to new users when the first log in to the system.


It is worth mentioning that the authors worked on the cold-start new user
problem for a movie recommender. The implications of this is that a user must
have watched a movie, in order to rate it. This is not so important for our
domain, as taking a quick look at the item should be sufficient to like/dislike
it. 

% We also currently have no way of calculating entropy as we have no rating distribution for our items
% Until we have implicit ratings in place, is this still suitable then?

We are also currently constrained to unobtrusively learn user-profiles from the natural
interactions of users with the system, meaning that we can not require the user
to rate e.g. 10 items before we can start providing recommendations. We
currently have a \emph{mixed initiative} system meaning that there is
provisions for both user and system controlled interactions. We (the system)
can only select which items to recommend to the user, and this does not mean
that the user actually will click an item or rate it.


\subsubsection{Hybrid Methods}

%TODO - Sort articles, some are more general cold-start solutions (not just new-item)
%TODO - Add some fancy math, wait until matrix factorization intro is in place (so we do not mess up notations)
%TODO - Find some sweet cf articles incorporating demographic information
%TODO - Merge the hybrid sections fro all subsubsections?

Another line of search for solving the cold-start problem is to utilize features of items and users. The content features can be used to capture the similarities between users and items, thus reducing the amount of data required to make accurate predictions. User data that may be collected typically includes age, gender, nationality, marital status, income, educational level and occupation. Item data could e.g. be the price of a product, title, description, editorial ratings and so.

%The idea is that people with a more common background share a more similar taste than someone with a random background, and therefore good %recommendations can be made as long as we know the new userâ€™s background.

This section will present some latent factor models presented recently proposed that incorporate both user/item features and past interactions. In Matrix factorization methods, the regularization is mostly based on a
zero-mean Gaussian prior on the factors, we refer to this method as ZeroMean. However. in these models the dyadic response matrix $Y$ is estimated by a latent factor model such that $Y \approx U^{T}V$, where the latent factor matrices, $U$ and $V$, are estimated by regression such that $U \approx FX$ and $V \approx MZ$. $X$ and $Z$ denote user attribute and item feature matrices, and $F$ and $M$ are weight matrices learned by regression. The difference between the following methods are how they estimate these weights.

% Regression-based Latent Factor Models

Agarwal et. al. \cite{Agarwal2009} propose a class of latent factors models
called regression-based latent factor model (RLFM) that incorporates both user/item features and past interaction data
into a single model. Their approach utilizes features of items and users as the
prior distribution for latent profiles in matrix factorization.

Users and items are anchored around a global feature-based one where profiles are constructed by estimating deviations from the global ones in a smooth fashion. The deviation depends on the amount of information available, e.g. items/users with sparse data are aggressively "shrunk" to the global one. New items and users start out with profiles based on their known features that gets refined smoothly with the availability of more data.

Regularizing latent factors through regression has important consequences when modeling sparse dyadic data. For users/items with little data, one obtain reliable factor estimates by using the regression estimates as a fallback. This allows the model to effectively deal with both cold start and warm start situations.

% fLDA: Matrix Factorization through Latent Dirichlet Allocation
%

Agarwal et. al \cite{Agarwal2010} propose a Matrix factorization method to predict ratings in recommender system
applications where a "bag-of-words" representation of item meta-data is natural. Their method regularizes both user and item factor simultaneously through user features and the bag of words associated with each item.

The key idea of their method is to let user factors take values in an Euclidean space of existing factorization models, but assign item factors through a richer prior based on Latent Dirichlet Allocation (LDA). The main idea behind LDA is to attach a discrete latent factor to each word of an item that can take $K$ different values ($K$-topics) and produce item topics by averaging the per-word topics in the item. An article where 80$\%$ of the words are assigned to politics and the rest to education would be though of as a political article related to the issue of education. This allows us to model the affinity between user $i$ and item $j$ as $s'{j}\hat{z_{j}}$, where $\hat{z_{j}}$ is the multinomial probability vector representing the soft cluster membership score of of item $j$ to the $K$ different latent topics.

% Matchbox: Large Scale Bayesian Recommendations
% 	Online algorithm

Stern et. al. \cite{Stern2009} presents a probabilistic model called Matchbox. The system makes use of content information in the form of user and
item meta data in combination with collaborative filtering information from previous user behavior in order to predict the value of an item for a user. Much like \cite{Agarwal2009} the factors are regularized by incorporating more flexibility in the Gaussian priors through regression on user and item factors.

Their model is dynamic, meaning that it allows an item's popularity, a user's taste or user's personal rating scale to drift over time, as well as being training online. This means that the value of weight matrices $F$ and $M$ will drift over time, this is accomplished by the addition of Gaussian noise each time step. Inference is accomplished a combination of message passing and expectation propagation.

% Learning Attribute-to-Feature Mappings for Cold-Start Recommendations
% 	Model for positive implicit feedback!
% 	Demonstrates usefulness for new-item recommendations
% 	See A. Item Recommendation from Implicit Feedback in the article for implicit feedback recommendations
%	k-NN worked best with MORE features than the linear mapping functions
%	Code can be found at: ismll.de/mymedialite

Gantner et. al. \cite{Ganter2010} propose a method on how to map additional
information such as user and item features to the latent features of a matrix
(or higher dimensional) factorization model. At the core of their approach is a
standard factorization model, optimized to the recommendation task. The
extensions include a mapping function that compute adequate latent
representations for new entities from their attribute representations. This
mapping function could allow new items and users latent features to be found
only based on content-information and further on be used as if they were
normally trained latent features. The training of the factorization model with
a mapping extension consists of the following steps:

\begin{enumerate}
\item training the factorization model using the data $S$, and then
\item learning the mapping functions from the latent features of the entities in the training data and their content attributes
\end{enumerate}

The authors use BPR-MF, a matrix factorization model based on the Bayesian Personalized Ranking (BPR) framework as their factorization model. The authors experimented with two different ways of mapping item/user attributes to the factor space (Only attribute-to-feature mapping for items are presented in the article):

\begin{enumerate}
\item k-NN Mapping:	Weighted k-NN regression for each factor. Determine the k-nearest neighbors as the most similar items according to the cosine similarity of the attribute vectors.
\item Linear Mapping: Each item factor is expressed by a weighted sum of the item attributes. Suitable parameters for the mapping function is learned by optimizing the model for the squared error on latent features.
\end{enumerate}

\subsubsection{Discussion}


\subsection{Fashion Recommendation}


\subsubsection{Theory}
This subsection will look into some background research on fashion.
And look into what fashion is and why a consumer behaves like the consumer does.

\textbf{What is fashion:}
There are a lot of different ways of defining what fashion really is.

\begin{itemize}
    \item Some claim it to be the entire spectrum of attractive clothes at any given time.
    \item Others may claim it to be the worship and pursuit in an era or at one
    time.
    \item In \cite{Fang2012} this definition on fashion is being used: "Fashion is
    the social norm recognized and advocated by a particular social class at one
    time. It affects all the fields in society, especially and famously in
    clothing. Sometimes, short-lived fashion is referred to as style."
    \label{items:fashionDefinitions}
\end{itemize}

As seen from the different definitions mentioned above, what is reoccurring is;
clothes, popularity, time and a cultural grouping.
\todo{Better flow}
enough items to make the assumption

\textbf{Task of fashion marketing:}
Fashion is subject to constant change as seen from the different definitions of fashion from ~\ref{items:fashionDefinitions}.
Some of these changes are due to human changes such as adoption of a new line of clothing, or something less controllable, such as the changing of the seasons.
How much of a product should be made to satisfy the need of the consumer, but still remain a desirable product the consumer would find itself unique and special with.
What is a reasonable price for the product, and how much is the name of the designer worth?
Who can distribute the product with the product loosing its value and fashion status.
This are just some of the questions the fashion industry has to answer.
Without them answered the potential of the product can't be reached.
Which could lead the consumer not to feel the uniqueness and prestige of the item.
Fashion trends comes and goes, and the new fashion starts with the refusal of what is old.

% Companies must: MTODO Not interesting in our case?
% - find consumer needs
% - consumer segment, how to approach
% - pos to reach segment
% - reqs of the segment
% - price
% - channel distribution demands
% - sails starts at segment

In fashion there is a big difference between men and women in what, where, when
and how they buy.  How to understand the behavior of the consumers and how they
act can come from a vast set of areas, the main factors influencing the consumer according to \cite{kotler2009marketing} is:

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|l}
      \textbf{Factors}        & \textbf{Examples} & \textbf{something?} \\ \hline
      Physiological factors   & Physical protection, commodity & - \\ \hline
      Socio-cultural factors  & Family, friends, work, social groups & - \\ \hline
      Personal factors        & Age, life cycle, occupation, personality & - \\ \hline
      Psychological factors   & Product reliance or sympathy & - \\ \hline % more expensive because more expensive - increase self-confidence
      Rational factors        & Brand of product, quality, designer, price & - \\
    \end{tabular}
    \label{table:FashionFactors}
    \caption [Fashion Factors]{Main factors influencing the consumer when it comes to their buying behavior}
\end{table}
\todo{Last column for in depth explanation (perhaps)}
When it comes to fashion it is mainly a socio-cultural phenomenon.

One central factor when it comes to shopping and fashion is price, a rational factor.
The consumer acts rational, when it comes to price and quality~\cite{Hanf1994}.
In the case of fashion, and a product connected with prestige, this rational behavior might not apply.

There is a set of product criterias a consumer evaluates when it comes to the acquisition of a product~\cite{dutton2006}, attributes found to have the most significant impact was styling, brand , price, place(store), fabrication/fiber content.

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|l}
      \multicolumn{2}{c|}{\textbf{Concrete Attributes}} & \textbf{Abstract Attributes} \\
      \multicolumn{2}{c|}{\textbf{(product features)}} & \multirow{2}{*}{\textbf{(attitude-based)}} \\ \cline{1-2}
      Intrinsic (hedonic)   & Extrinsic & \\ \hline
      \pbox{4cm}{
          - Style \\
          - Color \\
          - Patten \\
          - Fabric/fiber \\
          - Appearance \\
          - Fashionability \\
          - Durability \\
          - Comfort \\
          - Quality \\
          - Fit \\
          - Care \\
      } & \pbox{6cm}{
          - Price \\
          - Brand \\
          - Country of origin \\
          - Place(Store) \\
          - Salespeson's evaluation \\
          - Approval of others \\
          - Coordination with wardrobe \\
      } & \pbox{4cm}{
          - Fun \\
          - Entertainment \\
          - Enjoyment \\
          - Need \\
          - Function \\
        } \\ \hline
    \end{tabular}
    \label{table:ConsumersPurchaseDec}
    \caption [Consumers' Purchase Decisions]{The attributes effecting the consumer when in the process of consuming products~\cite{dutton2006}}
\end{table}

The modern consumer finds pleasure with consumption experience itself, not just the product, and this especially applies to the fashion domain.
The purchase is often not done by need, but for pleasure.

\textbf{Consumer buying behavior:}
A lot of information about the consumers behavior is lost due to the reasons for their behavior is held in an unconscious or implicit level.
The reason for a person is interested in a specific product could be based on some distant memory of the consumers life.
This could affect how a consumer views a particular brand or product for good or worse.
Brand choices are often made intuitively, based on their subconscious, and cannot tell why they mad that choice.

Culture is one of the main factors to determine consumer behavior.
Culture can be segmented into three parts: Culture, subculture and social class.
All consumers are included in many smaller subcultures such as nationality, religious subcultures and geographical subcultures.
Subcultures can be a efficient way of constructing marketing campaigns and aim similar products at, since they tend to form market segments.
The forming of a subculture happens through individuals seeking out other individuals with similar tastes regarding a variety of aspects~\cite{vignali2009fashion}.
There are a lot of different behavior emerging from subcultures, such as peer pressure.
Social psychology is used to understand the behavior of the individuals in subcultures~\cite{vignali2009fashion}.

\textbf{Customer satisfaction:}
There are two main concepts when it comes to customer satisfaction:
Transaction specific and cumulative specific.
The transaction specific satisfaction of the consumer is base on the expectations in the pre-purchase stage and the perceived performance of the product in the post-purchase stage.
Where the cumulative looks at the purchase as a whole, such as: the product, the purchase and the service received~\cite{kumari2012}.
The transaction specific focuses on the post-purchase, if the expectations of the product during the pre-purchase is met in the post-purchase stage, the likeliness of a repeat purchase is increased.



\subsubsection{What Other Fashion Recommender Systems Has Done?}
\todo{A shorter and more precise way of saying this}
This subsection will look methods of other fashion recommender systems and other proposed methods to recommend products in the fashion domain.

\textbf{Photograph based approach}
Fashion and the products it regards are highly dependent on visuals.
A fashion product would not be very interesting if no one saw it. %MTODO redundant?
An approach to use the importance of how the product looks regarding recommending is to utilize images of the product.
Fashion Coordinates Recommender System Using Photographs from Fashion Magazines~\cite{Iwata:2011} is a system doing this.
They teach their system by using fashion magazines with full body images.
They segment the image into two parts, top and bottom.
From this the system learns which top matches to which bottom and collects visual features of the products.
From this the system can recommend other tops to go with a selected bottom, or other way around.
The proposed system scored better\footnote{Accuracy of 50\% on the top 5 suggested items, whereas naive and random managed 18\% and under 5\% respectively} than both a more naive approach and a random selection.
Runtime was at 0.04 seconds per recommendation.


\textbf{Hot-or-not}
A recommender system called SuitUp~\cite{SuitUp} did a survey on some of their potential users.
One interesting finding was that many of the users enjoyed the Hot-or-Not feature of the system.
This feature gives the user a set of items and the option to either like or dislike.

\textbf{Trust based}


% Building Recommender Systems using a Knowledge Base of product semantics
% http://images.accenture.ca/SiteCollectionDocuments/PDF/recommenderws02.pdf
% 	- Would probably require some more product semantics...

%What are the challanges of making recommendations for fashion?

%	- For how long are items relevant?
	%	- Spring, Summer, Fall, Winter collections
			%Improving E-Commerce Recommender Systems by the Identification of Seasonal Products (Article)
	%	- Freshness, fresh decay operators

%	- Implicit feedback (Based around users fashion browsing habits and an occational purchase...)
	%	- What do we look at? What information is the most useful
	%		- Item category, item keywords, brand... ?

%	- Changing interest of users
%	- Unstructured content/multiple content providers
	% - How to select features for a content-based approach
		% E.g. keywords, when descriptions are in multiple languages
%	- Sparsity
	% - Can rating infromation from similar items be used to decrease sparsity? (Content infromation - Hybrid approaches)
%	- Trends?
	%	- How important is e.g. item popularity?

\subsection{Session Based Approach}
\todo{This subsection has been dead for a while. Just here in case of revival (time)}
Init Hypothesis:
Two users with similar session habits and similar product accessing pattern
have a stronger correlation to one-another than two users with just similar
product interests.


'product\_purchase\_intended' (user pushed to the product web store) shows a
wider specter of information about the product, including additional colors,
images and colors.  For some it might be natural to explore the item there
before "wanting" it. Making both

"product\_purchase\_intended" $\Rightarrow$ "product\_wanted"

and

"product\_purchase\_intended" $\notimplies$ "product\_wanted"

produce valuable information.

Must make different rules for the different stores:
"Bik Bok", "Cubus", "Gina Trik", "H\&M", "Bianco" has a broad specter of extra
functions inside the web store, whereas others might not, only shows the
product and a add to chart button.  This might divide the use pattern of the
users into a:

"product\_detail\_clicked" $\Rightarrow$ "product\_purchase\_intended" $\Rightarrow$ "product\_wanted"

"product\_detail\_clicked" $\Rightarrow$ "product\_purchase\_intended" $\notimplies$ "product\_wanted",

and

"product\_detail\_clicked" $\Rightarrow$ "product\_wanted"

based on the store accessed.

Use this to make a "rule set" with a probability.
Then again use this to recommend items for the users with that given
probability.

Find a "most popular session"-pattern
Find a "most likely to come after"-pattern

% db.sessions.group({key:{'storefront_name':1},cond:{},reduce:function(cur,result){result.count += 1}, initial: {count:0}})

Articles 4 l8er:
%http://dl.acm.org/citation.cfm?id=1136004
%http://link.springer.com/chapter/10.1007/3-540-46119-1_42
%http://dl.acm.org/citation.cfm?id=1082567
%http://link.springer.com/chapter/10.1007%2F978-3-540-30214-8_20
%http://dl.acm.org/citation.cfm?id=502935
%http://dl.acm.org/citation.cfm?id=1835896
%http://dl.acm.org/citation.cfm?id=345169
%http://dl.acm.org/citation.cfm?id=345169

Session issues:
Once in a blue moon a user will do a "product action" (purchase,want,details)
without having a previous frontstore-access event. Which leads to unknown
store-id of the item.

Issue is most probably from missing user-id in collection\_viewed, and a user
checks out an item from there. It is not possible to be 100\% sure which user
access the item from the collection\_viewed event, so this event is therefor
not integrated into the session-stack.


% thoughts:
Categorize stores
    prize
    items in store

Categorize items
    Type
    Prize
    View frequency

Predicting events...
    Value brought vs. clustering on the "item"-events value

Make a store

% event_id, events:

% Products:
%     "product_detail_clicked",
%     "product_wanted",
%     "wantlist_menu_entry_clicked"
%     "product_purchase_intended",

% Store clicked: (produces not NULL storefront_name) (db.sessions.find({'storefront_name':{$ne:'NULL'},$or:[{'event_id':'featured_storefront_clicked'},{event_id:'storefront_clicked'}]}).count())
%     "storefront_clicked",
%     "featured_storefront_clicked",

% Other store interactions
%     "store_clicked",
%     "around_me_clicked",
%     "stores_map_clicked",
%     "collection_viewed",
%     "featured_collection_clicked",

% Start:
%     "app_first_started",
%     "app_became_active",
%     "app_started",
%     "user_logged_in",
%     "facebook_login_failed",

% Other:
%     "friend_invited",he users rat
%     "activity_clicked",
%     "facebook_share_changed",

% Course:
%     App started
%     Check next events, a days timeframe


%Simple session form, no structure:
% {u'event_id': u'product_detail_clicked', u'count': 68.0}
% {u'event_id': u'product_wanted', u'count': 35.0}
% {u'event_id': u'storefront_clicked', u'count': 69.0}
% {u'event_id': u'app_started', u'count': 26.0}
% {u'event_id': u'featured_storefront_clicked', u'count': 4.0}
% {u'event_id': u'user_logged_in', u'count': 9.0}
% {u'event_id': u'product_purchase_intended', u'count': 2.0}
% {u'event_id': u'around_me_clicked', u'count': 7.0}
% {u'event_id': u'stores_map_clicked', u'count': 1.0}
% {u'event_id': u'store_clicked', u'count': 1.0}
% {'user_id': 100001385800886L}
% {'num_events': 222}
% Total amount:    222
% User:            100001385800886
% Total Sessions:  30
% Total Events:    936
% Date:            11 - 10 - 2013

% Structured session exploration: Probably more info in this
% > db.sessions.find({'user_id':1094505588,session:64},{'event_id':1,'server_time_stamp':1,'_id':0}).sort({'ts':1})

% > db.sessions.find({'user_id':100000140823565,session:440},{'event_id':1,'server_time_stamp':1,'_id':0}).sort({'ts':1})

% > db.sessions.find({'user_id':100000140823565,session:440},{'product_id':1,'event_id':1,'_id':0}).sort({'ts':1})

\subsection{Items clustering}
\subsection{One-Class Collaborative Filtering}

