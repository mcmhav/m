% !TEX root = ../../report.tex

% What will be adressed in the section?
  % Motivation for using implicit feedback

	% Implicit feedback vs. explicit feedback
		% How is implicit feedback related to user preference

  % Evaluating implicit ratings, extracted from implicit feedback.

  % Closely related article:
    % Evaluation -> And specifically, evaluation of implicit ratings and
    % predictions based on these.

\section{Implicit feedback to implicit ratings}

Explicit feedback is present in many of the largest recommender systems today
and hence, is extensively researched.\todo{Ref.} The user is commonly asked to
rate item $i$ on a Likert scale from $1$ to $k$, ranging from strongly
disagree to strongly agree with an item. Its advantages are, among others the
ability to get precise feedback from the user and capturing both positive and
negative preferences. However, although having a high popularity, the method
has multiple weaknesses. The most prominent weakness is the difficulty of
collecting ratings: the method requires the user to spend time rating items and
the amount of feedback is often scarce, creating sparse data sets. Further,
explicit ratings are often subject to inconsitencies known as natural
noise \cite{amatriain2009like} and users might also be pressed to report
different preferences due to peer of social pressure. The fact that we are
introducing a user overhead, makes it difficult to have a complete view on the
user preferences \cite{jawaheer2010characterisation}.

We can try to achieve better results and create a more pleasant experience for
the user, by looking at behavioural statistics, which is both easier to collect
and does not require any extra effort from the user. Our end-goal is to predict
a rating $r$ for a given user $u$ on item $i$, and thus we need some way of
translating our implicit feedback into what we call \textbf{implicit ratings} -
these are ratings, just like explicit ratings, but inferred by user behaviour
and as we will see in the succeeding sections need to be anylyzed with this in
mind.

The following table, partly inspired by Hu et. al. \cite{Hu2008}, identifies
the different characteristics in implicit and explicit ratings:

\begin{table}[h!]
    \begin{tabular}{|l|p{6cm}|p{6cm}|}
    \hline
    ~                  & \textbf{Explicit ratings}
                       & \textbf{Implicit ratings} \\ \hline

    \textit{Types of feedback}  
                       & Positive and negative.
                       & Positive. \\ \hline

    \textit{Meaning}
                       & Indicates preference. Often on a Likert scale ranging
                         from total dislike to really like.
                       & Indicates confidence. Higher frequency in implicit 
                         feedback does not necessarily indicate higher
                         preference. \\ \hline

    \textit{Noisiness}
                       & Medium. Depends on domain and can be removed 
                         \cite{amatriain2009like}.
                       & High. One of the main challenges in evaluating
                         implicit feedback is choosing which events to
                         consider. \\ \hline

    \textit{Evaluation metrics}
                       & Traditional metrics such as RMSE are commonly used.
                       & As users do not provide numerical scores, a
                         Precision-Recall scheme or similar or more often used.
                         \\ \hline

    \textit{Strengths}
                       & Heavily researched, good evaluation metrics, easy to
                         implement.
                       & Does not require feedback from user, less sparsity and
                         catches actual behavior, not influenced by peer
                         pressure etc. \\ \hline

    \textit{Weaknesses}
                       & \multicolumn{2}{c|}{See subsection \ref{implicit-weaknesses}} \\ \hline
    \end{tabular}
\end{table}

\subsection{Quantifying implicit feedback}

Previous work:

\begin{enumerate}
\item Naive rule based
\item Walk the Talk
\end{enumerate}

A hybrid approach using sequential pattern analysis and collaboraitve filtering
techniques are presented by Choi et. al. \cite{choi2012hybrid}. In their
algorithm they calculate an implicit rating by finding the absolute preference
AP:

\begin{equation}
  AP(u,i) = ln(\frac{trans(u,i)}{\sum_{e \in E}{trans(u, e)}} + 1)
\end{equation}

where $trans(u,i)$ is the number of transactions for user $u$ on item $i$.
However, this absolute preference only takes into account the frequency of
purchases and because the frequency is heavily dependent on price, item
category and lifespan of an item - they compare it with other users finding the
relative preference RP:

\begin{equation}
  RP(u,i) = \frac{AP(u,i)}{Max_{c \in U}(AP(c,i))}
\end{equation}

where $U$ denotes every user who purchased item $i$. The reason for using a
maximization function, is to make $RP(u,i)$ range from $0.0$ to $1.0$ (i.e.
normalization) and one can thus find a rating on a common Likert scale by
multiplying with $k$, here $k=5$:

\begin{equation}
  ImplicitRating(u,i) = \lceil 5 * RP(u,i) \rceil
\end{equation}

\subsection{Challanges and weaknesses}
\label{implicit-weaknesses}

Almost all of the research on implicit feedback has considered how behaviors
can be used as positive evidence, rather than negative evidence. However, one
can imagine behaviors which indicate that a user does not find something
relevant or which suggest that something is unimportant to the user, such as
delete. It is likely that little research has been conducted on negative
implicit feedback because there are fewer of these types of behaviors, and, in
general, less is understood about how to effectively use negative feedback,
whether for implicit or explicit relevance feedback.

Another challenge facing implicit feedback research is the notion of degree of
personalization offered by the system. In particular, individual differences
can greatly impact the effectiveness of using behavior as implicit relevance
feedback. People behave differently and have varying approaches to
information-seeking; thus, it is difficult to generate, and dangerous to apply,
all-purpose rules for describing how behavior can be used as implicit relevance
feedback.

\subsection{Evaluating convertion to implicit ratings}

Why we can not consider number of clicks on product implicit feedback.

