% !TEX root = ../../report.tex

% What will be adressed in the section?
  % Motivation for using implicit feedback

	% Implicit feedback vs. explicit feedback
		% How is implicit feedback related to user preference

  % Evaluating implicit ratings, extracted from implicit feedback.

  % Closely related article:
    % Evaluation -> And specifically, evaluation of implicit ratings and
    % predictions based on these.

\label{implicit-feedback}
\section{Implicit feedback to implicit ratings}
\label{sec:implicit}

%Helge: Poenget er at å rate i sobazar koster penger, mens det er "gratis" hos netflix

Explicit feedback is present in many of the largest recommender systems today
and hence, is extensively researched.% \todo{Ref.} The user is commonly asked to
rate item $i$ on a Likert scale from $1$ to $k$, ranging from strongly
disagree to strongly agree with an item. Its advantages are, among others the
ability to get precise feedback from the user and capturing both positive and
negative preferences. However, although having a high popularity, the method
has multiple weaknesses. The most prominent weakness is the difficulty of
collecting ratings: the method requires the user to spend time rating items and
the amount of feedback is often scarce, creating sparse data sets. Further,
explicit ratings are often subject to inconsistencies known as natural
noise~\cite{amatriain2009like} and users might also be pressed to report
different preferences due to peer of social pressure % \todo{ref}. The fact that we are
introducing a user overhead, makes it difficult to have a complete view on the
user preferences~\cite{Jawaheer2010}.

We can try to achieve better results and create a more pleasant experience for
the user, by looking at behavioral statistics, which is both easier to collect
and does not require any extra effort from the user. Our end-goal is to predict
a rating $r$ for a given user $u$ on item $i$, and thus we need some way of
translating our implicit feedback into what we call \textbf{implicit ratings} –
these are ratings, just like explicit ratings, but inferred by user behavior
and as we will see in the succeeding sections need to be analyzed with this in
mind.

The following table, partly inspired by Hu et al.~\cite{Hu2008}, identifies
the different characteristics in implicit and explicit ratings:

% \todo{Helge: Maybe best in plain text?}
\begin{table}[H]
    \begin{tabular}{l p{6cm} p{6cm}}
    \toprule
    –                  & \textbf{Explicit ratings}
                       & \textbf{Implicit ratings} \\ \midrule

    \textit{Types of feedback}
                       & Positive and negative.
                       & Positive. \\ \midrule

    \textit{Meaning}
                       & Indicates preference. Often on a Likert scale ranging
                         from total dislike to really like.
                       & Indicates confidence. Recurring events is more likely
                         to reflect the user opinion.
                         \\ \midrule

    \textit{Noisiness}
                       & Medium. Depends on domain and can be removed
                         \cite{amatriain2009like}.
                       & High. One of the main challenges in evaluating
                         implicit feedback is choosing which events to
                         consider. \\ \midrule

    \textit{Evaluation metrics}
                       & Traditional metrics such as RMSE are commonly used.
                       & As users do not provide numerical scores, a
                         Precision-Recall scheme or similar or more often used.
                         \\ \midrule

    \textit{Strengths}
                       & Heavily researched, good evaluation metrics, easy to
                         implement.
                       & Does not require feedback from user, less sparsity and
                         catches actual behavior, not influenced by peer
                         pressure etc. \\ \midrule

    \textit{Weaknesses}
                       & \multicolumn{2}{c}{See Subsection~\ref{implicit-weaknesses}} \\
    \bottomrule
    \end{tabular}
\end{table}

\clearpage

\subsection{Quantifying implicit feedback}

\subsubsection{Classifying levels of frequency}

The naive way of quantifying implicit feedback is to use a simple
\textit{counting scheme} where ratings are made based on frequency of certain
events. The two most important factors in such a scheme is how one choses these
events and how to weight them. We can imagine that in a e-commerce store the
following events would be useful:

\begin{table}[H]
  \centering
  \begin{tabular}{ll}
  \toprule
  Event type & Description \\ \midrule
  0 & Item purchased \\
  1 & Item placed in shopping cart \\
  2 & Item placed in wish list \\
  3 & Item browsed based on search result \\
  4 & Item browsed \\
  \bottomrule
  \end{tabular}
\end{table}

Then, our heuristic would count the frequency of each event for an item $i$ on
user $u$. However, just counting is not a bounded function – so if one give
\textit{Item browsed} the weight of $1$ and the \textit{Item purchased} event
a weight of $100$, then a user browsing an item more than 100 times would get a
higher implicit rating than a user buying it.

Instead,~\cite{pkghost2014implicit} proposes using a global rating mapping,
that uses \textit{levels of frequency}.

\begin{table}[H]
  \centering
  \begin{tabular}{ll}
  \toprule
  Event type & Scores \\ \midrule
  0 & 100 \\
  1 & 70, 77, 80 \\
  2 & 30, 40, 45, 48, 50 \\
  3 & 20, 25, 28, 30 \\
  4 & 10, 15, 18, 20 \\
  \bottomrule
  \end{tabular}
  \caption{Scores per event type, increases as frequency of each event
           increments}
\label{implicit-table}
\end{table}

Then a user browsing an item $i$ two times, would get a score of 40 and a if
buying it the score would be 100 (e.g.\ the event type with highest interest
level supersede all others). Equally, if a user browsed the item 100 times, it
would make no difference in score compared to browsing it 4 times. The scores
given to the various event types may vary from what kind of dataset one have,
the domain and should be evaluated using one or more of the methods mentioned
in Section~\ref{evaluation}.

If we wanted ratings between 1 and $k$ one can transform the score $s$ provided
by table~\ref{implicit-table}:

\begin{equation}
  ImplicitRating(s, k) = 100 * \frac{k-1}{100} + 1
\end{equation}

The advantage of this heuristic is that it requires no training, it is simple
to understand and works reasonably well, if weights are chosen correctly.
The latter is also its largest weakness – as finding these weights may be
difficult. If one had explicit feedback, as well as the implicit, one could
train a model automatically choosing weights. Further, when using the scores
defined above we only use a small percentage of the scale 0\-100 and for all
users the highest score within an event would be the same, even if one user
triggered the \textit{event type 3} 10 times more often than another user, as
long as both had reached the highest score of 30.

Improvements to this model are presented in Section~\ref{design}
% \todo{ref the correct section}

\subsubsection{Regression analysis}

This requires us to having some metric informing our algorithm about good or
bad weights. This is done in a study by Parra et al.~\cite{parra2011walk}
where they do a quantitative user study asking 114 active users to rate items
found in their activity history. They used this in order to do
a \textit{Regression analysis}, where the rating was the \textit{dependent
variable} and the implicit feedback, combined with other factors were the
\textit{independent variables}.

A regression analysis is using multiple data points in order to learn how the
independent variables relate to the dependent variable. Often the linear
regression model is formulated as:

\begin{equation}
  \label{eq-regression}
  \hat{y}(w,x) = w_0 + w_1 x_1 + \cdots + w_p x_p
\end{equation}

where we want to find the values of the $w$, also called the unknown
parameters. Using a dataset of $x$ (independent) and $y$ (dependent) values we
can solve the equation above and perform the analysis. As a simple example,
consider the following dataset:

\begin{figure}[H]
  \centering
  \begin{BVerbatim}
   x  | y
  --------
   -2 | 3
   3  | -1
   1  | 1
   -1 | 3
   2  | 3
   4  | -2
  \end{BVerbatim}
\end{figure}

Solving equation~\ref{eq-regression} we have the following equations where we
want to find the values of $w_0$ and $w_1$:

\begin{equation}
  \label{eqs-regression-example}
  \begin{split}
    -2 = w_0 + 3 w_1 \\
    \dots \\
    4 = w_0 - 2 w_1
  \end{split}
\end{equation}

Using a minimization method called \textit{Least squares} we try to minimize
the residual $e$, using sum of squares between the observed values in the
dataset and the values predicted by the linear approximation. Mathematically it
solved the problem of the following form:

\begin{equation}
  \min (SSE = \sum_{i=1}^{n} e_i^2)
\end{equation}

Carrying out this minimization for our equations~\ref{eqs-regression-example},
we obtain an intercept value ($w_0$) of $1.3787$ and a slope ($w_1$) of
$0.5454$, this can be validated as a good approximation to the data points, by
looking at the 2D-plot below:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis lines=middle,
        xmin=-5, xmax=5,
        ymin=-5, ymax=5,
        xtick=\empty, ytick=\empty
    ]
    \addplot[only marks]table[]{-2 -3
      -3 -1
      1 1
      -4 -2
      2 3
      -1 3
    };
    \addplot [domain=-5:5, samples=2, dashed] {0.5454*x+1.3787};
    \end{axis}
  \end{tikzpicture}
\end{figure}

Now, using this methodology on implicit feedback we need a regression model that
correctly explains the correlation between feedback and ratings. Four models
are proposed~\cite{parra2011walk}:

\noindent
Model 1: $r_{iu} = w_0 + w_1 if_{iu}$ \\
Model 2: $r_{iu} = w_{characterisation0} + w_1 if_{iu} + w_2 re_{iu}$ \\
Model 3: $r_{iu} = w_0 + w_1 if_{iu} + w_2 re_{iu} + w_3 gp_{i}$ \\
Model 4: $r_{iu} = w_0 + w_1 if_{iu} + w_2 re_{iu} + w_3 if_{iu} \cdot re_{iu}$

where $if_{iu}$ is a number, indicating a bucket of items between 1 and 3,
dependent on how many times user $u$ used item $i$. Equally $re_{iu}$ depicts
how recently user $u$ used item $i$ and $gp_{i}$ how popular product $i$ is,
globally. By accounting for recentness in their regression model (model 2\-4)
they achieved an improvement in the $R^2$ value by 10\%. $R^2$, also called the
coefficient of determination, is a standard metric in regression analysis
indicating how well data points fit a statistical model. Training they used
their $w$-vector, combined with the independent variables to predict future
ratings.

The buckets with values between 1 and 3 are used to get a reasonably
homogeneous sampling of items, as a random sampling strategy might yield only
very popular items. The three bins from 1 to 3 represent respectively low,
medium and high activity on an item within a independent variable. As suggested
in the article one may get better results by using fewer or more buckets. With
3 buckets and accounting for 3 different feedbacks (Implicit feedback, global
popularity and recentness) this creates in effect 27 buckets per user.

As one may see, this approach requires some form of explicit feedback, in order
to train our model with independent and dependent variables. In the research
done by Parra et al.\ they performed a user survey based on 114 active users,
that is they had more than 5000 events in the system. This requirement makes
regression analysis not suitable for domains where the relation between
implicit and explicit feedback are not linear nor symmetric, e.g.\ a purchase on
an item does not imply many clicks on the item, by the same user – whilst the
probability of the opposite is larger. See Section~\ref{} for a larger
discussion surrounding this topic. % \todo{link to correct section}

% \todo{Explain how number between 1 and 3 was given}

\subsubsection{Relative preferences using buying frequency}

A hybrid approach using sequential pattern analysis and collaborative filtering
techniques are presented by Choi et al.~\cite{choi2012hybrid}. In their
algorithm they calculate an implicit rating by finding the absolute preference
AP\@:

\begin{equation}
  AP(u,i) = \ln(\frac{trans(u,i)}{\sum_{e \in E}{trans(u, e)}} + 1)
\end{equation}

% \todo{Helge: E=?, hvilke sett er det}

where $trans(u,i)$ is the number of transactions for user $u$ on item $i$.
However, this absolute preference only takes into account the frequency of
purchases and because the frequency is heavily dependent on price, item
category and lifespan of an item — they compare it with other users finding the
relative preference RP@\:

\begin{equation}
  RP(u,i) = \frac{AP(u,i)}{Max_{c \in U}(AP(c,i))}
\end{equation}

where $U$ denotes every user who purchased item $i$. The reason for using a
maximization function, is to make $RP(u,i)$ range from $0.0$ to $1.0$ (i.e.\
normalization) and one can thus find a rating on a common Likert scale by
multiplying with $k$:

\begin{equation}
  ImplicitRating(u,i) = \lceil k * RP(u,i) \rceil
\end{equation}

% \todo{Explain a bit about Sequential pattern analysis}

\subsection{Challenges and weaknesses}
\label{implicit-weaknesses}

Almost all of the research on implicit feedback has considered how behaviors
can be used as positive evidence, rather than negative evidence. This is not to
say that we can not use user behaviors to get negative feedback, examples may
be analyzing bounce rates (e.g.\ reading an article for less than 5 seconds) or
deleting/hiding something from a feed of items. There are however much fewer of
these types of events, and they are easier retrieved in certain domains, such
as news.

Another challenge facing implicit feedback research is the notion of degree of
personalization offered by the system. In particular, individual differences
can greatly impact the effectiveness of using behavior as implicit relevance
feedback. People behave differently and have varying approaches to
information-seeking; thus, it is difficult to generate, and dangerous to apply,
all-purpose rules for describing how behavior can be used as implicit relevance
feedback.

% \todo{Add weaknesses of explicit ratings}

% \todo{Mention fashion-specific challenges, e.g.\ clicks does not equal high
% rating}

\subsection{Evaluating conversion to implicit ratings}

% \todo{Something about MPR (mean percentile ranking) and perhaps AP (average
% precision)}
